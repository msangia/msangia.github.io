<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 7 Arboles de decision | Ciencia de Datos</title>
  <meta name="description" content="BCRA" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 7 Arboles de decision | Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="BCRA" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 7 Arboles de decision | Ciencia de Datos" />
  <meta name="twitter:site" content="@msangia" />
  <meta name="twitter:description" content="BCRA" />
  

<meta name="author" content="Máximo Sangiácomo" />


<meta name="date" content="2022-02-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logit.html"/>
<link rel="next" href="cluster.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="r4ds.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Descripcion del curso</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduccion a R</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#primeros-pasos"><i class="fa fa-check"></i><b>1.1</b> Primeros pasos</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#busacar-ayuda"><i class="fa fa-check"></i><b>1.2</b> Busacar ayuda</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#tipos-de-datos"><i class="fa fa-check"></i><b>1.3</b> Tipos de datos</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#limpieza-de-memoria"><i class="fa fa-check"></i><b>1.4</b> Limpieza de memoria</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#asignación-de-valores"><i class="fa fa-check"></i><b>1.5</b> Asignación de valores</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#operadores-aritméticos"><i class="fa fa-check"></i><b>1.6</b> Operadores aritméticos</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#operadores-relacionales"><i class="fa fa-check"></i><b>1.7</b> Operadores relacionales</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#operadores-lógicos"><i class="fa fa-check"></i><b>1.8</b> Operadores lógicos</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#vectores"><i class="fa fa-check"></i><b>1.9</b> Vectores</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#secuencias"><i class="fa fa-check"></i><b>1.10</b> Secuencias</a></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#factores"><i class="fa fa-check"></i><b>1.11</b> Factores</a></li>
<li class="chapter" data-level="1.12" data-path="intro.html"><a href="intro.html#matrices"><i class="fa fa-check"></i><b>1.12</b> Matrices</a></li>
<li class="chapter" data-level="1.13" data-path="intro.html"><a href="intro.html#listas"><i class="fa fa-check"></i><b>1.13</b> Listas</a></li>
<li class="chapter" data-level="1.14" data-path="intro.html"><a href="intro.html#data-frames"><i class="fa fa-check"></i><b>1.14</b> Data frames</a></li>
<li class="chapter" data-level="1.15" data-path="intro.html"><a href="intro.html#r-base"><i class="fa fa-check"></i><b>1.15</b> R base</a></li>
<li class="chapter" data-level="1.16" data-path="intro.html"><a href="intro.html#apply-y-tapply"><i class="fa fa-check"></i><b>1.16</b> Apply y tapply</a></li>
<li class="chapter" data-level="1.17" data-path="intro.html"><a href="intro.html#map"><i class="fa fa-check"></i><b>1.17</b> Map</a></li>
<li class="chapter" data-level="1.18" data-path="intro.html"><a href="intro.html#loops"><i class="fa fa-check"></i><b>1.18</b> Loops</a></li>
<li class="chapter" data-level="1.19" data-path="intro.html"><a href="intro.html#condicionales"><i class="fa fa-check"></i><b>1.19</b> Condicionales</a></li>
<li class="chapter" data-level="1.20" data-path="intro.html"><a href="intro.html#funciones"><i class="fa fa-check"></i><b>1.20</b> Funciones</a><ul>
<li class="chapter" data-level="1.20.1" data-path="intro.html"><a href="intro.html#output-más-de-un-resultado"><i class="fa fa-check"></i><b>1.20.1</b> Output más de un resultado</a></li>
<li class="chapter" data-level="1.20.2" data-path="intro.html"><a href="intro.html#argumentos-con-valores-default"><i class="fa fa-check"></i><b>1.20.2</b> Argumentos con valores default</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bd.html"><a href="bd.html"><i class="fa fa-check"></i><b>2</b> Base de datos</a><ul>
<li class="chapter" data-level="2.1" data-path="bd.html"><a href="bd.html#directorio-de-trabajo"><i class="fa fa-check"></i><b>2.1</b> Directorio de trabajo</a></li>
<li class="chapter" data-level="2.2" data-path="bd.html"><a href="bd.html#cargar-datos"><i class="fa fa-check"></i><b>2.2</b> Cargar datos</a><ul>
<li class="chapter" data-level="2.2.1" data-path="bd.html"><a href="bd.html#ingrasar-datos-con-tidyverse"><i class="fa fa-check"></i><b>2.2.1</b> Ingrasar datos con <code>tidyverse</code></a></li>
<li class="chapter" data-level="2.2.2" data-path="bd.html"><a href="bd.html#bases-de-stata"><i class="fa fa-check"></i><b>2.2.2</b> Bases de Stata</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bd.html"><a href="bd.html#problemas-de-imputación"><i class="fa fa-check"></i><b>2.3</b> Problemas de imputación</a></li>
<li class="chapter" data-level="2.4" data-path="bd.html"><a href="bd.html#exportar-datos"><i class="fa fa-check"></i><b>2.4</b> Exportar datos</a></li>
<li class="chapter" data-level="2.5" data-path="bd.html"><a href="bd.html#variables"><i class="fa fa-check"></i><b>2.5</b> Variables</a></li>
<li class="chapter" data-level="2.6" data-path="bd.html"><a href="bd.html#merge"><i class="fa fa-check"></i><b>2.6</b> Merge</a></li>
<li class="chapter" data-level="2.7" data-path="bd.html"><a href="bd.html#group_by-mutate"><i class="fa fa-check"></i><b>2.7</b> group_by, mutate</a></li>
<li class="chapter" data-level="2.8" data-path="bd.html"><a href="bd.html#guardar-datos"><i class="fa fa-check"></i><b>2.8</b> Guardar datos</a></li>
<li class="chapter" data-level="2.9" data-path="bd.html"><a href="bd.html#valores-missing"><i class="fa fa-check"></i><b>2.9</b> Valores missing</a><ul>
<li class="chapter" data-level="2.9.1" data-path="bd.html"><a href="bd.html#eliminar-valores-missing"><i class="fa fa-check"></i><b>2.9.1</b> Eliminar valores missing</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="bd.html"><a href="bd.html#loop"><i class="fa fa-check"></i><b>2.10</b> Loop</a></li>
<li class="chapter" data-level="2.11" data-path="bd.html"><a href="bd.html#pivot"><i class="fa fa-check"></i><b>2.11</b> Pivot</a></li>
<li class="chapter" data-level="2.12" data-path="bd.html"><a href="bd.html#append"><i class="fa fa-check"></i><b>2.12</b> Append</a></li>
<li class="chapter" data-level="2.13" data-path="bd.html"><a href="bd.html#strings"><i class="fa fa-check"></i><b>2.13</b> Strings</a></li>
<li class="chapter" data-level="2.14" data-path="bd.html"><a href="bd.html#fechas"><i class="fa fa-check"></i><b>2.14</b> Fechas</a><ul>
<li class="chapter" data-level="2.14.1" data-path="bd.html"><a href="bd.html#year"><i class="fa fa-check"></i><b>2.14.1</b> Year</a></li>
<li class="chapter" data-level="2.14.2" data-path="bd.html"><a href="bd.html#month"><i class="fa fa-check"></i><b>2.14.2</b> Month</a></li>
<li class="chapter" data-level="2.14.3" data-path="bd.html"><a href="bd.html#day"><i class="fa fa-check"></i><b>2.14.3</b> Day</a></li>
<li class="chapter" data-level="2.14.4" data-path="bd.html"><a href="bd.html#manipulación-de-fechas"><i class="fa fa-check"></i><b>2.14.4</b> Manipulación de fechas</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="bd.html"><a href="bd.html#análisis-de-datos"><i class="fa fa-check"></i><b>2.15</b> Análisis de datos</a><ul>
<li class="chapter" data-level="2.15.1" data-path="bd.html"><a href="bd.html#tablas"><i class="fa fa-check"></i><b>2.15.1</b> Tablas</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="bd.html"><a href="bd.html#group_by-summarise"><i class="fa fa-check"></i><b>2.16</b> group_by, summarise</a></li>
<li class="chapter" data-level="2.17" data-path="bd.html"><a href="bd.html#vector-de-resultados"><i class="fa fa-check"></i><b>2.17</b> Vector de resultados</a></li>
<li class="chapter" data-level="2.18" data-path="bd.html"><a href="bd.html#gráficos"><i class="fa fa-check"></i><b>2.18</b> Gráficos</a></li>
<li class="chapter" data-level="2.19" data-path="bd.html"><a href="bd.html#ggplot"><i class="fa fa-check"></i><b>2.19</b> GGPlot</a></li>
<li class="chapter" data-level="2.20" data-path="bd.html"><a href="bd.html#guardar-un-gráfico"><i class="fa fa-check"></i><b>2.20</b> Guardar un gráfico</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conceptos.html"><a href="conceptos.html"><i class="fa fa-check"></i><b>3</b> Conceptos generales</a><ul>
<li class="chapter" data-level="3.1" data-path="conceptos.html"><a href="conceptos.html#estimacion"><i class="fa fa-check"></i><b>3.1</b> Estimacion</a></li>
<li class="chapter" data-level="3.2" data-path="conceptos.html"><a href="conceptos.html#prediccion"><i class="fa fa-check"></i><b>3.2</b> Prediccion</a></li>
<li class="chapter" data-level="3.3" data-path="conceptos.html"><a href="conceptos.html#inferencia"><i class="fa fa-check"></i><b>3.3</b> Inferencia</a></li>
<li class="chapter" data-level="3.4" data-path="conceptos.html"><a href="conceptos.html#metodos-parametricos"><i class="fa fa-check"></i><b>3.4</b> Metodos parametricos</a></li>
<li class="chapter" data-level="3.5" data-path="conceptos.html"><a href="conceptos.html#metodos-no-parametricos"><i class="fa fa-check"></i><b>3.5</b> Metodos no parametricos</a></li>
<li class="chapter" data-level="3.6" data-path="conceptos.html"><a href="conceptos.html#evaluacion-de-la-precision-del-modelo"><i class="fa fa-check"></i><b>3.6</b> Evaluacion de la precision del modelo</a><ul>
<li class="chapter" data-level="3.6.1" data-path="conceptos.html"><a href="conceptos.html#calidad-del-ajuste"><i class="fa fa-check"></i><b>3.6.1</b> Calidad del ajuste</a></li>
<li class="chapter" data-level="3.6.2" data-path="conceptos.html"><a href="conceptos.html#trade-off-sesgo-varianza"><i class="fa fa-check"></i><b>3.6.2</b> Trade-off Sesgo-Varianza</a></li>
<li class="chapter" data-level="3.6.3" data-path="conceptos.html"><a href="conceptos.html#clasificacion"><i class="fa fa-check"></i><b>3.6.3</b> Clasificacion</a></li>
<li class="chapter" data-level="3.6.4" data-path="conceptos.html"><a href="conceptos.html#matriz-de-confusion"><i class="fa fa-check"></i><b>3.6.4</b> Matriz de confusion</a></li>
<li class="chapter" data-level="3.6.5" data-path="conceptos.html"><a href="conceptos.html#curva-roc"><i class="fa fa-check"></i><b>3.6.5</b> Curva ROC</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="conceptos.html"><a href="conceptos.html#cross-validation"><i class="fa fa-check"></i><b>3.7</b> Cross Validation</a></li>
<li class="chapter" data-level="3.8" data-path="conceptos.html"><a href="conceptos.html#bootstrap"><i class="fa fa-check"></i><b>3.8</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mco.html"><a href="mco.html"><i class="fa fa-check"></i><b>4</b> Regresion lineal</a><ul>
<li class="chapter" data-level="4.1" data-path="mco.html"><a href="mco.html#aplicacion-practica"><i class="fa fa-check"></i><b>4.1</b> Aplicacion practica</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>5</b> Lasso</a><ul>
<li class="chapter" data-level="5.1" data-path="lasso.html"><a href="lasso.html#aplicacion-practica-1"><i class="fa fa-check"></i><b>5.1</b> Aplicacion practica</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="logit.html"><a href="logit.html"><i class="fa fa-check"></i><b>6</b> Logit</a><ul>
<li class="chapter" data-level="6.1" data-path="logit.html"><a href="logit.html#aplicacion-practica-2"><i class="fa fa-check"></i><b>6.1</b> Aplicacion practica</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="arboles.html"><a href="arboles.html"><i class="fa fa-check"></i><b>7</b> Arboles de decision</a><ul>
<li class="chapter" data-level="7.1" data-path="arboles.html"><a href="arboles.html#bagging"><i class="fa fa-check"></i><b>7.1</b> Bagging</a></li>
<li class="chapter" data-level="7.2" data-path="arboles.html"><a href="arboles.html#random-forest"><i class="fa fa-check"></i><b>7.2</b> Random Forest</a></li>
<li class="chapter" data-level="7.3" data-path="arboles.html"><a href="arboles.html#boosting"><i class="fa fa-check"></i><b>7.3</b> Boosting</a><ul>
<li class="chapter" data-level="7.3.1" data-path="arboles.html"><a href="arboles.html#ada-boost"><i class="fa fa-check"></i><b>7.3.1</b> Ada Boost</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="arboles.html"><a href="arboles.html#aplicacion-practica-3"><i class="fa fa-check"></i><b>7.4</b> Aplicacion practica</a><ul>
<li class="chapter" data-level="7.4.1" data-path="arboles.html"><a href="arboles.html#arboles-de-regresion"><i class="fa fa-check"></i><b>7.4.1</b> Arboles de regresion</a></li>
<li class="chapter" data-level="7.4.2" data-path="arboles.html"><a href="arboles.html#bagging-y-random-forests"><i class="fa fa-check"></i><b>7.4.2</b> Bagging y Random Forests</a></li>
<li class="chapter" data-level="7.4.3" data-path="arboles.html"><a href="arboles.html#boosting-1"><i class="fa fa-check"></i><b>7.4.3</b> Boosting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="cluster.html"><a href="cluster.html"><i class="fa fa-check"></i><b>8</b> Analisis de clusters</a><ul>
<li class="chapter" data-level="8.1" data-path="cluster.html"><a href="cluster.html#k-means-clustering"><i class="fa fa-check"></i><b>8.1</b> K-Means Clustering</a></li>
<li class="chapter" data-level="8.2" data-path="cluster.html"><a href="cluster.html#aplicacion-practica-4"><i class="fa fa-check"></i><b>8.2</b> Aplicacion practica</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="arboles" class="section level1">
<h1><span class="header-section-number">Capítulo 7</span> Arboles de decision</h1>
<p>Los métodos basados en árboles para regresión y clasificación estratifican o segmentan el espacio predictor en varias regiones. Para hacer una predicción para una observación dada normalmente utiliza el valor de respuesta promedio de las observaciones de la base de entrenamiento en la región a la que pertenece. En el caso de clasificación se asigna a la categoría mayoritaria dentro del nodo terminal.</p>
<p><em>Classification and Regression Tree</em></p>
<p>En el caso de árboles de regresión, si <span class="math inline">\(Y\)</span> es la respuesta y <span class="math inline">\(X_1\)</span> y <span class="math inline">\(X_2\)</span> los <em>inputs</em> se parte el espacio <span class="math inline">\((X_1, X_2)\)</span> en dos regiones, en base a una sola variable (partición horizontal o vertical). Dentro de cada región proponemos como predicción la media muestral de <span class="math inline">\(Y\)</span> en cada región.</p>
<p>Se busca elegir la variable y el punto de partición de manera óptima (mejor ajuste global). Es computacionalmente inviable considerar cada posible partición del espacio de atributos en <span class="math inline">\(J\)</span> regiones. Por lo tanto, toma un enfoque <em>top-down</em>, <em>greedy</em> que se conoce como división binaria recursiva. El enfoque es <em>top-down</em> porque comienza en la parte superior del árbol (en cuyo punto todas las observaciones pertenecen a una sola región) y luego divide sucesivamente el espacio predictor; cada división se indica a través de dos nuevas ramas más abajo en el árbol. Es <em>greedy</em> porque en cada paso del proceso de construcción del árbol, la mejor división se hace en ese paso en particular, en lugar de mirar hacia adelante y elegir una división que conducirá a un mejor árbol en algún paso futuro.</p>
<p>El panel izquierdo de la Figura <a href="arboles.html#fig:tree">7.1</a> muestra un árbol de regresión para predecir el logaritmo del salario (en miles de dólares) de un jugador de béisbol, basado en la cantidad de años que ha jugado en las ligas mayores y la cantidad de <em>hits</em> que hizo en el año anterior. En un un nodo interno dado, la etiqueta (de la forma <span class="math inline">\(X_j &lt; t_k\)</span>) indica la rama izquierda que sale de esa división, y la rama de la derecha corresponde a <span class="math inline">\(X_j \ge t_k\)</span>. Por ejemplo, la división en la parte superior del árbol da como resultado dos ramas grandes. El la rama izquierda corresponde a <code>Years &lt; 4,5</code>, y la rama derecha corresponde a <code>Years &gt;= 4,5</code>.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> El árbol tiene dos nodos internos y tres nodos terminales u hojas. El número en cada hoja es la media de la variable de respuesta de las observaciones que caen allí. Por ejemplo, la predicción para el nodo terminal de la izquierda es <span class="math inline">\(e^{5,107} \times 1.000 = \$165.174\)</span>. El panel derecho la Figura <a href="arboles.html#fig:tree">7.1</a> muestra las regiones en función de <code>Years</code> y <code>Hits</code>.</p>
<div class="figure"><span style="display:block;" id="fig:tree"></span>
<img src="8Tree.png" alt="Arbol de regresión" width="80%" />
<p class="caption">
Figura 7.1: Arbol de regresión
</p>
</div>
<p>Notar:</p>
<ul>
<li><p>Cada región tiene su propio modelo.</p></li>
<li><p>Ciertas variables importan en determinadas regiones y no en otras (<em>Hits</em>).</p></li>
</ul>
<p>Dado <span class="math inline">\(Y\)</span> y <span class="math inline">\(X\)</span> un vector de <span class="math inline">\(p\)</span> variables de <span class="math inline">\(n\)</span> observaciones.
El algoritmo busca determinar cuál variable usar para la partición y que punto de esa variable usar para la partición. Si <span class="math inline">\(j\)</span> es la variable de partición y el punto de partición es <span class="math inline">\(s\)</span>, se definen los siguientes semiplanos:</p>
<p><span class="math display">\[\begin{align*}
  R_1(j,s) = &amp; {X \mid X_j &lt; s} \\
  R_2(j,s) = &amp; {X \mid X_j \ge s}
\end{align*}\]</span></p>
<p>Se trata de buscar la variable de partición <span class="math inline">\(X_j\)</span> y el punto de partición <span class="math inline">\(s\)</span> que resuelvan (minimizar el <span class="math inline">\(MSE\)</span> en cada región):</p>
<p><span class="math display" id="eq:region">\[\begin{equation}
\tag{7.1}
  \sum_{i: x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +  \sum_{i: x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2 
\end{equation}\]</span></p>
<p>Donde <span class="math inline">\(\hat{y}_{R_1}\)</span> y <span class="math inline">\(\hat{y}_{R_2}\)</span> es el promedio de la respuesta en las regiones <span class="math inline">\(1\)</span> y <span class="math inline">\(2\)</span>, respectivamente. Para cada variable de partición y punto de partición, la minimización interna se corresponde con la <strong>media</strong> dentro de cada región.</p>
<p>¿Cuándo parar de realizar divisiones?</p>
<p>Un árbol demasiado extenso sobreajusta (<em>overfit</em>) los datos. Pero dado que el proceso es secuencial y cada corte no mira lo que puede suceder después, si detengo el proceso demasiado pronto puedo perder un “gran” corte más abajo. <em>Prunning</em>: ajustar un árbol grande y luego podarlo (<em>prune</em>) usando un criterio de cost-complexity.</p>
<p><strong><em>Weakest link pruning</em></strong></p>
<p>Un subárbol <span class="math inline">\(T \in T_0\)</span> es un árbol que se obtiene colapsando los nodos terminales de otro árbol (cortando ramas).</p>
<p><em>Cost-complexity</em> del árbol <span class="math inline">\(T\)</span>:</p>
<p><span class="math display" id="eq:cc">\[\begin{equation}
\tag{7.2}
   C_{\alpha}(T) = \sum_{m=1}^{|T|} n_mQ_m(T) + \alpha[T]
\end{equation}\]</span></p>
<p>con <span class="math inline">\(Q_m(T) = \frac{1}{n_m} \sum_{x_i \in R_m} (y_i - \hat{c}_m)^2\)</span> (impureza) y <span class="math inline">\(n_m\)</span> cantidad de observaciones en cada partición. Entonces, el primer término mide el (mal) ajuste y el segundo la complejidad. Cuando <span class="math inline">\(\alpha = 0\)</span>, entonces el subárbol <span class="math inline">\(T\)</span> simplemente será igual a <span class="math inline">\(T_0\)</span>, porque entonces <a href="arboles.html#eq:cc">(7.2)</a> solo mide el error de entrenamiento. Sin embargo, a medida que <span class="math inline">\(\alpha\)</span> aumenta, hay que pagar un costo por tener un árbol con muchos nodos terminales, por lo que <a href="arboles.html#eq:cc">(7.2)</a> tenderá a minimizarse para un subárbol más pequeño.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<p><strong>Objetivo:</strong> para un <span class="math inline">\(\alpha\)</span> dado, encontrar la poda óptima que minimiza <span class="math inline">\(C_{\alpha}(T)\)</span>.</p>
<p>Mecanismo de búsqueda de <span class="math inline">\(T_{\alpha}\)</span> (poda óptima dado <span class="math inline">\(\alpha\)</span>).
Resultado: para cada <span class="math inline">\(\alpha\)</span> hay un único subárbol <span class="math inline">\(T_{\alpha}\)</span> que minimiza <span class="math inline">\(C_{\alpha}(T)\)</span>. <em>Weakest link</em>: eliminar sucesivamente las ramas que producen el mínimo incremento en <span class="math inline">\(\sum_{m=1}^{[T]} n_mQ_m(T)\)</span> (impureza). Recordar que un árbol grande aumenta la varianza, colapsamos la partición menos necesaria. Un árbol más pequeño con menos divisiones (es decir, menos regiones <span class="math inline">\(R1,...,RJ\)</span>) tiene menor varianza y es más fácil de interpretar a costa de un pequeño sesgo.</p>
<p>El proceso eventualmente colapsa en el nodo inicial, pero pasa por una sucesión de árboles, desde el más grande, hasta el más chico, por el proceso de <em>weakest link pruning</em>. El árbol óptimo <span class="math inline">\(T_{\alpha}\)</span> pertenece a esta sucesión.</p>
<p><strong><em>Classification tree</em></strong></p>
<p>Un árbol de clasificación es muy similar a un árbol de regresión, excepto que se utiliza para predecir una respuesta cualitativa en lugar de una cuantitativa. Recordar que para un árbol de regresión, la respuesta predicha para una observación esta dada por la respuesta media de las observaciones de entrenamiento que pertenecen al mismo nodo terminal. En contraste, para un árbol de clasificación, predecimos que cada observación pertenece a la clase que ocurre más comúnmente en las observaciones de entrenamiento en la región a la que pertenece. Se basa en el error de clasificación o índice de Gini (pureza), análogo a <span class="math inline">\(RSS\)</span> en un árbol de regresión.</p>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">7.1</span> Bagging</h2>
<p>Ventajas y desventajas de <span class="math inline">\(CART\)</span>:</p>
<ul>
<li><p>Forma inteligente de representar no linealidades.</p></li>
<li><p>Arriba quedan las variables más relevantes entonces es fácil de comunicar. Reproduce proceso decisorio humano.</p></li>
<li><p>Si la estructura es lineal, <span class="math inline">\(CART\)</span> no anda bien.</p></li>
<li><p>Poco robusto, variaciones en los datos modifican el resultado.</p></li>
</ul>
<p>Un método de <em>ensemble</em> es un enfoque que combina muchos modelos simples en uno único y potencialmente muy poderoso. Los modelos simples se conocen como modelos de aprendizaje débil, ya que por sí mismos pueden generar predicciones mediocres.</p>
<p><em>Bootstrap aggregation</em></p>
<p><em>Bootstrap training sets</em>: tomar como predicción el promedio de las
predicciones <em>bootstrap</em>.</p>
<p><span class="math display" id="eq:bag">\[\begin{equation}
\tag{7.3}
   \hat{f}_{bag} = \frac{1}{B} \sum_{b=1}^{B} \hat{f}^{*b}(x)
\end{equation}\]</span></p>
<p><strong>Idea</strong>: la varianza del promedio es menor que la de una predicción sola. Bajo independencia si <span class="math inline">\(V(x) = \sigma^2\)</span> entonces <span class="math inline">\(V(\overline{x}) = \frac{\sigma^2}{n}\)</span>.</p>
<p><strong>Problema</strong>: si hay un predictor fuerte, distintos árboles son muy similares entre sí y, por lo tanto, alta correlación.</p>
</div>
<div id="random-forest" class="section level2">
<h2><span class="header-section-number">7.2</span> Random Forest</h2>
<p>Busca bajar la correlación entre los árboles en el <em>bootstrap</em>. Al igual que en <em>bagging</em>, construye una serie de árboles de decisión en muestras de entrenamiento <em>bootstrap</em>. Pero al construir estos árboles de decisión, cada vez que se considera una división en un árbol, se elige como candidatos de división una muestra aleatoria de <span class="math inline">\(m\)</span> predictores del conjunto completo de <span class="math inline">\(p\)</span> predictores (<span class="math inline">\(m &lt; p\)</span>).</p>
</div>
<div id="boosting" class="section level2">
<h2><span class="header-section-number">7.3</span> Boosting</h2>
<p><em>Boosting</em> funciona de manera similar a <em>bagging</em>, excepto que los árboles van crecido secuencialmente: cada árbol crece usando información de árboles elaborados previamente. <em>Boosting</em> no implica un muestreo <em>bootstrap</em>; en cambio cada árbol se ajusta a una versión modificada de la base de datos original.</p>
<p><em>Weak classifier</em>: clasificador marginalmente mejor que tirar una moneda. Tasa de error apenas mejor que <span class="math inline">\(0,5\)</span>. Por ejemplo, <span class="math inline">\(CART\)</span> con pocas ramas (<em>stump</em>, clasificador con dos ramas).</p>
<p><em>Boosting</em>: promedio ponderado de una sucesión de clasificadores débiles. Notable mejora.</p>
<p><strong>Definiciones</strong></p>
<p><span class="math inline">\(y \in -1,1\)</span></p>
<p>Clasificador = <span class="math inline">\(\hat{y} = G(X)\)</span></p>
<p>Error de predicción = <span class="math inline">\(\frac{1}{N} \sum_{i=1}^{N}I(y_i \neq G(x_i))\)</span></p>
<div id="ada-boost" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Ada Boost</h3>
<ol style="list-style-type: decimal">
<li><p>Comienza con con pesos <span class="math inline">\(w_i = \frac{1}{N}\)</span></p></li>
<li><p>Para <span class="math inline">\(m = 1,...,M\)</span>:</p></li>
</ol>
<ul>
<li><p>Calcula una predicción</p></li>
<li><p>Calcula el error de predicción agregado</p></li>
<li><p>Calcula <span class="math inline">\(\alpha_m = ln[\frac{1 - err_m}{err_m}]\)</span></p></li>
<li><p>Actualiza los ponderadores <span class="math inline">\(w_i\)</span> <span class="math inline">\(\leftarrow\)</span> <span class="math inline">\(w_ic_i\)</span></p></li>
</ul>
<p>con <span class="math inline">\(c_i =\)</span> exp <span class="math inline">\([\alpha_m \underbrace{I(y_i \neq G(x_i))}_{{0/1}}]\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li><em>Output</em>: <span class="math inline">\(G(x) =\)</span> sgn <span class="math inline">\([\sum_{m=1}^{M} \alpha_m G_m(x)]\)</span> (signo del promedio).</li>
</ol>
<p>Si <span class="math inline">\(i\)</span> estuvo correctamente predicha, <span class="math inline">\(c_i = 1\)</span>, entonces no hay ajuste. Caso contrario, <span class="math inline">\(c_i =\)</span> exp<span class="math inline">\((\alpha_m) = \frac{1 - err_m}{err_m} &gt; 1\)</span>. Notar que si siempre predigo la clase mayoritaria la tasa de error nunca puede ser mayor al <span class="math inline">\(50\%\)</span> y por eso la expresión anterior es mayor a <span class="math inline">\(1\)</span>.</p>
<p>En cada paso el método da más importancia relativa a las observaciones mal predichas. <strong>Paso final</strong>: promedio ponderado de predicciones en cada paso.</p>
<div class="figure"><span style="display:block;" id="fig:boost"></span>
<img src="Boosting.png" alt="Ada boost" width="80%" />
<p class="caption">
Figura 7.2: Ada boost
</p>
</div>
</div>
</div>
<div id="aplicacion-practica-3" class="section level2">
<h2><span class="header-section-number">7.4</span> Aplicacion practica</h2>
<p>La biblioteca <code>tree</code> se utiliza para construir árboles de clasificación y regresión.</p>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb415-1" title="1"><span class="kw">library</span>(tree)</a></code></pre></div>
<p>Primero usamos árboles de clasificación para analizar la base de datos <code>Carseats</code>. <code>Sales</code> es una variable continua, por lo que comenzamos recodificándola como una variable binaria. Usamos la función <code>ifelse()</code> para crear una variable, llamada <code>High</code>, que toma un valor de <code>Yes</code> si la variable <code>Sales</code> excede <span class="math inline">\(8\)</span> y toma un valor de <code>No</code> en caso contrario.</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb416-1" title="1"><span class="kw">library</span>(ISLR2)</a>
<a class="sourceLine" id="cb416-2" title="2"><span class="kw">attach</span>(Carseats)</a>
<a class="sourceLine" id="cb416-3" title="3">High &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">ifelse</span>(Sales <span class="op">&lt;=</span><span class="st"> </span><span class="dv">8</span>, <span class="st">&#39;No&#39;</span>, <span class="st">&#39;Yes&#39;</span>))</a></code></pre></div>
<p>Finalmente, usamos la función <code>data.frame()</code> para unir <code>High</code> con el resto de los datos de <code>Carseats</code>.</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb417-1" title="1">Carseats &lt;-<span class="st"> </span><span class="kw">data.frame</span>(Carseats, High)</a></code></pre></div>
<p>Ahora usamos la función <code>tree()</code> para ajustar un árbol de clasificación con el fin de predecir <code>High</code> usando todas las variables excepto <code>Sales</code>. La sintaxis de la función <code>tree()</code> es bastante similar a la de la función <code>lm()</code>.</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb418-1" title="1">tree.carseats &lt;-<span class="st"> </span><span class="kw">tree</span>(High <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>Sales, Carseats)</a>
<a class="sourceLine" id="cb418-2" title="2"><span class="kw">summary</span>(tree.carseats)</a></code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = High ~ . - Sales, data = Carseats)
## Variables actually used in tree construction:
## [1] &quot;ShelveLoc&quot;   &quot;Price&quot;       &quot;Income&quot;      &quot;CompPrice&quot;   &quot;Population&quot; 
## [6] &quot;Advertising&quot; &quot;Age&quot;         &quot;US&quot;         
## Number of terminal nodes:  27 
## Residual mean deviance:  0.4575 = 170.7 / 373 
## Misclassification error rate: 0.09 = 36 / 400</code></pre>
<p>Vemos que la tasa de error de entrenamiento es <span class="math inline">\(9\%\)</span>. Para árboles de clasificación, la desviación reportada en la salida de <code>summary()</code> es
dada por:</p>
<p><span class="math display">\[
-2 \sum_m \sum_k n_{mk} \log \hat{p}_{mk},
\]</span></p>
<p>donde <span class="math inline">\(n_{mk}\)</span> es el número de observaciones en el nodo terminal <span class="math inline">\(m\)</span> que pertenecen a la clase <span class="math inline">\(k\)</span>. Una desviación pequeña indica un árbol que proporciona un buen ajuste a los datos (de entrenamiento). La <strong>desviación media residual</strong> informada es simplemente la desviación dividida por <span class="math inline">\(n-|{T}_0|\)</span>, que en este caso es <span class="math inline">\(400-27=373\)</span>.</p>
<p>Una de las propiedades más atractivas de los árboles es que se pueden representar gráficamente. Usamos la función <code>plot()</code> para mostrar la estructura de árbol y la función <code>text()</code> para mostrar las etiquetas de los nodos. El argumento <code>pretty = 0</code> indica a <code>R</code> que incluya los nombres de categoría para cualquier predictor cualitativo, en lugar de simplemente mostrar una letra para cada categoría.</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb420-1" title="1"><span class="kw">plot</span>(tree.carseats)</a>
<a class="sourceLine" id="cb420-2" title="2"><span class="kw">text</span>(tree.carseats, <span class="dt">pretty =</span> <span class="dv">0</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree5-1.png" width="672" /></p>
<p>La variable más importante para <code>Sales</code> parece ser la ubicación de las estanterías, ya que la primera rama diferencia las ubicaciones <code>Good</code> de las ubicaciones <code>Bad</code> y <code>Medium</code>.</p>
<p>Si solo escribimos el nombre del objeto del árbol, <code>R</code> imprime la salida correspondiente a cada rama del árbol. <code>R</code> muestra el criterio de división (ej. <code>Price &lt; 92,5</code>), el número de observaciones en esa rama, el desvío, la predicción general para la rama (<code>Yes</code> o <code>No</code>) y la fracción de observaciones en esa rama que toma valores de <code>Yes</code> y <code>No</code>. Las ramas que conducen a los nodos terminales se indican con asteriscos.</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb421-1" title="1">tree.carseats</a></code></pre></div>
<pre><code>## node), split, n, deviance, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 400 541.500 No ( 0.59000 0.41000 )  
##     2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 )  
##       4) Price &lt; 92.5 46  56.530 Yes ( 0.30435 0.69565 )  
##         8) Income &lt; 57 10  12.220 No ( 0.70000 0.30000 )  
##          16) CompPrice &lt; 110.5 5   0.000 No ( 1.00000 0.00000 ) *
##          17) CompPrice &gt; 110.5 5   6.730 Yes ( 0.40000 0.60000 ) *
##         9) Income &gt; 57 36  35.470 Yes ( 0.19444 0.80556 )  
##          18) Population &lt; 207.5 16  21.170 Yes ( 0.37500 0.62500 ) *
##          19) Population &gt; 207.5 20   7.941 Yes ( 0.05000 0.95000 ) *
##       5) Price &gt; 92.5 269 299.800 No ( 0.75465 0.24535 )  
##        10) Advertising &lt; 13.5 224 213.200 No ( 0.81696 0.18304 )  
##          20) CompPrice &lt; 124.5 96  44.890 No ( 0.93750 0.06250 )  
##            40) Price &lt; 106.5 38  33.150 No ( 0.84211 0.15789 )  
##              80) Population &lt; 177 12  16.300 No ( 0.58333 0.41667 )  
##               160) Income &lt; 60.5 6   0.000 No ( 1.00000 0.00000 ) *
##               161) Income &gt; 60.5 6   5.407 Yes ( 0.16667 0.83333 ) *
##              81) Population &gt; 177 26   8.477 No ( 0.96154 0.03846 ) *
##            41) Price &gt; 106.5 58   0.000 No ( 1.00000 0.00000 ) *
##          21) CompPrice &gt; 124.5 128 150.200 No ( 0.72656 0.27344 )  
##            42) Price &lt; 122.5 51  70.680 Yes ( 0.49020 0.50980 )  
##              84) ShelveLoc: Bad 11   6.702 No ( 0.90909 0.09091 ) *
##              85) ShelveLoc: Medium 40  52.930 Yes ( 0.37500 0.62500 )  
##               170) Price &lt; 109.5 16   7.481 Yes ( 0.06250 0.93750 ) *
##               171) Price &gt; 109.5 24  32.600 No ( 0.58333 0.41667 )  
##                 342) Age &lt; 49.5 13  16.050 Yes ( 0.30769 0.69231 ) *
##                 343) Age &gt; 49.5 11   6.702 No ( 0.90909 0.09091 ) *
##            43) Price &gt; 122.5 77  55.540 No ( 0.88312 0.11688 )  
##              86) CompPrice &lt; 147.5 58  17.400 No ( 0.96552 0.03448 ) *
##              87) CompPrice &gt; 147.5 19  25.010 No ( 0.63158 0.36842 )  
##               174) Price &lt; 147 12  16.300 Yes ( 0.41667 0.58333 )  
##                 348) CompPrice &lt; 152.5 7   5.742 Yes ( 0.14286 0.85714 ) *
##                 349) CompPrice &gt; 152.5 5   5.004 No ( 0.80000 0.20000 ) *
##               175) Price &gt; 147 7   0.000 No ( 1.00000 0.00000 ) *
##        11) Advertising &gt; 13.5 45  61.830 Yes ( 0.44444 0.55556 )  
##          22) Age &lt; 54.5 25  25.020 Yes ( 0.20000 0.80000 )  
##            44) CompPrice &lt; 130.5 14  18.250 Yes ( 0.35714 0.64286 )  
##              88) Income &lt; 100 9  12.370 No ( 0.55556 0.44444 ) *
##              89) Income &gt; 100 5   0.000 Yes ( 0.00000 1.00000 ) *
##            45) CompPrice &gt; 130.5 11   0.000 Yes ( 0.00000 1.00000 ) *
##          23) Age &gt; 54.5 20  22.490 No ( 0.75000 0.25000 )  
##            46) CompPrice &lt; 122.5 10   0.000 No ( 1.00000 0.00000 ) *
##            47) CompPrice &gt; 122.5 10  13.860 No ( 0.50000 0.50000 )  
##              94) Price &lt; 125 5   0.000 Yes ( 0.00000 1.00000 ) *
##              95) Price &gt; 125 5   0.000 No ( 1.00000 0.00000 ) *
##     3) ShelveLoc: Good 85  90.330 Yes ( 0.22353 0.77647 )  
##       6) Price &lt; 135 68  49.260 Yes ( 0.11765 0.88235 )  
##        12) US: No 17  22.070 Yes ( 0.35294 0.64706 )  
##          24) Price &lt; 109 8   0.000 Yes ( 0.00000 1.00000 ) *
##          25) Price &gt; 109 9  11.460 No ( 0.66667 0.33333 ) *
##        13) US: Yes 51  16.880 Yes ( 0.03922 0.96078 ) *
##       7) Price &gt; 135 17  22.070 No ( 0.64706 0.35294 )  
##        14) Income &lt; 46 6   0.000 No ( 1.00000 0.00000 ) *
##        15) Income &gt; 46 11  15.160 Yes ( 0.45455 0.54545 ) *</code></pre>
<p>Para evaluar correctamente la <em>performance</em> de un árbol de clasificación
con estos datos, debemos estimar el error de <em>test</em> en lugar de calcular el error de entrenamiento. Dividimos las observaciones en la base de entrenamiento y una base de <em>test</em>, se construye el árbol usando la base de entrenamiento, y se evalúa su desempeño en los datos en <em>test</em>. La función <code>predict()</code> se usa para este propósito. En el caso de un árbol de clasificación, el argumento <code>type = "class"</code> instruye a <code>R</code> para devolver una predicción de clase. Este modelo produce predicciones correctas en alrededor del <span class="math inline">\(77\%\)</span> de los casos en la base de datos de <em>test</em>.</p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb423-1" title="1"><span class="kw">set.seed</span>(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb423-2" title="2">train &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Carseats), <span class="dv">200</span>)</a>
<a class="sourceLine" id="cb423-3" title="3">Carseats.test &lt;-<span class="st"> </span>Carseats[<span class="op">-</span>train, ]</a>
<a class="sourceLine" id="cb423-4" title="4">High.test &lt;-<span class="st"> </span>High[<span class="op">-</span>train]</a>
<a class="sourceLine" id="cb423-5" title="5">tree.carseats &lt;-<span class="st"> </span><span class="kw">tree</span>(High <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>Sales, Carseats,</a>
<a class="sourceLine" id="cb423-6" title="6">    <span class="dt">subset =</span> train)</a>
<a class="sourceLine" id="cb423-7" title="7">tree.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(tree.carseats, Carseats.test,</a>
<a class="sourceLine" id="cb423-8" title="8">    <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb423-9" title="9"><span class="kw">table</span>(tree.pred, High.test)</a></code></pre></div>
<pre><code>##          High.test
## tree.pred  No Yes
##       No  104  33
##       Yes  13  50</code></pre>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb425-1" title="1">(<span class="dv">104</span> <span class="op">+</span><span class="st"> </span><span class="dv">50</span>) <span class="op">/</span><span class="st"> </span><span class="dv">200</span></a></code></pre></div>
<pre><code>## [1] 0.77</code></pre>
<p>A continuación, consideramos si podar el árbol podría llevar a mejores resultados. La función <code>cv.tree()</code> realiza una <em>cross-validation</em> para determinar el nivel óptimo de complejidad del árbol; se utiliza <em>cost complexity pruning</em> para seleccionar una secuencia de árboles.</p>
<p>Usamos el argumento <code>FUN = prune.misclass</code> para indicar que queremos que la tasa de error de clasificación guíe el proceso de <em>cross-validation</em> y poda, en lugar del valor predeterminado para la función <code>cv.tree()</code>, que es el desvío. La función <code>cv.tree()</code> reporta el número de nodos terminales de cada árbol considerado (<code>size</code>), así como la tasa de error correspondiente y el valor del parámetro <em>cost complexity</em> utilizado (<code>k</code>, que corresponde a $ $ en <a href="arboles.html#eq:cc">(7.2)</a>).</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb427-1" title="1"><span class="kw">set.seed</span>(<span class="dv">7</span>)</a>
<a class="sourceLine" id="cb427-2" title="2">cv.carseats &lt;-<span class="st"> </span><span class="kw">cv.tree</span>(tree.carseats, <span class="dt">FUN =</span> prune.misclass)</a>
<a class="sourceLine" id="cb427-3" title="3"><span class="kw">names</span>(cv.carseats)</a></code></pre></div>
<pre><code>## [1] &quot;size&quot;   &quot;dev&quot;    &quot;k&quot;      &quot;method&quot;</code></pre>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb429-1" title="1">cv.carseats</a></code></pre></div>
<pre><code>## $size
## [1] 21 19 14  9  8  5  3  2  1
## 
## $dev
## [1] 75 75 75 74 82 83 83 85 82
## 
## $k
## [1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0
## 
## $method
## [1] &quot;misclass&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;</code></pre>
<p>A pesar de su nombre, <code>dev</code> corresponde al número de errores de <em>cross-validation</em>. El árbol con 9 nodos terminales da como resultado solo 74 errores de <em>cross-validation</em>. Graficamos la tasa de error en función tanto del <code>size</code> y <code>k</code>.</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb431-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb431-2" title="2"><span class="kw">plot</span>(cv.carseats<span class="op">$</span>size, cv.carseats<span class="op">$</span>dev, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</a>
<a class="sourceLine" id="cb431-3" title="3"><span class="kw">plot</span>(cv.carseats<span class="op">$</span>k, cv.carseats<span class="op">$</span>dev, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree9-1.png" width="672" /></p>
<p>Ahora aplicamos la función <code>prune.misclass()</code> para podar el árbol y obtener el árbol de nueve nodos.</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb432-1" title="1">prune.carseats &lt;-<span class="st"> </span><span class="kw">prune.misclass</span>(tree.carseats, <span class="dt">best =</span> <span class="dv">9</span>)</a>
<a class="sourceLine" id="cb432-2" title="2"><span class="kw">plot</span>(prune.carseats)</a>
<a class="sourceLine" id="cb432-3" title="3"><span class="kw">text</span>(prune.carseats, <span class="dt">pretty =</span> <span class="dv">0</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree10-1.png" width="672" /></p>
<p>¿Qué tan bien se desempeña este árbol podado en la base de datos de <em>test</em>? Una vez más, aplicamos la función <code>predict()</code>.</p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb433-1" title="1">tree.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(prune.carseats, Carseats.test,</a>
<a class="sourceLine" id="cb433-2" title="2">    <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb433-3" title="3"><span class="kw">table</span>(tree.pred, High.test)</a></code></pre></div>
<pre><code>##          High.test
## tree.pred No Yes
##       No  97  25
##       Yes 20  58</code></pre>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb435-1" title="1">(<span class="dv">97</span> <span class="op">+</span><span class="st"> </span><span class="dv">58</span>) <span class="op">/</span><span class="st"> </span><span class="dv">200</span></a></code></pre></div>
<pre><code>## [1] 0.775</code></pre>
<p>Ahora el <span class="math inline">\(77,5\%\)</span> de las observaciones de <em>test</em> se clasifican correctamente, por lo que el proceso de poda no solo produjo un árbol más interpretable, sino que también mejoró ligeramente la precisión de la clasificación.</p>
<p>Si aumentamos el valor de <code>best</code>, obtenemos un árbol podado más grande con menor precisión de clasificación:</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb437-1" title="1">prune.carseats &lt;-<span class="st"> </span><span class="kw">prune.misclass</span>(tree.carseats, <span class="dt">best =</span> <span class="dv">14</span>)</a>
<a class="sourceLine" id="cb437-2" title="2"><span class="kw">plot</span>(prune.carseats)</a>
<a class="sourceLine" id="cb437-3" title="3"><span class="kw">text</span>(prune.carseats, <span class="dt">pretty =</span> <span class="dv">0</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree12-1.png" width="672" /></p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb438-1" title="1">tree.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(prune.carseats, Carseats.test,</a>
<a class="sourceLine" id="cb438-2" title="2">    <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb438-3" title="3"><span class="kw">table</span>(tree.pred, High.test)</a></code></pre></div>
<pre><code>##          High.test
## tree.pred  No Yes
##       No  102  31
##       Yes  15  52</code></pre>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb440-1" title="1">(<span class="dv">102</span> <span class="op">+</span><span class="st"> </span><span class="dv">52</span>) <span class="op">/</span><span class="st"> </span><span class="dv">200</span></a></code></pre></div>
<pre><code>## [1] 0.77</code></pre>
<div id="arboles-de-regresion" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Arboles de regresion</h3>
<p>Aquí ajustamos un árbol de regresión a la base de datos <code>Boston</code>. Primero, creamos una base de entrenamiento y ajustamos el árbol a esos datos.</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb442-2" title="2">train &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Boston), <span class="kw">nrow</span>(Boston) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb442-3" title="3">tree.boston &lt;-<span class="st"> </span><span class="kw">tree</span>(medv <span class="op">~</span><span class="st"> </span>., Boston, <span class="dt">subset =</span> train)</a>
<a class="sourceLine" id="cb442-4" title="4"><span class="kw">summary</span>(tree.boston)</a></code></pre></div>
<pre><code>## 
## Regression tree:
## tree(formula = medv ~ ., data = Boston, subset = train)
## Variables actually used in tree construction:
## [1] &quot;rm&quot;    &quot;lstat&quot; &quot;crim&quot;  &quot;age&quot;  
## Number of terminal nodes:  7 
## Residual mean deviance:  10.38 = 2555 / 246 
## Distribution of residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800</code></pre>
<p>Notar que la salida de <code>summary()</code> indica que solo cuatro de las variables han sido usadas en la construcción del árbol. En el contexto de un árbol de regresión, el desvío es simplemente la suma de los errores al cuadrado del árbol. Ahora graficamos el árbol.</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" title="1"><span class="kw">plot</span>(tree.boston)</a>
<a class="sourceLine" id="cb444-2" title="2"><span class="kw">text</span>(tree.boston, <span class="dt">pretty =</span> <span class="dv">0</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree14-1.png" width="672" /></p>
<p>La variable <code>lstat</code> mide el porcentaje de personas con bajo nivel socioeconómico, mientras que la variable <code>rm</code> corresponde al número promedio de habitaciones. El árbol indica que valores más altos de <code>rm</code>, o valores más bajos de <code>lstat</code>, corresponden a casas más caras. Por ejemplo, el árbol predice un precio medio de vivienda de <span class="math inline">\(\$45.400\)</span> para viviendas en distritos censales en los que <code>rm &gt;= 7,553</code>.</p>
<p>Podríamos haber estimado un árbol mucho más grande, usando el argumento <code>control = tree.control(nobs = length(train), mindev = 0)</code> en la función <code>tree()</code>.</p>
<p>Ahora usamos la función <code>cv.tree()</code> para ver si podar el árbol mejorará el rendimiento.</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb445-1" title="1">cv.boston &lt;-<span class="st"> </span><span class="kw">cv.tree</span>(tree.boston)</a>
<a class="sourceLine" id="cb445-2" title="2"><span class="kw">plot</span>(cv.boston<span class="op">$</span>size, cv.boston<span class="op">$</span>dev, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree15-1.png" width="672" /></p>
<p>En este caso, el árbol más complejo se selecciona mediante <em>cross-validation</em>. Sin embargo, si deseamos podar el árbol, podemos hacerlo de la siguiente manera, usando la función <code>prune.tree()</code>:</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb446-1" title="1">prune.boston &lt;-<span class="st"> </span><span class="kw">prune.tree</span>(tree.boston, <span class="dt">best =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb446-2" title="2"><span class="kw">plot</span>(prune.boston)</a>
<a class="sourceLine" id="cb446-3" title="3"><span class="kw">text</span>(prune.boston, <span class="dt">pretty =</span> <span class="dv">0</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree16-1.png" width="672" /></p>
<p>De acuerdo con los resultados de la <em>cross-validation</em>, usamos el árbol no podado para hacer predicciones en la base de <em>test</em>.</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb447-1" title="1">yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(tree.boston, <span class="dt">newdata =</span> Boston[<span class="op">-</span>train, ])</a>
<a class="sourceLine" id="cb447-2" title="2">boston.test &lt;-<span class="st"> </span>Boston[<span class="op">-</span>train, <span class="st">&quot;medv&quot;</span>]</a>
<a class="sourceLine" id="cb447-3" title="3"><span class="kw">plot</span>(yhat, boston.test)</a>
<a class="sourceLine" id="cb447-4" title="4"><span class="kw">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree17-1.png" width="672" /></p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb448-1" title="1"><span class="kw">mean</span>((yhat <span class="op">-</span><span class="st"> </span>boston.test)<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 35.28688</code></pre>
<p>En otras palabras, el <span class="math inline">\(MSE\)</span> de la la base de <em>test</em> asociado con el árbol de regresión es de <span class="math inline">\(35,29\)</span>. Por lo tanto, la raíz cuadrada del <span class="math inline">\(MSE\)</span> es de alrededor de <span class="math inline">\(5,941\)</span>, lo que indica que este modelo lleva a predicciones de <em>test</em> que están (en promedio) dentro de aproximadamente <span class="math inline">\(\$5.941\)</span> del valor medio real de la vivienda para el distrito censal.</p>
</div>
<div id="bagging-y-random-forests" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Bagging y Random Forests</h3>
<p>Aquí aplicamos <em>bagging</em> y <em>random forests</em> a los datos de <code>Boston</code>, usando el paquete <code>randomForest</code> en <code>R</code>. Los resultados exactos obtenidos en esta sección pueden depender de la versión de <code>R</code> y la versión del paquete <code>randomForest</code> instalado. Recuerde que el <em>bagging</em> es simplemente un caso especial de un <em>random forests</em> con
<span class="math inline">\(m = p\)</span>. Por lo tanto, la función <code>randomForest()</code> se puede utilizar para realizar <em>random forests</em> y <em>bagging</em>. Realizamos el <em>bagging</em> de la siguiente manera:</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb450-1" title="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb450-2" title="2"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb450-3" title="3">bag.boston &lt;-<span class="st"> </span><span class="kw">randomForest</span>(medv <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Boston,</a>
<a class="sourceLine" id="cb450-4" title="4">    <span class="dt">subset =</span> train, <span class="dt">mtry =</span> <span class="dv">12</span>, <span class="dt">importance =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb450-5" title="5">bag.boston</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 12
## 
##           Mean of squared residuals: 11.40162
##                     % Var explained: 85.17</code></pre>
<p>El argumento <code>mtry = 12</code> indica que se deben considerar todos los predictores (hay <span class="math inline">\(12\)</span> en total) para cada división del árbol; en otras palabras, se debe realizar el <em>bagging</em>.
¿Qué tan bien se desempeña este modelo en la base de <em>test</em>?</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb452-1" title="1">yhat.bag &lt;-<span class="st"> </span><span class="kw">predict</span>(bag.boston, <span class="dt">newdata =</span> Boston[<span class="op">-</span>train, ])</a>
<a class="sourceLine" id="cb452-2" title="2"><span class="kw">plot</span>(yhat.bag, boston.test)</a>
<a class="sourceLine" id="cb452-3" title="3"><span class="kw">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree19-1.png" width="672" /></p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb453-1" title="1"><span class="kw">mean</span>((yhat.bag <span class="op">-</span><span class="st"> </span>boston.test)<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 23.41916</code></pre>
<p>El <span class="math inline">\(MSE\)</span> de la base de <em>test</em> asociado con el árbol de regresión <em>bagging</em> es de <span class="math inline">\(23,42\)</span>, aproximadamente dos tercios del obtenido utilizando un árbol único podado de manera óptima.
Podríamos cambiar el número de árboles elaborados por <code>randomForest()</code> usando el argumento <code>ntree</code>:</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb455-1" title="1">bag.boston &lt;-<span class="st"> </span><span class="kw">randomForest</span>(medv <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Boston,</a>
<a class="sourceLine" id="cb455-2" title="2">    <span class="dt">subset =</span> train, <span class="dt">mtry =</span> <span class="dv">12</span>, <span class="dt">ntree =</span> <span class="dv">25</span>)</a>
<a class="sourceLine" id="cb455-3" title="3">yhat.bag &lt;-<span class="st"> </span><span class="kw">predict</span>(bag.boston, <span class="dt">newdata =</span> Boston[<span class="op">-</span>train, ])</a>
<a class="sourceLine" id="cb455-4" title="4"><span class="kw">mean</span>((yhat.bag <span class="op">-</span><span class="st"> </span>boston.test)<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 25.75055</code></pre>
<p>El estimación de un <em>random forests</em> procede exactamente de la misma manera, excepto que usamos un valor más pequeño del argumento <code>mtry</code>. Por defecto, <code>randomForest()</code> usa <span class="math inline">\(p/3\)</span> variables cuando construye un <em>random forests</em> de árboles de regresión, y <span class="math inline">\(\sqrt{p}\)</span> variables cuando construye un <em>random forests</em> de árboles de clasificación. Aquí usamos <code>mtry = 6</code>.</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb457-2" title="2">rf.boston &lt;-<span class="st"> </span><span class="kw">randomForest</span>(medv <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Boston,</a>
<a class="sourceLine" id="cb457-3" title="3">    <span class="dt">subset =</span> train, <span class="dt">mtry =</span> <span class="dv">6</span>, <span class="dt">importance =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb457-4" title="4">yhat.rf &lt;-<span class="st"> </span><span class="kw">predict</span>(rf.boston, <span class="dt">newdata =</span> Boston[<span class="op">-</span>train, ])</a>
<a class="sourceLine" id="cb457-5" title="5"><span class="kw">mean</span>((yhat.rf <span class="op">-</span><span class="st"> </span>boston.test)<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 20.06644</code></pre>
<p>El <span class="math inline">\(MSE\)</span> de la base de datos de <em>test</em> es de <span class="math inline">\(20,07\)</span>; esto indica que <em>random forests</em> produjo una mejora con respecto a <em>bagging</em> en este caso.</p>
<p>Usando la función <code>importance()</code>, podemos ver la importancia de cada variable.</p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" title="1"><span class="kw">importance</span>(rf.boston)</a></code></pre></div>
<pre><code>##           %IncMSE IncNodePurity
## crim    19.435587    1070.42307
## zn       3.091630      82.19257
## indus    6.140529     590.09536
## chas     1.370310      36.70356
## nox     13.263466     859.97091
## rm      35.094741    8270.33906
## age     15.144821     634.31220
## dis      9.163776     684.87953
## rad      4.793720      83.18719
## tax      4.410714     292.20949
## ptratio  8.612780     902.20190
## lstat   28.725343    5813.04833</code></pre>
<p>Se reportan dos medidas de importancia variable. El primero se basa en la disminución media de la precisión en las predicciones sobre las muestras <em>out of bag</em> cuando se permuta una variable determinada. La segunda es una medida de la disminución total en la impureza de los nodos que resulta de las divisiones sobre esa variable, promediada sobre todos los árboles. En el caso de los árboles de regresión, la impureza del nodo se mide por el <span class="math inline">\(RSS\)</span> de entrenamiento, y para los árboles de clasificación por el desvío. Los gráficos de estas medidas de importancia se pueden producir utilizando la función <code>varImpPlot()</code>.</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" title="1"><span class="kw">varImpPlot</span>(rf.boston)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree23-1.png" width="672" /></p>
<p>Los resultados indican que en todos los árboles considerados en el <em>random forests</em>, la riqueza de la comunidad (<code>lstat</code>) y el tamaño de la casa (<code>rm</code>) son las dos variables más importantes.</p>
</div>
<div id="boosting-1" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Boosting</h3>
<p>Aquí usamos el paquete <code>gbm</code> y la función <code>gbm()</code>, para ajustar árboles de regresión <em>boosting</em> a la base de datos <code>Boston</code>. Ejecutamos <code>gbm()</code> con la opción <code>distribution = "gaussian"</code> ya que este es un problema de regresión; si fuera un problema de clasificación binaria, usaríamos <code>distribution = "bernoulli"</code>.
El argumento <code>n.trees = 5000</code> indica que queremos árboles de <span class="math inline">\(5.000\)</span>, y la opción <code>interaction. depth = 4</code> limita la profundidad de cada árbol.</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" title="1"><span class="kw">library</span>(gbm)</a>
<a class="sourceLine" id="cb462-2" title="2"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb462-3" title="3">boost.boston &lt;-<span class="st"> </span><span class="kw">gbm</span>(medv <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Boston[train, ],</a>
<a class="sourceLine" id="cb462-4" title="4">    <span class="dt">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="dt">n.trees =</span> <span class="dv">5000</span>,</a>
<a class="sourceLine" id="cb462-5" title="5">    <span class="dt">interaction.depth =</span> <span class="dv">4</span>)</a></code></pre></div>
<p>La función <code>summary()</code> produce un gráfico y estadísticas de influencia relativa.</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb463-1" title="1"><span class="kw">summary</span>(boost.boston)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree25-1.png" width="672" /></p>
<pre><code>##             var     rel.inf
## rm           rm 44.48249588
## lstat     lstat 32.70281223
## crim       crim  4.85109954
## dis         dis  4.48693083
## nox         nox  3.75222394
## age         age  3.19769210
## ptratio ptratio  2.81354826
## tax         tax  1.54417603
## indus     indus  1.03384666
## rad         rad  0.87625748
## zn           zn  0.16220479
## chas       chas  0.09671228</code></pre>
<p>Nuevamente, vemos que <code>lstat</code> y <code>rm</code> son las variables más importantes. También podemos producir <strong>gráficos de dependencia parcial</strong> para estas dos variables. Estos gráficos ilustran el efecto marginal de las variables seleccionadas en la respuesta después de <strong>integrar</strong> las otras variables. En este caso, como cabría esperar, los precios medios de la vivienda aumentan con <code>rm</code> y disminuyen con <code>lstat</code>.</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb465-1" title="1"><span class="kw">plot</span>(boost.boston, <span class="dt">i =</span> <span class="st">&quot;rm&quot;</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree26-1.png" width="672" /></p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" title="1"><span class="kw">plot</span>(boost.boston, <span class="dt">i =</span> <span class="st">&quot;lstat&quot;</span>)</a></code></pre></div>
<p><img src="Libro_files/figure-html/tree26-2.png" width="672" /></p>
<p>Ahora usamos el modelo <em>boosting</em> para predecir <code>medv</code> en la base de <em>test</em>:</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" title="1">yhat.boost &lt;-<span class="st"> </span><span class="kw">predict</span>(boost.boston,</a>
<a class="sourceLine" id="cb467-2" title="2">    <span class="dt">newdata =</span> Boston[<span class="op">-</span>train, ], <span class="dt">n.trees =</span> <span class="dv">5000</span>)</a>
<a class="sourceLine" id="cb467-3" title="3"><span class="kw">mean</span>((yhat.boost <span class="op">-</span><span class="st"> </span>boston.test)<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 18.39057</code></pre>
<p>El <span class="math inline">\(MSE\)</span> de <em>test</em> obtenido es de <span class="math inline">\(18,39\)</span>: esto es superior al <span class="math inline">\(MSE\)</span> de <em>test</em> de <em>random forests</em> y <em>bagging</em>. Podemos realizar un <em>boosting</em> con un valor diferente del parámetro de contracción <span class="math inline">\(\lambda\)</span>. El valor predeterminado es <span class="math inline">\(0,001\)</span>, pero esto se modifica fácilmente. Aquí tomamos <span class="math inline">\(\lambda=0,2\)</span>.</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb469-1" title="1">boost.boston &lt;-<span class="st"> </span><span class="kw">gbm</span>(medv <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Boston[train, ],</a>
<a class="sourceLine" id="cb469-2" title="2">    <span class="dt">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="dt">n.trees =</span> <span class="dv">5000</span>,</a>
<a class="sourceLine" id="cb469-3" title="3">    <span class="dt">interaction.depth =</span> <span class="dv">4</span>, <span class="dt">shrinkage =</span> <span class="fl">0.2</span>, <span class="dt">verbose =</span> F)</a>
<a class="sourceLine" id="cb469-4" title="4">yhat.boost &lt;-<span class="st"> </span><span class="kw">predict</span>(boost.boston,</a>
<a class="sourceLine" id="cb469-5" title="5">    <span class="dt">newdata =</span> Boston[<span class="op">-</span>train, ], <span class="dt">n.trees =</span> <span class="dv">5000</span>)</a>
<a class="sourceLine" id="cb469-6" title="6"><span class="kw">mean</span>((yhat.boost <span class="op">-</span><span class="st"> </span>boston.test)<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 16.54778</code></pre>
<p>En este caso, usar <span class="math inline">\(\lambda=0,2\)</span> produce a un <span class="math inline">\(MSE\)</span> de prueba más bajo que <span class="math inline">\(\lambda=0,001\)</span>.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p>Al estar arriba, <code>Years</code> es la variable más importante para explicar el salario.<a href="arboles.html#fnref13" class="footnote-back">↩</a></p></li>
<li id="fn14"><p>Idea similar a <em>Lasso</em>.<a href="arboles.html#fnref14" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logit.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cluster.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-arboles.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Libro.pdf", "Libro.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
