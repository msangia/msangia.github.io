[{"path":"index.html","id":"descripcion-del-curso","chapter":"Descripcion del curso","heading":"Descripcion del curso","text":"El objetivo del curso es abordar distintas metodologías de análisis de datos desde el punto de vista teórico y práctico utilizando el programa R. Si bien, dada la extención de las clases, la cobertura de cada tema busca ser exhaustiva intenta capturar las principales intuiciones de cada método.1En los Capítulos 1 y 2 se hace una breve introducción al manejo de bases de datos en R. El Capitulo 3 se ocupa de los principales conceptos teóricos que ser utilizados en la práctica. Luego, los Capítulos 4 8 revisan distintas metodologías donde primero se presentan las principales intuiciones y después se realizan aplicaciones prácticas utilizando los ejemplos expuestos en (James et al. 2021).","code":""},{"path":"intro.html","id":"intro","chapter":"Capítulo 1 Introduccion a R","heading":"Capítulo 1 Introduccion a R","text":"La programación de rutinas en software específico para la manipulación y análisis de bases de datos permite asegurar procesos homogéneos, documentados, fácilmente auditables, modificables y que pueden ser compartidos entre diferentes usuarios. Ademas, resulta sumamente útil para realizar tareas repetitivas generando ganancias de eficiencia.La programación de rutinas en software específico para la manipulación y análisis de bases de datos permite asegurar procesos homogéneos, documentados, fácilmente auditables, modificables y que pueden ser compartidos entre diferentes usuarios. Ademas, resulta sumamente útil para realizar tareas repetitivas generando ganancias de eficiencia.Es muy importante realizar anotaciones, tanto para compartir código con otro usuario como para mi mismo en el futuro (cuales son los insumos/output, los ¿por qué? en casos específicos). Cmd/Ctrl + Shift + R para crear secciones en scripts. Sirve para navegar y ser ordenados.Es muy importante realizar anotaciones, tanto para compartir código con otro usuario como para mi mismo en el futuro (cuales son los insumos/output, los ¿por qué? en casos específicos). Cmd/Ctrl + Shift + R para crear secciones en scripts. Sirve para navegar y ser ordenados.la gloria se llega por un camino de rosas. Practicar, practicar y practicar.— Osvaldo Zubeldia.","code":""},{"path":"intro.html","id":"primeros-pasos","chapter":"Capítulo 1 Introduccion a R","heading":"1.1 Primeros pasos","text":"R es un lenguaje orientado objetos (vectores, listas, matrices). Si bien al principio puede parecer demasiado complejo, es así. De hecho, una característica destacada de R es su flexibilidad.Mientras que un software clásico muestra inmediatamente los resultados de un comando, R los almacena en un objeto, por lo que se puede realizar un análisis sin el resultado desplegado.R (el motor) puede complementarse con RStudio (tablero de instrumentos) que es una IDE (integrated development environment) para operar de manera mas amigable (editor con sintaxis y distintos espacios de trabajo). Cada uno se encuentra disponible en2:RRStudioUna vez instalados los programas se deben descargar “paquetes” que agregan funcionalidades al paquete que viene incorporado (base).Eventualmente se puede utilizar una función específica de un paquete previamente instalado sin cargar el paquete completo. Por ejemplo, si se quiere utilizar la función read_excel() del paquete readxl.","code":"\n#Descarga de programa\ninstall.packages('tidyverse')\n\n#Carga de programa antes de utilizarlo en un script\nlibrary(tidyverse)\nreadxl::read_excel()"},{"path":"intro.html","id":"busacar-ayuda","chapter":"Capítulo 1 Introduccion a R","heading":"1.2 Busacar ayuda","text":"Buscar el tab Help en la ventana de abajo la derecha de RStudioGoogle","code":"\n# Por comando\n?rm # Para poder ver la ayuda el paquete debe estar instalado\nhelp(lm)"},{"path":"intro.html","id":"tipos-de-datos","chapter":"Capítulo 1 Introduccion a R","heading":"1.3 Tipos de datos","text":"character/stringnumericfactor (variables categóricas, importante para clasificación)integerlogicaldate","code":""},{"path":"intro.html","id":"limpieza-de-memoria","chapter":"Capítulo 1 Introduccion a R","heading":"1.4 Limpieza de memoria","text":"","code":"\nrm(list = ls()) # Elimina todos los objetos en memoria\ngc() # Garbage Collection"},{"path":"intro.html","id":"asignación-de-valores","chapter":"Capítulo 1 Introduccion a R","heading":"1.5 Asignación de valores","text":"","code":"\n# nombre_objeto <- valor\nx <- 1\nx = 5\nx## [1] 5\ny = x\ny = 4"},{"path":"intro.html","id":"operadores-aritméticos","chapter":"Capítulo 1 Introduccion a R","heading":"1.6 Operadores aritméticos","text":"","code":"\ny + x## [1] 9\ny - x## [1] -1\ny * x## [1] 20\n4 / 8## [1] 0.5\n8 %% 4## [1] 0\n2**5## [1] 32\n2^5## [1] 32\nsqrt(9)## [1] 3\nlog(1)## [1] 0"},{"path":"intro.html","id":"operadores-relacionales","chapter":"Capítulo 1 Introduccion a R","heading":"1.7 Operadores relacionales","text":"","code":"\n# < <= > >= == !=\nx == 1## [1] FALSE\nx == y## [1] FALSE\ny < x## [1] TRUE"},{"path":"intro.html","id":"operadores-lógicos","chapter":"Capítulo 1 Introduccion a R","heading":"1.8 Operadores lógicos","text":"Nota: Se puede usar || (o) y && (y) para combinar múltiples expresiones lógicas. Estos operadores se llaman “cortocircuito”: tan pronto como || ve el primer VERDADERO devuelve VERDADERO sin calcular nada más. Tan pronto como && ve el primer FALSO, devuelve FALSO.","code":"\n# ! & | && ||\nTRUE## [1] TRUE\n!FALSE## [1] TRUE\n!F## [1] TRUE\nF & T## [1] FALSE\nF | T## [1] TRUE\nx == 1 & y==4## [1] FALSE\nx == 1 | y==4## [1] TRUE\n!(x > y); x <= y## [1] FALSE## [1] FALSE"},{"path":"intro.html","id":"vectores","chapter":"Capítulo 1 Introduccion a R","heading":"1.9 Vectores","text":"Una característica distintiva de los vectores es que son atómicos / homogéneos.","code":"\na = c(0,2,5,3,8) \ntypeof(a) ## [1] \"double\"\nclass(a)## [1] \"numeric\"\nb = c(\"a\",\"b\",\"c\",\"d\",\"e\")\ntypeof(b)## [1] \"character\"\nf = c(F,T,F,T,T)\ntypeof(f)## [1] \"logical\"\nc(a,b)   # coerción (transforma todo en character)##  [1] \"0\" \"2\" \"5\" \"3\" \"8\" \"a\" \"b\" \"c\" \"d\" \"e\"\ntypeof(c(a,b))  ## [1] \"character\"\nc(NA,5,2,3,4)   ## [1] NA  5  2  3  4\nc(NULL,5,2,3,4) ## [1] 5 2 3 4\nlength(a) ## [1] 5\nnames(a)  ## NULL\nnames(a) = b\na## a b c d e \n## 0 2 5 3 8\n## Subsetting / extraer \n\n# por posicion\na[3] ## c \n## 5\na[3:5]## c d e \n## 5 3 8\na[c(1,5)]## a e \n## 0 8\na[-1]## b c d e \n## 2 5 3 8\n# por nombre\na[c('a','e')]## a e \n## 0 8\n# con booleanos\na[a>=4]## c e \n## 5 8\na[!(a>=4)]## a b d \n## 0 2 3\na[a>2 & a<7]## c d \n## 5 3\na[a==0 | a==5]## a c \n## 0 5\na[a %in% c(0,5)]## a c \n## 0 5\nwhich(a<4) # muestra las posiciones que cumplen con la condición (no los valores)## a b d \n## 1 2 4"},{"path":"intro.html","id":"secuencias","chapter":"Capítulo 1 Introduccion a R","heading":"1.10 Secuencias","text":"","code":"\n# Utilizar una funcion\n# nombre_funcion(arg1 = val1, arg2 = val2, ...)\n# Los argumentos se pueden utilizar en forma implícita, quedando definido el valor por default\nd = c(1,1,2,3,4,4,5,6,8,9,7,7,0)\nunique(d)##  [1] 1 2 3 4 5 6 8 9 7 0\nrep(5,10) ##  [1] 5 5 5 5 5 5 5 5 5 5\nseq(1,20) ##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\nseq(1,20, by = 0.7)##  [1]  1.0  1.7  2.4  3.1  3.8  4.5  5.2  5.9  6.6  7.3  8.0  8.7  9.4 10.1 10.8\n## [16] 11.5 12.2 12.9 13.6 14.3 15.0 15.7 16.4 17.1 17.8 18.5 19.2 19.9\nseq_along(a)## [1] 1 2 3 4 5\n1:20##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\nset.seed(8) \nmuestra = sample(1:10,15,replace=T)\nmuestra##  [1]  4  7  2  7 10  7  1  2  3  3  6  6  4  8  4\nsample(letters,10,rep=T)##  [1] \"n\" \"i\" \"i\" \"l\" \"g\" \"h\" \"s\" \"f\" \"g\" \"b\"\n# operaciones vectorizadas\na*c(1,0,10,1,5)##  a  b  c  d  e \n##  0  0 50  3 40\na==c(0,1,5,0,0)##     a     b     c     d     e \n##  TRUE FALSE  TRUE FALSE FALSE\n1:6 + c(10,2) # reciclaje (extiende el vector mas corto)## [1] 11  4 13  6 15  8"},{"path":"intro.html","id":"factores","chapter":"Capítulo 1 Introduccion a R","heading":"1.11 Factores","text":"","code":"\n#Variables categóricas con valores limitados (importante para modelos de clasificación)\ndata = c(1,2,2,3,1,2,3,3,1,2,3,3,1)\nfdata = as.factor(data)\nfdata##  [1] 1 2 2 3 1 2 3 3 1 2 3 3 1\n## Levels: 1 2 3\n# Niveles definidos por el usuario \nfdata2 = factor(data,labels=c(\"a\",\"b\",\"c\"))\nfdata2##  [1] a b b c a b c c a b c c a\n## Levels: a b c"},{"path":"intro.html","id":"matrices","chapter":"Capítulo 1 Introduccion a R","heading":"1.12 Matrices","text":"","code":"\nmatrix(rep(3,8))##      [,1]\n## [1,]    3\n## [2,]    3\n## [3,]    3\n## [4,]    3\n## [5,]    3\n## [6,]    3\n## [7,]    3\n## [8,]    3\nmatrix(rep(3,8),nrow=4)##      [,1] [,2]\n## [1,]    3    3\n## [2,]    3    3\n## [3,]    3    3\n## [4,]    3    3\nmatrix(seq(1:12),nrow=4)##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\nmatrix(seq(1:12),nrow=4,byrow = T)##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9\n## [4,]   10   11   12\nmatrix(a, nr=2,nc=4) # completa pero avisa sobre las dimensiones## Warning in matrix(a, nr = 2, nc = 4): la longitud de los datos [5] no es un\n## submúltiplo o múltiplo del número de filas [2] en la matriz##      [,1] [,2] [,3] [,4]\n## [1,]    0    5    8    2\n## [2,]    2    3    0    5\ncbind(1:5,5:1)##      [,1] [,2]\n## [1,]    1    5\n## [2,]    2    4\n## [3,]    3    3\n## [4,]    4    2\n## [5,]    5    1\nrbind(1:10, letters[10:1])##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \n## [2,] \"j\"  \"i\"  \"h\"  \"g\"  \"f\"  \"e\"  \"d\"  \"c\"  \"b\"  \"a\"\nmat1 = matrix(muestra,nc=5)\nmat1##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    4    7    1    3    4\n## [2,]    7   10    2    6    8\n## [3,]    2    7    3    6    4\ndim(mat1) ## [1] 3 5\n# subsetting/extraer\nmat1[5]## [1] 10\nmat1[2,3]## [1] 2\nmat1[3,]## [1] 2 7 3 6 4\nmat1[,4]## [1] 3 6 6\nmat1[,4,drop=F] # para no perder la dimensión de matriz##      [,1]\n## [1,]    3\n## [2,]    6\n## [3,]    6\nmat1[c(1,3),c(1:3,5)]##      [,1] [,2] [,3] [,4]\n## [1,]    4    7    1    4\n## [2,]    2    7    3    4\nmat1[-2,-4]##      [,1] [,2] [,3] [,4]\n## [1,]    4    7    1    4\n## [2,]    2    7    3    4\nmat2 = matrix(sample(1:10,4,replace=T),nc=2)\nmat3 = matrix(sample(1:10,4,replace=T),nc=2)\nmat4 = mat2  *  mat3 # elemento a elemento\nmat5 = mat2 %*% mat3 # multiplicación de matrices\n\nmat2##      [,1] [,2]\n## [1,]    5   10\n## [2,]   10    6\nmat3##      [,1] [,2]\n## [1,]    9    8\n## [2,]   10    5\nmat4##      [,1] [,2]\n## [1,]   45   80\n## [2,]  100   30\nmat5##      [,1] [,2]\n## [1,]  145   90\n## [2,]  150  110"},{"path":"intro.html","id":"listas","chapter":"Capítulo 1 Introduccion a R","heading":"1.13 Listas","text":"Una característica distintiva de las listas es que son recursivas y heterogéneas.","code":"\n# Las listas pueden contener elementos de distinto tipo\nms <- list(\"a\", 1L, 1.5, TRUE)\nstr(ms)## List of 4\n##  $ : chr \"a\"\n##  $ : int 1\n##  $ : num 1.5\n##  $ : logi TRUE\nlista1 <- list(mat1,a,b)\nstr(lista1)## List of 3\n##  $ : int [1:3, 1:5] 4 7 2 7 10 7 1 2 3 3 ...\n##  $ : Named num [1:5] 0 2 5 3 8\n##   ..- attr(*, \"names\")= chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n##  $ : chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\nnames(lista1) = c(\"a\",\"b\",\"c\")\n\n# subsetting/extraer\nlista1$a##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    4    7    1    3    4\n## [2,]    7   10    2    6    8\n## [3,]    2    7    3    6    4\nlista1[[1]]##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    4    7    1    3    4\n## [2,]    7   10    2    6    8\n## [3,]    2    7    3    6    4\nlista1[[1]][1,1]## [1] 4\nlista1[\"a\"]## $a\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    4    7    1    3    4\n## [2,]    7   10    2    6    8\n## [3,]    2    7    3    6    4\nlista1$c[5]## [1] \"e\""},{"path":"intro.html","id":"data-frames","chapter":"Capítulo 1 Introduccion a R","heading":"1.14 Data frames","text":"","code":"\ndf1 = data.frame(mat1)\nnames(df1) = c(\"var1\",\"var2\",\"var3\",\"var4\",\"var5\")\ndf2 = data.frame(\n  v1=1:10,\n  v2=10:1,\n  v3=sample(1:10,rep=T),\n  v4=0,\n  v5=sample(letters,10,rep=T),\n  stringsAsFactors=F\n)\nstr(df2)## 'data.frame':    10 obs. of  5 variables:\n##  $ v1: int  1 2 3 4 5 6 7 8 9 10\n##  $ v2: int  10 9 8 7 6 5 4 3 2 1\n##  $ v3: int  5 3 6 5 9 10 9 10 3 7\n##  $ v4: num  0 0 0 0 0 0 0 0 0 0\n##  $ v5: chr  \"h\" \"n\" \"m\" \"e\" ...\ndim(df2)## [1] 10  5\nlength(df2)## [1] 5\n# subsetting/extraer\ndf2[,4]##  [1] 0 0 0 0 0 0 0 0 0 0\ndf2[,\"v4\"]##  [1] 0 0 0 0 0 0 0 0 0 0\ndf2$v4##  [1] 0 0 0 0 0 0 0 0 0 0\ndf2[\"v4\"]##    v4\n## 1   0\n## 2   0\n## 3   0\n## 4   0\n## 5   0\n## 6   0\n## 7   0\n## 8   0\n## 9   0\n## 10  0\ndf2$v5[1:5]## [1] \"h\" \"n\" \"m\" \"e\" \"t\"\ndf2[3:8,c(1,5)]##   v1 v5\n## 3  3  m\n## 4  4  e\n## 5  5  t\n## 6  6  o\n## 7  7  y\n## 8  8  q\n# con booleanos:\ndf2[df2$v5==\"m\",]##   v1 v2 v3 v4 v5\n## 3  3  8  6  0  m\ndf2[df2$v2>=4,\"v5\"]## [1] \"h\" \"n\" \"m\" \"e\" \"t\" \"o\" \"y\"\ndf2[df2$v2 %in% c(3,5),\"v5\"]## [1] \"o\" \"q\"\nhead(df2,5)##   v1 v2 v3 v4 v5\n## 1  1 10  5  0  h\n## 2  2  9  3  0  n\n## 3  3  8  6  0  m\n## 4  4  7  5  0  e\n## 5  5  6  9  0  t\ntail(df2)##    v1 v2 v3 v4 v5\n## 5   5  6  9  0  t\n## 6   6  5 10  0  o\n## 7   7  4  9  0  y\n## 8   8  3 10  0  q\n## 9   9  2  3  0  f\n## 10 10  1  7  0  h\n# View(df2[c(\"v4\",\"v1\")])"},{"path":"intro.html","id":"r-base","chapter":"Capítulo 1 Introduccion a R","heading":"1.15 R base","text":"Se muestran algunas funciones básicas con R base para que puedan conocerlas aunque mas adelante utilizaremos tityverse para la manipulación y transformación de datos.","code":"## [1] \"v1\" \"v2\" \"v3\" \"v4\" \"v5\" \"v7\"## [1] \"v1\" \"v2\" \"v3\" \"v4\" \"v5\"##  [1] \"a\"       \"b\"       \"d\"       \"data\"    \"df1\"     \"df2\"     \"f\"      \n##  [8] \"fdata\"   \"fdata2\"  \"lista1\"  \"mat1\"    \"mat2\"    \"mat3\"    \"mat4\"   \n## [15] \"mat5\"    \"ms\"      \"muestra\" \"x\"       \"y\""},{"path":"intro.html","id":"apply-y-tapply","chapter":"Capítulo 1 Introduccion a R","heading":"1.16 Apply y tapply","text":"","code":"\n# MARGIN 1 = FILAS\n# MARGIN 2 = COLUMNAS\napply(df1,MARGIN=1,sum)## [1] 19 33 22\nlista = list(1:5, 20, 12:9)\nlapply(lista,mean)## [[1]]\n## [1] 3\n## \n## [[2]]\n## [1] 20\n## \n## [[3]]\n## [1] 10.5\ndata(iris)\nstr(iris)## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n# Aplica una funcion a un vector en los subvectores que define otro vector. Notar que la variable Species es factor\ntapply(iris$Petal.Length, iris$Species, mean)##     setosa versicolor  virginica \n##      1.462      4.260      5.552"},{"path":"intro.html","id":"map","chapter":"Capítulo 1 Introduccion a R","heading":"1.17 Map","text":"Toma un vector como entrada, aplica una función cada pieza y luego devuelve un nuevo vector que tiene la misma longitud (y tiene los mismos nombres) que el de entrada.","code":"\npurrr::map(lista, mean)## [[1]]\n## [1] 3\n## \n## [[2]]\n## [1] 20\n## \n## [[3]]\n## [1] 10.5\n# Se puede aplicar una función anónima (sin nombre) \n# Ver como definir una función (y sus componentes) más abajo \n# Una función corta se puede usar sin llaves {}\nresultado = purrr::map(lista, function(x) x + 5) \nresultado## [[1]]\n## [1]  6  7  8  9 10\n## \n## [[2]]\n## [1] 25\n## \n## [[3]]\n## [1] 17 16 15 14"},{"path":"intro.html","id":"loops","chapter":"Capítulo 1 Introduccion a R","heading":"1.18 Loops","text":"","code":"\n# for \nfor (x in 1:5) {\n  print(x*2)\n  print(\"listo\")\n}## [1] 2\n## [1] \"listo\"\n## [1] 4\n## [1] \"listo\"\n## [1] 6\n## [1] \"listo\"\n## [1] 8\n## [1] \"listo\"\n## [1] 10\n## [1] \"listo\"\nresultado = c()\nfor (i in seq_along(a)) {\n  resultado[i] = a[i]*2\n}\nresultado## [1]  0  4 10  6 16\na * 2##  a  b  c  d  e \n##  0  4 10  6 16\n# seq_along vs. length(). En un vector de vacío length(0) hace el proceso correcto\nconj1 = 1:10; conj2 = 20:30\nx = intersect(conj1, conj2)\nseq_along(x)## integer(0)\n3:seq_along(x)## Error in 3:seq_along(x): argument of length 0\n3:length(x)      # secuencia descendente de 3 a 0      ## [1] 3 2 1 0\n# while\nresultado = c()\ni = 1 \nwhile (i <= length(a)) {\n  resultado[i] = a[i]*2\n  i = i+1\n}\nresultado## [1]  0  4 10  6 16"},{"path":"intro.html","id":"condicionales","chapter":"Capítulo 1 Introduccion a R","heading":"1.19 Condicionales","text":"","code":"\ny = 2; f = 'auto'\n\nif (is.numeric(y)) {\n  print('es numerico!')\n}## [1] \"es numerico!\"\nif (is.numeric(y)) print('es numerico!')## [1] \"es numerico!\"\nif (is.numeric(f)) {\n  print('es numerico!')\n} else {\n  print('no es numerico!')\n}## [1] \"no es numerico!\"\nifelse(is.numeric(y), y+8, NA)## [1] 10\nifelse(a>=5, a+10, 0)##  a  b  c  d  e \n##  0  0 15  0 18\nif (this) {\n    # hace esto\n} else if (that) {\n    # hace esto otro\n} else {\n    # hace otra cosa\n}"},{"path":"intro.html","id":"funciones","chapter":"Capítulo 1 Introduccion a R","heading":"1.20 Funciones","text":"","code":"\n# Componentes: nombre, argumentos, cuerpo y return (opcional, pero generalmente se usa)\n# Si el programa es para uso personal no es necesario validar inputs\nsuma = function(x, y) {\n  s = x + y\n  return(s)\n}\nres = suma(3,5)\nres## [1] 8"},{"path":"intro.html","id":"output-más-de-un-resultado","chapter":"Capítulo 1 Introduccion a R","heading":"1.20.1 Output más de un resultado","text":"","code":"\nsuma2 = function(x, y) {\n  s = x + y\n  m = x * y\n  return(list(suma = s, mult = m))\n}\nres = suma2(3,5)\nres['suma']## $suma\n## [1] 8\nres['mult']## $mult\n## [1] 15"},{"path":"intro.html","id":"argumentos-con-valores-default","chapter":"Capítulo 1 Introduccion a R","heading":"1.20.2 Argumentos con valores default","text":"Nota: se puede escribir una funcion (o varias) en un script y luego utilizar source(mifuncion.R) para tenerlas diponibles en el espacio de trabajo.","code":"\nsuma3 = function(x, y = 2) {\n  s = x + y\n  return(s)\n}\nsuma3(7)## [1] 9\nsuma3(7, y = 0)## [1] 7"},{"path":"bd.html","id":"bd","chapter":"Capítulo 2 Base de datos","heading":"Capítulo 2 Base de datos","text":"En esta clase nos vamos centrar en el uso de tidyverse. Además vamos utilizar funciones de lubridate y zoo que tienen algunas funciones especiales para trabajar con fechas.En R existen dos tipos de bases de datos data.frame() y tibble() que son las bases de datos de tidyverse el mejor paquete para manipulación y transformación de datos (ver Wickham Grolemund (2017)). Un data.frame (objeto df) se convierte fácilmente tibble (y viceversa).Las tibbles tienen algunas funciones especiales como poder usar nombres de variables con espacio (se deben utilizar back ticks).","code":"\n# Un data.frame (objeto df) se convierte fácilmente a tibble\ntib = as_tibble(df)\nlibrary(tidyverse)\ntb <- tibble(\n  `Plazo Fijo` = \"espacio\", \n  `2000` = \"numero\"\n)\ntb## # A tibble: 1 x 2\n##   `Plazo Fijo` `2000`\n##   <chr>        <chr> \n## 1 espacio      numero"},{"path":"bd.html","id":"directorio-de-trabajo","chapter":"Capítulo 2 Base de datos","heading":"2.1 Directorio de trabajo","text":"Coloco las rutas de acceso en una variable para usar más adelante.","code":"\ngetwd() # Para ver en que directorio estamos trabajando\nsetwd('C:/Documentos/CianciaDatos') # Definir directorio. Notar barras invertidas en la ruta\ndatos_ts = 'C:/Users/msang/OneDrive - BCRA/CursoR/2021/Clase01/datos_ts.xlsx'\ndatos_pd = 'C:/Users/msang/OneDrive - BCRA/CursoR/2021/Clase02/datos_wb.xlsx'"},{"path":"bd.html","id":"cargar-datos","chapter":"Capítulo 2 Base de datos","heading":"2.2 Cargar datos","text":"Tabla 2.1: Vista de la base de datos (World Bank)","code":"\n# CSV\nbd = read.csv(\"b_datos.csv\", header=TRUE, stringsAsFactors=TRUE, sep=\",\")\n\n# TXT / Esta es la más eficiente porque permite paralelizar con multithread\nbd = data.table::fread('b_datos.txt', header=TRUE, stringsAsFactors=F, sep='\\t', nThread=2)\n\nbd = read.delim('datos/b_datos.txt', header=TRUE, stringsAsFactors=F, sep='\\t')\n# Excel\nbd = readxl::read_excel(datos_pd, sheet='1') # En datos_pd esta la ruta de acceso completa "},{"path":"bd.html","id":"ingrasar-datos-con-tidyverse","chapter":"Capítulo 2 Base de datos","heading":"2.2.1 Ingrasar datos con tidyverse","text":"","code":""},{"path":"bd.html","id":"bases-de-stata","chapter":"Capítulo 2 Base de datos","heading":"2.2.2 Bases de Stata","text":"","code":"\nlibrary(heaven)\nread_dta()\nwrite_dta()"},{"path":"bd.html","id":"problemas-de-imputación","chapter":"Capítulo 2 Base de datos","heading":"2.3 Problemas de imputación","text":"Ver más especificaciones aquí","code":"\n# Parsear vectores\nlibrary(readr)\nstr(parse_logical(c(\"TRUE\", \"FALSE\", \"NA\")))##  logi [1:3] TRUE FALSE NA\nstr(parse_integer(c(\"1\", \"2\", \"3\")))##  int [1:3] 1 2 3\nstr(parse_date(c(\"2010-01-01\", \"1979-10-14\")))##  Date[1:2], format: \"2010-01-01\" \"1979-10-14\"\nparse_integer(c(\"1\", \"231\", \".\", \"456\"), na = \".\")## [1]   1 231  NA 456\nparse_double(\"1,23\", locale = locale(decimal_mark = \",\"))## [1] 1.23\n# Base de datos\nchallenge <- read_csv(\n  readr_example(\"challenge.csv\"), \n  col_types = cols(\n    x = col_double(),\n    y = col_date()\n  )\n)\ntail(challenge)## # A tibble: 6 x 2\n##       x y         \n##   <dbl> <date>    \n## 1 0.805 2019-11-21\n## 2 0.164 2018-03-29\n## 3 0.472 2014-08-04\n## 4 0.718 2015-08-16\n## 5 0.270 2020-02-04\n## 6 0.608 2019-01-06"},{"path":"bd.html","id":"exportar-datos","chapter":"Capítulo 2 Base de datos","heading":"2.4 Exportar datos","text":"","code":"\n# CSV\nwrite.csv(bd,\"b_datos.csv\")\nwrite_csv()\nwrite_excel_csv()\n# TXT \nwrite_delim()\nwrite_tsv()\n\n# Excel\nlibrary(\"xlsx\")\n# Primera base de datos\nwrite.xlsx(USArrests, file = \"b_datos.xlsx\", sheetName = \"IRIS\", append = FALSE)\n# Segunda base de datos\nwrite.xlsx(mtcars, file = \"b_datos.xlsx\", sheetName=\"MTCARS\", append=TRUE)"},{"path":"bd.html","id":"variables","chapter":"Capítulo 2 Base de datos","heading":"2.5 Variables","text":"","code":"\nlibrary(tidyverse)\nbd1 = bd %>% \n  mutate(gdp_pc2010bis = gdp_2010 / popu,   # crear\n         open = exports + imports,\n         inv_demean = inv - mean(inv)) %>%\n  rename(poblacion = popu) %>%              # rename (newname = oldname)\n  mutate(gdp_2010 = NULL)                   # drop (también con select())"},{"path":"bd.html","id":"merge","chapter":"Capítulo 2 Base de datos","heading":"2.6 Merge","text":"","code":"\nmeta = readxl::read_excel(datos_pd, sheet='2')\nbd = left_join(bd, meta, by=c('ccode'))"},{"path":"bd.html","id":"group_by-mutate","chapter":"Capítulo 2 Base de datos","heading":"2.7 group_by, mutate","text":"","code":"\nbd$id = as.numeric(factor(bd$ccode))\n# Si quiero usar una función propia\ndemean = function(x) {x - mean(x, na.rm = TRUE)}\nbd = bd %>% \n  mutate(open = exports + imports) %>%\n  dplyr::select(ccode, year, region, gdp_pc2017, credit_ps, inv, id, open) %>%\n    arrange(ccode, year) %>%\n  group_by(ccode) %>%\n  mutate(obs = seq(1:length(ccode)),     # igual con row_number()\n         gdp_gr = 100 * (gdp_pc2017 / lag(gdp_pc2017,1) - 1),\n         credit_ps_mean = mean(credit_ps, na.rm = TRUE),\n         dev = if_else(region=='Latin America & Caribbean', 0, 1),\n         gdp_dem = demean(gdp_pc2017)) %>%\n  ungroup()\nhead(bd[c('ccode', 'dev', 'year', 'gdp_pc2017', 'gdp_gr', 'gdp_dem')],10)## # A tibble: 10 x 6\n##    ccode   dev  year gdp_pc2017 gdp_gr  gdp_dem\n##    <chr> <dbl> <dbl>      <dbl>  <dbl>    <dbl>\n##  1 ARG       0  2011     24648.  NA     1451.  \n##  2 ARG       0  2012     24119.  -2.15   922.  \n##  3 ARG       0  2013     24424.   1.27  1227.  \n##  4 ARG       0  2014     23550.  -3.58   353.  \n##  5 ARG       0  2015     23934.   1.63   737.  \n##  6 ARG       0  2016     23190.  -3.11    -7.58\n##  7 ARG       0  2017     23597.   1.76   400.  \n##  8 ARG       0  2018     22759.  -3.55  -438.  \n##  9 ARG       0  2019     22064.  -3.06 -1133.  \n## 10 ARG       0  2020     19687. -10.8  -3511.\n# case_when() permite evaluar más de 2 alternativas\n\ndf <- tibble(\n  a = seq(1,5)\n)\ndf = df %>% mutate(b = case_when(a <= 2 ~ 1,\n                                 a > 2 & a <= 4 ~ 2,\n                                 TRUE ~ as.double(a)))\ndf## # A tibble: 5 x 2\n##       a     b\n##   <int> <dbl>\n## 1     1     1\n## 2     2     1\n## 3     3     2\n## 4     4     2\n## 5     5     5"},{"path":"bd.html","id":"guardar-datos","chapter":"Capítulo 2 Base de datos","heading":"2.8 Guardar datos","text":"","code":"\nbd = bd %>% select(ccode, year, region, gdp_gr, credit_ps, inv, open)\nsave(bd, file=\"datos_wb.rda\")"},{"path":"bd.html","id":"valores-missing","chapter":"Capítulo 2 Base de datos","heading":"2.9 Valores missing","text":"Son tratados como los valores más grandes de todos pero el replace los respeta.","code":"\ndf2 <- tibble(\n  a = sample(1:5, 5, replace = F),\n  b = seq(5,1),\n)\ndf2## # A tibble: 5 x 2\n##       a     b\n##   <int> <int>\n## 1     1     5\n## 2     3     4\n## 3     2     3\n## 4     5     2\n## 5     4     1\ndf2 = df2 %>% mutate(b = ifelse(b == 2, NA, b)) \ndf2 = df2 %>% mutate(c = ifelse(b > 3, 0, b))\ndf2 = df2 %>% arrange(b)\ndf2## # A tibble: 5 x 3\n##       a     b     c\n##   <int> <int> <dbl>\n## 1     4     1     1\n## 2     2     3     3\n## 3     3     4     0\n## 4     1     5     0\n## 5     5    NA    NA"},{"path":"bd.html","id":"eliminar-valores-missing","chapter":"Capítulo 2 Base de datos","heading":"2.9.1 Eliminar valores missing","text":"Se debe tener presente que se elimina la fila completa.","code":"\n#  Volvemos a la base de WB. Recordamos la estructura\nstr(bd)## tibble [60 x 7] (S3: tbl_df/tbl/data.frame)\n##  $ ccode    : chr [1:60] \"ARG\" \"ARG\" \"ARG\" \"ARG\" ...\n##  $ year     : num [1:60] 2011 2012 2013 2014 2015 ...\n##  $ region   : chr [1:60] \"Latin America & Caribbean\" \"Latin America & Caribbean\" \"Latin America & Caribbean\" \"Latin America & Caribbean\" ...\n##  $ gdp_gr   : num [1:60] NA -2.15 1.27 -3.58 1.63 ...\n##  $ credit_ps: num [1:60] 14 15.2 15.7 13.8 14.4 ...\n##  $ inv      : num [1:60] 17.2 15.9 16.3 16 15.6 ...\n##  $ open     : num [1:60] 35.2 30.5 29.3 28.4 22.5 ...\nsummary(bd[,1:4])##     ccode                year         region              gdp_gr        \n##  Length:60          Min.   :2011   Length:60          Min.   :-10.7750  \n##  Class :character   1st Qu.:2013   Class :character   1st Qu.: -2.7656  \n##  Mode  :character   Median :2016   Mode  :character   Median :  0.7100  \n##                     Mean   :2016                      Mean   : -0.6777  \n##                     3rd Qu.:2018                      3rd Qu.:  1.4095  \n##                     Max.   :2020                      Max.   :  4.3092  \n##                                                       NA's   :6\nbd1 = na.omit(bd)\nnrow(bd1)## [1] 50\nsum(ifelse(is.na(bd$credit_ps),1,0)) # cuenta valores missing de CreditoSPriv## [1] 4\nrm('bd1')"},{"path":"bd.html","id":"loop","chapter":"Capítulo 2 Base de datos","heading":"2.10 Loop","text":"","code":"\nset.seed(1234)\ndf <- tibble(\n  a = runif(100, min=0, max=100),\n  b = rnorm(100, 0, 1),\n  c = rnorm(100, mean=5, sd=3),\n)\nhead(df,3)## # A tibble: 3 x 3\n##       a      b     c\n##   <dbl>  <dbl> <dbl>\n## 1  11.4 -1.81   3.87\n## 2  62.2 -0.582  5.29\n## 3  60.9 -1.11   9.92\n# Reemplazar variables existentes\nvars = names(df)\nfor (v in vars) {\n  df[v] = df[v] * 100\n}\n\nhead(df,3)## # A tibble: 3 x 3\n##       a      b     c\n##   <dbl>  <dbl> <dbl>\n## 1 1137. -181.   387.\n## 2 6223.  -58.2  529.\n## 3 6093. -111.   992.\n# Generar varaibles nuevas\n# Dividir las dos ultimas (b y c) por la primera (a)\nset.seed(1234)\ndf1 <- tibble(\n  a = rep(2, 100),\n  b = rnorm(100, 0, 1),\n  c = rnorm(100, 5, 3),\n)\nvars = names(df1[2:length(df1)])\nfor (v in vars) {\n  df1[paste0(v,'_a')] = df1[v] / df1[['a']]\n}\nhead(df1,3)## # A tibble: 3 x 5\n##       a      b     c    b_a   c_a\n##   <dbl>  <dbl> <dbl>  <dbl> <dbl>\n## 1     2 -1.21   6.24 -0.604  3.12\n## 2     2  0.277  3.58  0.139  1.79\n## 3     2  1.08   5.20  0.542  2.60"},{"path":"bd.html","id":"pivot","chapter":"Capítulo 2 Base de datos","heading":"2.11 Pivot","text":"","code":"\n# Long\nbdAR = bd %>% \n         filter(ccode=='ARG') %>%\n         select(year, credit_ps, inv) %>%\n         pivot_longer(cols=-year, names_to=\"Var\", values_to=\"Val\") %>%\n         arrange(year, desc(Var))    # Sort\n# Wide\nbdAR2 = bdAR %>% \n         filter(Var=='credit_ps', year <=2015) %>%\n         mutate(ccode = 'ARG') %>%\n         pivot_wider(id_cols=ccode, names_from=year, values_from=Val) %>%\n         rename_with(~ paste0(\"CREDIT\", 2011:2015), where(is.numeric))\n# Otra forma de llevar a long starts_with()\nbdAR3 = bdAR2 %>% \n       pivot_longer(cols = starts_with(\"CREDIT\"), names_to=\"Var\", values_to=\"Val\") %>% \n       separate(Var, c(\"V\",\"year\"), sep = 6)\n#unite() para concatenar variables"},{"path":"bd.html","id":"append","chapter":"Capítulo 2 Base de datos","heading":"2.12 Append","text":"","code":"\nbdAR = bd %>%\n  filter(ccode==\"ARG\",\n         year>=2018) %>%\n  select(year, ccode, gdp_gr)\nbdBR = bd %>%\n  filter(ccode==\"BRA\",\n         year>=2018) %>%\n  select(year, ccode, gdp_gr)\nbdARBR = rbind(bdAR, bdBR)\n# Ver bind_rows() de tidyverse"},{"path":"bd.html","id":"strings","chapter":"Capítulo 2 Base de datos","heading":"2.13 Strings","text":"","code":"\nwords = c('uno','dos', 'tres')\nlength(words)   # cuenta los elementos del vector## [1] 3\nstr_length(words)## [1] 3 3 4\nwords = gsub('tres','cinco',words)\nwords## [1] \"uno\"   \"dos\"   \"cinco\"\nwords = str_replace(words, \"uno\", \"diez\")\nwords## [1] \"diez\"  \"dos\"   \"cinco\"\nhi = 'hola'\nnchar(hi)## [1] 4"},{"path":"bd.html","id":"fechas","chapter":"Capítulo 2 Base de datos","heading":"2.14 Fechas","text":"","code":""},{"path":"bd.html","id":"year","chapter":"Capítulo 2 Base de datos","heading":"2.14.1 Year","text":"%Y (4 digitos).%y (2 digitos); 00-69 -> 2000-2069, 70-99 -> 1970-1999.","code":""},{"path":"bd.html","id":"month","chapter":"Capítulo 2 Base de datos","heading":"2.14.2 Month","text":"%m (2 digitos).%b (nombre abreviado, “Jan”).%B (nombre completo, “January”).","code":""},{"path":"bd.html","id":"day","chapter":"Capítulo 2 Base de datos","heading":"2.14.3 Day","text":"%d (2 digitos).","code":"\nlibrary(zoo)\nlibrary(lubridate)\ndatos = readxl::read_excel(datos_ts, sheet='datos')\ndatos$fecha = as.Date(datos$fecha, format = '%Y-%m-%d')\ndatos$fecha2 = datos$fecha + days(15)\ndatos$fecha3 = floor_date(datos$fecha2, 'month')\ndatos$mes = month(datos$fecha)\ndatos$year = year(datos$fecha)\ndatos = datos[,c(1,5,6,7,8,2,3,4)] # reordena la base de datos"},{"path":"bd.html","id":"manipulación-de-fechas","chapter":"Capítulo 2 Base de datos","heading":"2.14.4 Manipulación de fechas","text":"Utilizamos el paquete zoo","code":"\ndatosq <- readxl::read_excel(datos_ts, sheet='trim')  %>%\n  mutate(fecha=as.Date(as.yearqtr(paste(year, trim), format=\"%Y %q\")))"},{"path":"bd.html","id":"análisis-de-datos","chapter":"Capítulo 2 Base de datos","heading":"2.15 Análisis de datos","text":"","code":"\nstr(bd)## tibble [60 x 7] (S3: tbl_df/tbl/data.frame)\n##  $ ccode    : chr [1:60] \"ARG\" \"ARG\" \"ARG\" \"ARG\" ...\n##  $ year     : num [1:60] 2011 2012 2013 2014 2015 ...\n##  $ region   : chr [1:60] \"Latin America & Caribbean\" \"Latin America & Caribbean\" \"Latin America & Caribbean\" \"Latin America & Caribbean\" ...\n##  $ gdp_gr   : num [1:60] NA -2.15 1.27 -3.58 1.63 ...\n##  $ credit_ps: num [1:60] 14 15.2 15.7 13.8 14.4 ...\n##  $ inv      : num [1:60] 17.2 15.9 16.3 16 15.6 ...\n##  $ open     : num [1:60] 35.2 30.5 29.3 28.4 22.5 ...\ndim(bd)## [1] 60  7\nnames(bd)## [1] \"ccode\"     \"year\"      \"region\"    \"gdp_gr\"    \"credit_ps\" \"inv\"      \n## [7] \"open\"\nglimpse(bd)## Rows: 60\n## Columns: 7\n## $ ccode     <chr> \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG~\n## $ year      <dbl> 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, ~\n## $ region    <chr> \"Latin America & Caribbean\", \"Latin America & Caribbean\", \"L~\n## $ gdp_gr    <dbl> NA, -2.1452844, 1.2656852, -3.5785805, 1.6296643, -3.1100639~\n## $ credit_ps <dbl> 14.00872, 15.21282, 15.72909, 13.82377, 14.41423, 13.66763, ~\n## $ inv       <dbl> 17.24828, 15.85753, 16.28951, 15.97995, 15.56475, 14.27236, ~\n## $ open      <dbl> 35.20615, 30.52654, 29.33393, 28.40679, 22.48623, 26.09389, ~\nhead(bd)## # A tibble: 6 x 7\n##   ccode  year region                    gdp_gr credit_ps   inv  open\n##   <chr> <dbl> <chr>                      <dbl>     <dbl> <dbl> <dbl>\n## 1 ARG    2011 Latin America & Caribbean  NA         14.0  17.2  35.2\n## 2 ARG    2012 Latin America & Caribbean  -2.15      15.2  15.9  30.5\n## 3 ARG    2013 Latin America & Caribbean   1.27      15.7  16.3  29.3\n## 4 ARG    2014 Latin America & Caribbean  -3.58      13.8  16.0  28.4\n## 5 ARG    2015 Latin America & Caribbean   1.63      14.4  15.6  22.5\n## 6 ARG    2016 Latin America & Caribbean  -3.11      13.7  14.3  26.1"},{"path":"bd.html","id":"tablas","chapter":"Capítulo 2 Base de datos","heading":"2.15.1 Tablas","text":"Los valores NA afectan todas las estadísticas. Opción na.rm = F/T","code":"\nbd %>% summarise(\n  credit_ps_media = mean(credit_ps),\n  inv_max = max(inv),\n  open_min = min(open)\n)## # A tibble: 1 x 3\n##   credit_ps_media inv_max open_min\n##             <dbl>   <dbl>    <dbl>\n## 1              NA    24.9     22.5"},{"path":"bd.html","id":"group_by-summarise","chapter":"Capítulo 2 Base de datos","heading":"2.16 group_by, summarise","text":"","code":"\ntab = bd %>% \n  dplyr::select(credit_ps, inv, open) %>% \n  mutate(id = 1) %>%     # esta variable es solo para usar pivot, despues la dropeo\n  pivot_longer(cols = -id, names_to = 'Variable', values_to = 'Value') %>%\n  mutate(id = NULL) %>%\n  group_by(Variable) %>% \n  summarise(\n    Obs = n(),\n    Media = mean(Value, na.rm = T),\n    Mediana = median(Value, na.rm = T),\n    SD = sd(Value, na.rm = T),\n    Min = min(Value, na.rm = T),\n    Max = max(Value, na.rm = T)) %>% \n  ungroup()\ntab## # A tibble: 3 x 7\n##   Variable    Obs Media Mediana    SD   Min   Max\n##   <chr>     <int> <dbl>   <dbl> <dbl> <dbl> <dbl>\n## 1 credit_ps    60  89.7    94.6 38.9   13.7 171. \n## 2 inv          60  18.8    17.8  3.19  13.4  24.9\n## 3 open         60  49.1    56.2 15.7   22.5  72.2\ntab = bd %>% \n  filter(year>=2015) %>% \n  group_by(year, region) %>% \n  summarise(\n    inv_sum = sum(inv, na.rm = T),\n    inv_sd = sd(inv, na.rm = T),\n    credit_ps_p50 = median(credit_ps, na.rm = T),\n    obs = n()\n    ) %>% \n  arrange(year) %>% \n  ungroup()\ntab## # A tibble: 12 x 6\n##     year region                    inv_sum inv_sd credit_ps_p50   obs\n##    <dbl> <chr>                       <dbl>  <dbl>         <dbl> <int>\n##  1  2015 Europe & Central Asia        55.7   2.55          95.1     3\n##  2  2015 Latin America & Caribbean    57.2   4.24          66.8     3\n##  3  2016 Europe & Central Asia        56.7   2.54          97.4     3\n##  4  2016 Latin America & Caribbean    52.5   4.57          62.2     3\n##  5  2017 Europe & Central Asia        58.0   2.76         101.      3\n##  6  2017 Latin America & Caribbean    50.7   3.57          59.5     3\n##  7  2018 Europe & Central Asia        58.6   2.93         104.      3\n##  8  2018 Latin America & Caribbean    51.2   3.81          88.6     3\n##  9  2019 Europe & Central Asia        59.6   3.26         108.      3\n## 10  2019 Latin America & Caribbean    51.8   4.98          93.2     3\n## 11  2020 Europe & Central Asia        58.2   3.02         124.      3\n## 12  2020 Latin America & Caribbean    50.7   3.78          70.2     3\ntab = tab %>% mutate(region = str_replace(region, c('Europe & Central Asia', 'Latin America & Caribbean'), c('EU', 'LA'))) %>% \n pivot_longer(cols=-c(year, region), names_to='Var', values_to='Val') %>%\n unite(id,region, Var) %>% \n pivot_wider(id_cols=year, names_from=id, values_from=Val)\ntab## # A tibble: 6 x 9\n##    year EU_inv_sum EU_inv_sd EU_credit_ps_p50 EU_obs LA_inv_sum LA_inv_sd\n##   <dbl>      <dbl>     <dbl>            <dbl>  <dbl>      <dbl>     <dbl>\n## 1  2015       55.7      2.55             95.1      3       57.2      4.24\n## 2  2016       56.7      2.54             97.4      3       52.5      4.57\n## 3  2017       58.0      2.76            101.       3       50.7      3.57\n## 4  2018       58.6      2.93            104.       3       51.2      3.81\n## 5  2019       59.6      3.26            108.       3       51.8      4.98\n## 6  2020       58.2      3.02            124.       3       50.7      3.78\n## # ... with 2 more variables: LA_credit_ps_p50 <dbl>, LA_obs <dbl>"},{"path":"bd.html","id":"vector-de-resultados","chapter":"Capítulo 2 Base de datos","heading":"2.17 Vector de resultados","text":"","code":"\nset.seed(1234)\ndf <- tibble(\n  a = runif(100, min=0, max=100),\n  b = rnorm(100,0,1),\n  c = rnorm(100, mean=5, sd=3),\n)\n\noutput <- vector(\"double\", ncol(df))  # 1. verctor de resultados (vacio)\nfor (i in seq_along(df)) {            # 2. secuencia\n  output[[i]] <- mean(df[[i]])        # 3. cuerpo\n}\noutput## [1] 43.74972619  0.07975381  5.40972645"},{"path":"bd.html","id":"gráficos","chapter":"Capítulo 2 Base de datos","heading":"2.18 Gráficos","text":"","code":"\nplot(datos$fecha, datos$ipc )\nplot(datos$fecha, log(datos$ipc), type= 'l', col = 'red', xlab ='Mes', ylab ='Log(IPC)')"},{"path":"bd.html","id":"ggplot","chapter":"Capítulo 2 Base de datos","heading":"2.19 GGPlot","text":"Grammar Graphics.","code":"\n#SINTAXIS\n# ggplot(data = <DATA>) + \n#   <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>)) # se agregan layers (piont, line, etc.)\n# aes() \"aesthetic\" define la estética del gráfico\nlibrary(ggplot2)\ndatos1 = datos %>% \n     select(fecha, ipc) \n\nggplot(datos1, aes(x=fecha, y=ipc)) +\n  geom_line(color = 'steelblue2', size = 1.2) +\n  theme_minimal() +\n  labs(title=\"Indice de precios al consumidor\", x=\"Mes\", y=\"IPC\") +\n  theme(legend.position=\"none\") +\n  NULL\n# Selecciona el mejor de cada clase de acuerdo al consumo en highway\nbest_in_class <- mpg %>%\n  group_by(class) %>%\n  filter(row_number(desc(hwy)) == 1)\n\ng = ggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_smooth(se = FALSE) +\n  geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.5) + \n  labs(title = \"Fuel efficiency generally decreases with engine size\",\n       subtitle = \"Two seaters (sports cars) are an exception because of their light weight\",\n       x = \"Engine displacement (L)\",\n       y = \"Highway fuel economy (mpg)\",\n       colour = \"Car type\",\n       caption = \"Data from fueleconomy.gov\") + \n  NULL\ng"},{"path":"bd.html","id":"guardar-un-gráfico","chapter":"Capítulo 2 Base de datos","heading":"2.20 Guardar un gráfico","text":"","code":"\nwork = \"C:/Users/msang/OneDrive - BCRA/CursoR/2021/Clase02/\"\nfilesave = paste0(work,'ts.png')\nggsave(filesave, g, width=10, height=8) "},{"path":"conceptos.html","id":"conceptos","chapter":"Capítulo 3 Conceptos generales","heading":"Capítulo 3 Conceptos generales","text":"El objetivo de la Ciencia de Datos es aprender algo de los datos. Si se dispone de una variable output3 y otras input4 el aprendizaje se denomina supervisado, si sólo hay inputs el aprendizaje es supervisado.Dentro aprendizaje supervisado podemos distinguir la predicción, cuando la variable output es cuantitativa, de la clasificación donde la la variable output es discreta/categórica (ej. \\(0/1\\)).5 Por su parte, el análisis supervisado busca relaciones y estructura dentro de los datos (ej. distinguir clusters/grupos de clientes para promociones publicitarias).","code":""},{"path":"conceptos.html","id":"estimacion","chapter":"Capítulo 3 Conceptos generales","heading":"3.1 Estimacion","text":"Supongamos que se quiere estudiar la relación entre el gasto en publicidad través de diversos canales como televisión, radio, diarios (inputs) y las ventas en distintos mercados (output).\\[\\begin{equation}\n\\tag{3.1}\n  Y = f(X) + \\epsilon\n\\end{equation}\\]donde \\(f\\) es una función desconocida de \\((X_1, X_2, X_3)\\) y \\(\\epsilon\\) es un término de error aleatorio independiente de \\(X\\) con media \\(0\\).En esencia, el aprendizaje estadístico se refiere un conjunto de enfoques para estimar \\(f\\).","code":""},{"path":"conceptos.html","id":"prediccion","chapter":"Capítulo 3 Conceptos generales","heading":"3.2 Prediccion","text":"Supongamos que se dispone de datos de variables independientes por de la dependiente, en ese caso, dado que el error en promedio es \\(0\\) podríamos predecir \\(Y\\) utilizando:\\[\\begin{equation}\n\\tag{3.2}\n  \\hat{Y} = \\hat{f}(X)\n\\end{equation}\\]donde \\(\\hat{f}\\) representa nuestra estimación de \\(f\\) y \\(\\hat{Y}\\) representa la predicción de \\(Y\\). En este contexto, \\(\\hat{f}\\) menudo se trata como una caja negra, en el sentido que importa la forma exacta de \\(\\hat{f}\\), siempre que produzca predicciones precisas de \\(Y\\).La precisión con la que \\(\\hat{Y}\\) se acerca \\(Y\\) depende de dos cantidades, el error reducible y el irreducible. En general, \\(\\hat{f}\\) será una estimación perfecta de \\(f\\), y esta inexactitud introducirá\nun error que es reducible porque potencialmente podemos mejorar la\nprecisión de \\(\\hat{f}\\) usando la técnica de aprendizaje estadístico más apropiada para estimar \\(f\\). Sin embargo, si fuera posible estimar \\(f\\) de manera perfecta de manera que la respuesta estimada \\(\\hat{Y} = f(X)\\), nuestra predicción todavía tendría algún error dado que \\(Y\\) también es función de \\(\\epsilon\\), que por definición, se puede predecir usando \\(X\\). Por lo tanto, la variabilidad asociada con \\(\\epsilon\\) también afecta la precisión de nuestras predicciones. Esto se conoce como el error irreducible, porque importa qué tan bien estimemos \\(f\\), puede reducir el error introducido por \\(\\epsilon\\).El término de error \\(\\epsilon\\) puede contener variables observables que son útiles para predecir \\(Y\\) y, por lo tanto, \\(f\\) puede usarlos para su predicción.\\[\\begin{align*}\n  E(Y - \\hat{Y})^2 & = E[f(X) + \\epsilon - \\hat{f}(X)]^2 \\\\\n                   & = \\underbrace{[f(X) - \\hat{f}(X)]^2}_{\\text{Reducible}} + \\underbrace{Var(\\epsilon)}_{\\text{Irreducible}}\n\\end{align*}\\]donde de \\(E(Y − \\hat{Y})^2\\) representa el promedio, o valor esperado, de la diferencia entre el valor predicho y real de \\(Y\\) al cuadrado, y \\(Var(\\epsilon)\\) representa la varianza asociada al término de error \\(\\epsilon\\).El foco de este curso está en las técnicas para estimar \\(f\\) con el objetivo de minimizar el error reducible. Es importante tener en cuenta que la error irreducible siempre proporcionará un límite superior en la precisión de nuestra predicción para \\(Y\\) que en la práctica casi siempre es desconocido.","code":""},{"path":"conceptos.html","id":"inferencia","chapter":"Capítulo 3 Conceptos generales","heading":"3.3 Inferencia","text":"En este caso el interés está en comprender la asociación entre \\(Y\\) y \\(X_1,..., X_p\\). Se busca estimar \\(f\\), pero el objetivo es necesariamente hacer predicciones sobre \\(Y\\). Ahora \\(\\hat{f}\\) puede ser tratada como una caja negra, porque se necesita conocer su forma exacta. Así se busca determinar:Qué variables se deben incluir en el modeloCómo es la relación entre el la variable explicada y cada predictorSi la relación se puede aproximar con un modelo lineal o uno más complejo","code":""},{"path":"conceptos.html","id":"metodos-parametricos","chapter":"Capítulo 3 Conceptos generales","heading":"3.4 Metodos parametricos","text":"Se realiza en dos etapas:Asumir una forma funcional (modelo, por ejemplo lineal)\\[\\begin{equation}\n\\tag{3.3}\n  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\n\\end{equation}\\]Estimar los parámetros (método, por ejemplo MCO)Si bien el problema se reduce estimar \\(p + 1\\) parámetros, la desventaja es que la forma funcional elegida puede diferir de la verdadera \\(f\\).","code":""},{"path":"conceptos.html","id":"metodos-no-parametricos","chapter":"Capítulo 3 Conceptos generales","heading":"3.5 Metodos no parametricos","text":"realizan supuestos sobre la forma funcional de \\(f\\) sino que tratan de buscar una estimación que se acerque lo más posible los datos sin ser ni demasiado tosco ni demasiado ondulado. El costo es que se necesitan más datos para estimar.","code":""},{"path":"conceptos.html","id":"evaluacion-de-la-precision-del-modelo","chapter":"Capítulo 3 Conceptos generales","heading":"3.6 Evaluacion de la precision del modelo","text":"Ningún método domina al resto sobre todas las bases de datos posibles. En un conjunto de datos en particular, un método específico puede funcionar mejor, pero algún otro método lo puede superar con otra base de datos. Por lo tanto, en cada caso se debe decidir qué método produce los mejores resultados.","code":""},{"path":"conceptos.html","id":"calidad-del-ajuste","chapter":"Capítulo 3 Conceptos generales","heading":"3.6.1 Calidad del ajuste","text":"Para evaluar el desempeño de un método de aprendizaje estadístico en\nun conjunto de datos dado, se necesita alguna forma de medir qué tan bien sus predicciones coinciden con los datos observados. Es decir, se necesita cuantificar el grado en el cual el valor pronosticado para una observación dada está cerca de el verdadero valor de respuesta para esa observación. En el escenario de regresión, la medida más utilizada es el error cuadrático medio (\\(MSE\\)):\\[\\begin{equation}\n\\tag{3.4}\n  MSE = \\frac{1}{n} \\sum_{=1}^{n}(y_i - \\hat{f}(x_i))^2\n\\end{equation}\\]donde \\(\\hat{f}(x_i)\\) es la predicción que \\(\\hat{f}\\) hace sobre la observación \\(\\). El \\(MSE\\) será pequeño si las respuestas predichas están muy cerca de las respuestas verdaderas y será grande si para algunas observaciones difieren demasiado.El \\(MSE\\) en (3.4) se calcula usando los datos de entrenamiento (training) que se usaron para estimar el modelo, por lo que debería denominarse con mayor precisión \\(MSE\\) de entrenamiento. Pero en general, nos interesa realmente qué tan bien funciona el método\nsobre los datos de entrenamiento. Más bien, estamos interesados en la precisión de las predicciones que obtenemos cuando aplicamos nuestro método datos de test que fueron visto antes (datos utilizados para entrenar el modelo). Es decir, se busca elegir el método que produzca el menor \\(MSE\\) de test.\\[\\begin{equation}\n\\tag{3.5}\n  Prom(y_0 - \\hat{f}(x_0))^2\n\\end{equation}\\]el error de predicción cuadrático promedio para estas observaciones de test \\((y_0, x_0)\\).¿Qué sucede si se elige en base al MSE de training (3.4) ? hay garantía de que el método con el MSE de entrenamiento más bajo también tenga el MSE de test más bajo.El panel de la izquierda de la Figura 3.1 muestra la verdadera \\(f\\) dada por la curva negra. Las curvas naranja, azul y verde ilustran\ntres posibles estimaciones de \\(f\\) obtenidas utilizando métodos con\nniveles de flexibilidad. La línea naranja es el ajuste de regresión lineal, que es relativamente inflexible. Las curvas azul y verde se produjeron usando splines con diferentes niveles de suavidad. Es claro que medida que aumenta el nivel de flexibilidad, las curvas se ajustan mejor los datos observados. La curva verde es la más flexible y coincide con la datos muy bien; sin embargo, se observa que se ajusta mal la verdadera \\(f\\) (en negro) porque es demasiado ondulada. Cambiando el nivel de flexibilidad del spline se pueden producir ajustes diferentes para estos datos.En el panel de la derecha de la Figura 3.1 la curva gris\nmuestra el \\(MSE\\) de entrenamiento promedio en función de la flexibilidad, o más formalmente los grados de libertad (resume la flexibilidad de una curva), para una serie de splines. Los cuadrados naranja, azul y verde\nindican los \\(MSE\\) asociados con las curvas correspondientes en el panel izquierdo. El \\(MSE\\) de entrenamiento MSE disminuye monótonamente medida que aumenta la flexibilidad. En este ejemplo el verdadera \\(f\\) es lineal, por lo que el ajuste lineal naranja es lo suficientemente flexible para estimar bien \\(f\\). La curva verde tiene el \\(MSE\\) de entrenamiento más bajo de los tres métodos, ya que corresponde la más flexible de las tres curvas.En este ejemplo, se conoce la verdadera función \\(f\\), por lo que también se puede calcular el \\(MSE\\) de test (en general \\(f\\) es desconocida, por lo que esto es posible). El \\(MSE\\) de test se muestra usando la curva roja en el panel derecho de la Figura 3.1. Como con el MSE de entrenamiento, el MSE de test disminuye inicialmente medida que el nivel de flexibilidad aumenta. Sin embargo, en algún momento el MSE de test se nivela y luego empieza aumentar. En consecuencia, las curvas naranja y verde tienen un \\(MSE\\) de test alto. La curva azul minimiza el \\(MSE\\) de test, dado que visualmente parece estimar \\(f\\) lo mejor en el panel izquierdo. La línea discontinua horizontal indica \\(Var(\\epsilon)\\), el error irreducible en la ecuación de \\(E(Y - \\hat{Y})^2\\), que corresponde al menor alcanzable por el MSE de test entre todos los métodos posibles. Por lo tanto, la spline de suavizado representada por la curva azul está cerca del óptimo.En el panel de la derecha de la Figura 3.1, como la flexibilidad del método de aprendizaje aumente, se observa una disminución monótona en el \\(MSE\\) de entrenamiento y una forma de U en el \\(MSE\\) de test. Esta es una propiedad fundamental de aprendizaje estadístico que se mantiene independientemente del conjunto de datos particular en cuestión e independientemente del método estadístico que se utilice.Cuando un método dado produce un \\(MSE\\) de entrenamiento pequeño pero un \\(MSE\\) de test grande, se dice que está haciendo overfitting/sobreajustando los datos. Esto sucede porque nuestro aprendizaje estadístico está trabajando demasiado para encontrar patrones en los datos de entrenamiento, y puede estar detectando algunos patrones que son causados por casualidad en lugar de por las verdaderas propiedades de la función desconocida \\(f\\). Overfitting se refiere específicamente al caso en el que un modelo menos flexible podría haber producido un menor error de predicción en test.\nFigura 3.1: Datos en curva y MSE\nLa Figura 3.2 proporciona otro ejemplo en el que la verdadera \\(f\\) es aproximadamente lineal por lo que este tipo de modelos obtienen el menor \\(MSE\\) en test.\nFigura 3.2: Datos lineales y MSE\n","code":""},{"path":"conceptos.html","id":"trade-off-sesgo-varianza","chapter":"Capítulo 3 Conceptos generales","heading":"3.6.2 Trade-off Sesgo-Varianza","text":"La Figura 3.3 muestra el trade-Sesgo - Varianza intuitivamente.\nFigura 3.3: Estimacion y MSE\nLa forma de U observada en las curvas \\(MSE\\) de test es el resultado de dos propiedades que compiten en los métodos de aprendizaje estadístico. El \\(MSE\\) de test esperado, para un valor dado \\(x_0\\), puede descomponerse en la suma de tres cantidades fundamentales: la\nvarianza de \\(\\hat{f}(x_0)\\), el sesgo al cuadrado de \\(\\hat{f}(x_0)\\) y la varianza del error \\(\\epsilon\\).\\[\\begin{equation}\n\\tag{3.6}\n  E(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Sesgo(\\hat{f}(x_0))]^2 + Var(\\epsilon)\n\\end{equation}\\]donde \\(E(y_0 - \\hat{f}(x_0))^2\\) el valor esperado de \\(MSE\\) de test en \\(x_0\\). Para minimizar el error de test esperado, se necesita seleccionar un método de aprendizaje estadístico que logre simultáneamente baja varianza y bajo sesgo.La varianza se refiere al valor en que \\(f\\) cambiaría si\nse estimara utilizando una base de datos de entrenamiento diferente. Sesgo se refiere al error que se introduce al aproximar un problema de la vida real, que puede ser extremadamente complicado, por mucho modelo más simple. Como regla general, medida que se utilizan métodos más flexibles, la varianza aumenta y el sesgo disminuye. La tasa relativa de cambio de estos dos cantidades determina si el \\(MSE\\) de test aumenta o disminuye.Los dos paneles de la Figura 3.4 ilustran la Ecuación (3.6) para los ejemplos en Figuras 3.1 y 3.2. En cada caso, la curva sólida azul representa el cuadrado del sesgo, para diferentes niveles de flexibilidad, mientras que la curva naranja corresponde la varianza. La línea discontinua horizontal representa \\(Var(\\epsilon)\\), el error irreducible. Finalmente, la curva roja, corresponde al MSE de test, es la suma de estas tres cantidades.\nFigura 3.4: Estimacion y MSE\n","code":""},{"path":"conceptos.html","id":"clasificacion","chapter":"Capítulo 3 Conceptos generales","heading":"3.6.3 Clasificacion","text":"Muchos de los conceptos del contexto de regresion, como\nel trade-sesgo-varianza, se transfieren al entorno de clasificación donde ahora \\(y_i\\) es cualitativa. El el enfoque más común para cuantificar la precisión de la estimación \\(\\hat{f}\\) es la tasa de error de entrenamiento, la proporción de errores que se cometen si aplicamos nuestra estimación \\(\\hat{f}\\) las observaciones de entrenamiento\\[\\begin{equation}\n\\tag{3.7}\n  \\frac{1}{n} \\sum_{=1}^{n}(y_i \\neq \\hat{y_i})\n\\end{equation}\\]Aquí \\(\\hat{y_i}\\) es la etiqueta de clase predicha para la -ésima observación usando \\(\\hat{f}\\). Y \\((y_i \\neq \\hat{y_i})\\) es una variable indicadora que es igual 1 si \\(y_i \\neq \\hat{y_i}\\) y cero si \\(y_i = \\hat{y_i}\\). indicador Si \\((y_i \\neq \\hat{y_i}) = 0\\) entonces la -ésima observación fue clasificada correctamente por el método de clasificación; de lo contrario, fue mal clasificado.La tasa de error de test asociada con un conjunto de observaciones de test de la forma \\((x_0, y_0)\\) está dada por:\\[\\begin{equation}\n\\tag{3.8}\n  Prom((y_0 \\neq \\hat{y_0}))\n\\end{equation}\\]donde \\(\\hat{y_0}\\) es la etiqueta de clase predicha que resulta de aplicar el clasificador la observación de prueba con predictor \\(x_0\\). Un buen clasificador es aquel para el cual el error de prueba (3.8) es el más pequeño.","code":""},{"path":"conceptos.html","id":"clasificador-de-bayes","chapter":"Capítulo 3 Conceptos generales","heading":"3.6.3.1 Clasificador de Bayes","text":"Es posible mostrar bajo penalidad simétrica6 la tasa de error de test postulada en (3.8) se minimiza, en promedio, por un clasificador muy simple que asigna cada observación la clase más probable, dados sus valores predictores. En otras palabras, se debería asignar una observación de test con vector predictor \\(x_0\\) la clase \\(j\\) para la cual (3.9) es mayor.\\[\\begin{equation}\n\\tag{3.9}\n  Pr(Y = j \\mid X = x_0)\n\\end{equation}\\]Es decir, en un problema donde sólo hay dos categorías el clasificador de Bayes predice la clase \\(1\\) si \\(Pr(Y = 1 \\mid X = x_0)>0.5\\) y la clase \\(2\\) en caso contrario.","code":""},{"path":"conceptos.html","id":"matriz-de-confusion","chapter":"Capítulo 3 Conceptos generales","heading":"3.6.4 Matriz de confusion","text":"\\(VN\\): Verdadero Negativo; \\(FN\\): Falso Negativo; \\(FP\\): Falso Positivo; \\(VP\\): Verdadero PositivoMétricas para comparar modelos. La precisión es la cantidad de predicciones correctas, la sensibilidad es la cantidad de \\(VP\\) identificados sobre el total de positivos y la especificidad es la cantidad de \\(VN\\) identificados sobre el total de negativos.\\[\\begin{align*}\nPrecision & = \\frac{VP + VN}{VP + VN + FP + FN} \\\\ \nSensiblilidad & = \\frac{VP}{VP + FN} \\\\ \nEspecificidad & = \\frac{VN}{VN + FP} \n\\end{align*}\\]","code":""},{"path":"conceptos.html","id":"curva-roc","chapter":"Capítulo 3 Conceptos generales","heading":"3.6.5 Curva ROC","text":"El nombre viene de receiver operating characteristics (comunicación). Si se modifica el umbral \\(p_i > c\\), cambian los resultados de la matriz de confusión. Por ejemplo, al estimar la probabilidad de default, para un banco podría resultar relativamente más costoso clasificar un mal deudor como default que uno bueno como default. Entonces podría bajar el umbral \\(p_i > 0,3\\) (asimétrico) para clasificar casos positivos afectando las tasas de errores.Si se define:\n\\[\\begin{align*}\nTPR = & VP / P \\\\ \nFPR = & FP / N\n\\end{align*}\\]La curva \\(ROC\\) representa la relación entre \\(TPR\\) y \\(FPR\\) (notar que la \\(FPR = 1 - Especificidad\\)) para todos los valores posibles de \\(c \\[0, 1]\\). La curva \\(ROC\\) permite medir capacidad predictiva y comparar modelos.\nFigura 3.5: Curva ROC\nCasos extremos\\(c = 1\\) todos clasificados como negativos ; \\(tpr = 0\\), \\(fpr = 0\\)\\(c = 0\\) todos clasificados como positivos ; \\(tpr = 1\\), \\(fpr = 1\\)\nFigura 3.6: Curva ROC puntos importantes\n\\(AUC\\) (o \\(AUROC\\)): área bajo la curva \\(ROC\\). Cuán parecida es la curva \\(ROC\\) la ideal, es decir cuanto \\(AUC\\) está más cerca de \\(1\\) mejor es el clasificador. Por su parte, un clasificador aleatorio debe tener un \\(AUC = 0,5\\) (línea de \\(45^\\circ\\)).","code":""},{"path":"conceptos.html","id":"cross-validation","chapter":"Capítulo 3 Conceptos generales","heading":"3.7 Cross Validation","text":"Recordar la diferencia entre la tasa de error de test y la\ntasa de error de entrenamiento. El error de test es el error promedio que resulta de usar un método de aprendizaje estadístico para predecir la respuesta en una nueva observación, es decir, que fue utilizada en el entrenamiento del método.Modelos complejos predicen bien dentro de la muestra pero mal fuera de la misma (overfit) y nuestro interés está puesto en esta última.Objetivo: buscar el nivel de complejidad óptimo para predecir fuera de la muestra.PérdidaRegresión = \\((Y - \\hat{Y})^2\\)Clasificación = \\(1(Y \\neq \\hat{Y})\\)k-Fold cross-validationDividir la muestra en \\(K\\) partes al azar.Tomar \\(K - 1\\) partes y estimar en modelo.Calcular el error de predicción para los datos utilizados.Repetir para \\(k = 1,...,K\\)\nFigura 3.7: K-Fold cross-validation\nLa estimación por cross-validation del error de predicción es:\\[\\begin{equation}\n\\tag{3.10}\n  CV(\\hat{f}) = \\frac{1}{N}L(Y_i - \\hat{Y}_{-k}(x_i))\n\\end{equation}\\]donde \\(\\hat{Y}_{-k}(x_i)\\) es la predicción hecha cuando la observación fue usada para estimar. Cada observación se utiliza en dos roles: Entrenamiento y test. De esta forma se estima el modelo \\(K\\) veces para construir el error de pronóstico.Cross-validation para eleccion de modelos\nSi \\(\\alpha\\) representa la complejidad de un modelo (por ejemplo el grado de un polinomio)\\[\\begin{equation}\n\\tag{3.11}\n  CV(\\hat{f}, \\alpha) = \\frac{1}{N}L(Y_i - \\hat{Y}_{-k}(x_i, \\alpha))\n\\end{equation}\\]Computar \\(CV(\\hat{f}, \\alpha)\\) para distintos valores de \\(\\alpha\\) y elegir el modelo que minimiza el error.7","code":""},{"path":"conceptos.html","id":"bootstrap","chapter":"Capítulo 3 Conceptos generales","heading":"3.8 Bootstrap","text":"Bootstrap es una herramienta estadística que se puede utilizar para cuantificar la incertidumbre asociada con un estimador o un método de aprendizaje estadístico.Dado \\(Y_1, Y_2,...,Y_n\\) iid \\(Y \\sim (\\mu, \\sigma^2)\\)Se quiere estimar la varianza de la media muestral \\(V(\\overline{Y}) = \\frac{\\sigma^2}{n}\\)Formula: \\(\\frac{\\hat{\\sigma}^2}{n}\\)\\[\\begin{equation}\n\\tag{3.12}\n  \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{=1}^{n}(Y_i-\\overline{Y})^2 \n\\end{equation}\\]Método sin fórmulaDe los \\(N\\) datos originales \\(y_1, y_2,...,y_N\\):Tomar una muestra con remplazo de tamaño \\(n\\) (una observación puede entrar más de una vez y otra puede entrar nunca).Computar la media muestral de esta muestra.Repetir \\(B\\) veces. Al terminar tendremos \\(B\\) estimaciones de la media.Calcular la varianza de las \\(B\\) medias.En términos generalesDado \\(Y_i\\) con \\(= 1,...,n\\) y \\(\\theta\\) es una magnitud de interésTomar una muestra con remplazo de tamaño \\(n\\) (muestra bootstrap).Computar \\(\\hat{\\theta}_j\\), con \\(j = 1,...,B\\).Repetir \\(B\\) veces.Calcular:\\[\\begin{equation}\n\\tag{3.13}\n  \\hat{V}(\\hat{\\theta})_B = \\frac{1}{B} \\sum_{j=1}^{B}(\\hat{\\theta}_j - \\overline{\\hat{\\theta}})^2 \n\\end{equation}\\]Ejemplo:","code":"\nset.seed(1234)\npoblacion = rnorm(1000)\n\n# Bootstrap con 10000 repeticiones:\nmuestra_boot = c()\nfor (i in 1:10000) {\n  muestra = sample(poblacion, 300, replace=TRUE)\n  muestra_boot = c(muestra_boot, mean(muestra))\n}  \n\n# Media calculada con MUESTRA BOOTSTRAP\nsimulated_mean = mean(muestra_boot)\n\n# Varianza de la MUESTRA BOOTSTRAP\nsimulated_var = sd(muestra_boot)^2\n\n# Comparemos medias:\nmean(poblacion); simulated_mean## [1] -0.0265972## [1] -0.02612666\n# Comparemos varianza:\nsd(poblacion)^2/300; simulated_var## [1] 0.003315608## [1] 0.003312673"},{"path":"mco.html","id":"mco","chapter":"Capítulo 4 Regresion lineal","heading":"Capítulo 4 Regresion lineal","text":"En machine learning el objetivo principal es estimar y hacer inferencia como en la econometría clásica sino hacer predicciones. El modelo de regresión lineal es una herramienta útil para predecir\ncuando la variable de respuesta es cuantitativa.Modelo lineal simple\\[\\begin{equation}\n\\tag{4.1}\n  Y = \\beta_0 + \\beta_1 X_1 + u \n\\end{equation}\\]donde \\(u\\) es un término de error aleatorio que captura todo lo que puede representarse con este modelo simple (factores observables, errores de medición).Modelo lineal múltiple\\[\\begin{equation}\n\\tag{4.2}\n  Y = X \\beta + u \n\\end{equation}\\]Método de Mínimos Cuadrados Ordinarios \\((MCO)\\)\\[\\begin{equation}\n\\tag{4.3}\n \\hat{\\beta} = min \\sum_{=1}^{n}e_i^2  \n\\end{equation}\\]\\[\\begin{equation}\n\\tag{4.4}\n \\hat{\\beta} = (X'X)^{-1}X'Y  \n\\end{equation}\\]\nFigura 4.1: Modelo lineal y MCO\nBajo los supuestos clásicos el estimador de \\(MCO\\) es el mejor dentro de los lineales e insesgados (MELI).El \\(R^2\\) mide la proporción de la variabilidad de \\(Y\\) explicada por el modelo. \\(R^2 \\[0, 1].\\)\\[\\begin{equation}\n\\tag{4.5}\n R^2 = 1 - \\frac{SCR}{SCT}\n\\end{equation}\\]Dado que \\(SCT = SCE + SCR\\) y que \\(SCT\\) es una magnitud fija, por lo tanto, \\(MCO\\) maximiza el \\(R^2\\).¿Cómo se hace para predecir \\(Y\\)?\\(Y_i = X_i^{'} \\beta + u_i\\) con \\(=1,...,n\\)\\(\\hat{Y_i} \\equiv X_i^{'} \\hat{\\beta}\\)\\(E(\\hat{Y_i}) = X_i^{'} \\beta\\)\\(V(\\hat{Y_i}) = X_i^{'} V(\\hat{\\beta})X_i = \\sigma^2X_i^{'}(X'X)^{-1}X_i\\)Resultado: si el estimador \\(\\hat{\\beta}\\) es insegado y de varianza mínima, entonces \\(\\hat{Y_i}\\) es un predictor insegado y de varianza mínima, ambos en la clase de estimadores/predictores lineales e insesgados.Error cuadrático medio \\((EMC)\\):Predecir requiere estimar \\((\\beta)\\)Descomposición sesgo varianza:\\[\\begin{equation}\n\\tag{4.6}\n EMC(\\hat{\\beta}) = E(\\hat{\\beta} - \\beta)^2\n\\end{equation}\\]Intuición: cuan mal estima \\(\\hat{\\beta}\\) depende de cuan descentrado esta en relación la verdad (sesgo) más cuan disperso es relación su propio centro (varianza).\\[\\begin{equation}\n\\tag{4.7}\n EMC(\\hat{\\beta}) = Sesgo^2(\\hat{\\beta}) + V(\\hat{\\beta})\n\\end{equation}\\]ECM y error de pronósticoRelacion entre el error de estimación con el error de predicción.EMC\\[\\begin{equation}\n\\tag{4.8}\n Err(\\hat{Y}) = E(Y - \\hat{f})^2\n\\end{equation}\\]Error de pronostico\\[\\begin{equation}\n\\tag{4.9}\n Err(Y - \\hat{f}) = EMC(\\hat{f}) + \\sigma^2\n\\end{equation}\\]Error reducible \\((ECM)\\) mas irreducible \\((\\sigma^2)\\).Link predicción y estimación. Predecir correctamente \\((Err(Y - \\hat{f}))\\) requiere estimar \\((EMC(\\hat{f}))\\) correctamente.Utilizando la descomposición:\\[\\begin{equation}\n\\tag{4.10}\n Err(Y-\\hat{f}) = \\underbrace{Sesgo^2(\\hat{f}) + V(\\hat{f})}_{\\text{ECM}} + \\sigma^2\n\\end{equation}\\]Machine learning: estrategias sesgadas pueden implicar una reducción drástica en la varianza.Puede ser que el mínimo ECM ocurra para predictores sesgados.","code":""},{"path":"mco.html","id":"aplicacion-practica","chapter":"Capítulo 4 Regresion lineal","heading":"4.1 Aplicacion practica","text":"La biblioteca ISLR2 contiene la base de datos de Boston, que registra medv (mediana del valor de las casas) para \\(506\\) distritos censales en Boston. Buscaremos predecir medv usando \\(12\\) predictores como rm (número promedio de habitaciones por casa), age (edad promedio de las casas) y lstat (porcentaje de hogares con bajo status socioeconómico).Comenzaremos usando la función lm() para ajustar un modelo de regresión lineal simple, con medv como respuesta y lstat como predictor.La función predict() se puede utilizar para producir intervalos de predicción para medv para un valor dado de lstat.continuación se examinan algunos graficos de diagnóstico con la función plot(). En general, este comando producirá un gráfico la vez (presionando Enter se generará el siguiente gráfico). Sin embargo, se pueden ver los cuatro gráficos juntos con las funciones par() y mfrow(), que le dicen R que divida la pantalla de visualización en paneles separados. Por ejemplo, par(mfrow = c(2, 2)) divide la region del gráfico región en una cuadrícula de paneles de \\(2 \\times 2\\).Regresión lineal múltiple:","code":"\nlibrary(MASS)\nlibrary(ISLR2)\nhead(Boston)##      crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n## 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n## 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n## 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n## 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n## 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n## 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\nlm.fit = lm(medv ~ lstat, data = Boston)\nsummary(lm.fit)## \n## Call:\n## lm(formula = medv ~ lstat, data = Boston)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.168  -3.990  -1.318   2.034  24.500 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 34.55384    0.56263   61.41   <2e-16 ***\n## lstat       -0.95005    0.03873  -24.53   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.216 on 504 degrees of freedom\n## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 \n## F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\nnames(lm.fit)##  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n##  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n##  [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"\nattach(Boston)\nplot(lstat, medv)\nabline(lm.fit)\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = 'prediction')##        fit       lwr      upr\n## 1 29.80359 17.565675 42.04151\n## 2 25.05335 12.827626 37.27907\n## 3 20.30310  8.077742 32.52846\npar(mfrow = c(2, 2))\nplot(lm.fit)\nlm.fit = lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)## \n## Call:\n## lm(formula = medv ~ lstat + age, data = Boston)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.981  -3.978  -1.283   1.968  23.158 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\n## lstat       -1.03207    0.04819 -21.416  < 2e-16 ***\n## age          0.03454    0.01223   2.826  0.00491 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.173 on 503 degrees of freedom\n## Multiple R-squared:  0.5513, Adjusted R-squared:  0.5495 \n## F-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\nlm.fit = lm(medv ~ ., data = Boston)\nsummary(lm.fit)## \n## Call:\n## lm(formula = medv ~ ., data = Boston)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.1304  -2.7673  -0.5814   1.9414  26.2526 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\n## crim         -0.121389   0.033000  -3.678 0.000261 ***\n## zn            0.046963   0.013879   3.384 0.000772 ***\n## indus         0.013468   0.062145   0.217 0.828520    \n## chas          2.839993   0.870007   3.264 0.001173 ** \n## nox         -18.758022   3.851355  -4.870 1.50e-06 ***\n## rm            3.658119   0.420246   8.705  < 2e-16 ***\n## age           0.003611   0.013329   0.271 0.786595    \n## dis          -1.490754   0.201623  -7.394 6.17e-13 ***\n## rad           0.289405   0.066908   4.325 1.84e-05 ***\n## tax          -0.012682   0.003801  -3.337 0.000912 ***\n## ptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\n## lstat        -0.552019   0.050659 -10.897  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.798 on 493 degrees of freedom\n## Multiple R-squared:  0.7343, Adjusted R-squared:  0.7278 \n## F-statistic: 113.5 on 12 and 493 DF,  p-value: < 2.2e-16\nlm.fit1 = lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)## \n## Call:\n## lm(formula = medv ~ . - age, data = Boston)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.1851  -2.7330  -0.6116   1.8555  26.3838 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\n## crim         -0.121426   0.032969  -3.683 0.000256 ***\n## zn            0.046512   0.013766   3.379 0.000785 ***\n## indus         0.013451   0.062086   0.217 0.828577    \n## chas          2.852773   0.867912   3.287 0.001085 ** \n## nox         -18.485070   3.713714  -4.978 8.91e-07 ***\n## rm            3.681070   0.411230   8.951  < 2e-16 ***\n## dis          -1.506777   0.192570  -7.825 3.12e-14 ***\n## rad           0.287940   0.066627   4.322 1.87e-05 ***\n## tax          -0.012653   0.003796  -3.333 0.000923 ***\n## ptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\n## lstat        -0.547409   0.047669 -11.483  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.794 on 494 degrees of freedom\n## Multiple R-squared:  0.7343, Adjusted R-squared:  0.7284 \n## F-statistic: 124.1 on 11 and 494 DF,  p-value: < 2.2e-16"},{"path":"lasso.html","id":"lasso","chapter":"Capítulo 5 Lasso","heading":"Capítulo 5 Lasso","text":"Cuando el objetivo es encontrar el mejor modelo, es decir, aquel con menor error de predicción en la muestra de test existen distintas alternativas de selección de modelos. La idea de shrinkage es ajustar un modelo que contenga todos los \\(p\\) predictores usando una técnica que restringe o regulariza las estimaciones de coeficientes hacia cero.Dado:\\(Y = \\beta_0 + \\beta_1 X_1 +...+ \\beta_p X_p + u\\)Suponer que \\(M_k\\) con \\(k = 1,...,K\\) es una serie modelos.Elección de modelos: buscar en \\(M_k\\) del mejor modelo para predecir fuera de la muestra. La primera alternativa podría ser realizar una búsqueda exhaustiva. En la practica, puede ser impracticable dado que con \\(p\\) predictores, se pueden construir \\(2^p\\) modelos (con \\(p = 15\\) hay \\(32.768\\) modelos).Lasso: una manera formal y algorítmica de realizar esa tarea. Para \\(\\lambda \\ge 0\\) dado, se considera la siguiente función objetivo (minimizar):\\[\\begin{equation}\n\\tag{5.1}\n R_l(\\beta) = \\sum_{=1}^{n}(y_i - x_i^{'} \\beta)^2 + \\lambda \\sum_{s=2}^{p}|\\beta_s|\n\\end{equation}\\]donde \\(x_i\\) contiene un intercepto.\\(\\lambda = 0\\) entonces estimador de \\(MCO\\)\\(\\lambda = 0\\) entonces estimador de \\(MCO\\)\\(\\lambda = \\infty\\) el segundo termino \\(\\\\infty\\) entonces \\(\\beta_2,...\\beta_p = 0\\)\\(\\lambda = \\infty\\) el segundo termino \\(\\\\infty\\) entonces \\(\\beta_2,...\\beta_p = 0\\)\\(\\sum_{=1}^{n}(y_i - x_i^{'} \\beta)^2\\) penaliza la falta de ajuste.\\(\\sum_{=1}^{n}(y_i - x_i^{'} \\beta)^2\\) penaliza la falta de ajuste.\\(\\sum_{s=2}^{p}|\\beta_s|\\) penaliza complejidad.\\(\\sum_{s=2}^{p}|\\beta_s|\\) penaliza complejidad.\\(0 < \\lambda < \\infty\\) Lasso hace algo intermedio entre \\(MCO\\) y \\(0\\)Lasso: automáticamente elige que variables entran \\((\\beta_s \\neq 0)\\) y cuales \\((\\beta_s = 0)\\)Por ejemplo, moviéndose de izquierda derecha de la Figura 5.1 se observa que al principio lasso resulta en un modelo que contiene solo el predictor de rating. Luego, ingresan al modelo student y limit casi simultáneamente, seguidas de cerca por income. Finalmente lo hacen el resto de las variables.\nFigura 5.1: Lasso\nIntuitivamente si \\((\\hat{\\beta_l} = 0)\\), al moverse \\(0\\) (incluir la variable) la función de penalidad de Lasso por un lado “cobra” y por el otro “paga.” El costo por incluir la variable sube (\\(\\lambda\\)) y el de ajuste baja.Si el primero sube más rápido que lo que el segundo baja conviene quedarse en \\(0\\). En caso contrario conviene moverse afuera de \\(0\\). Entonces conviene moverse de \\(0\\) si la relación es lo suficientemente fuerte (mejora el ajuste), sino conviene evitar la penalización.","code":""},{"path":"lasso.html","id":"aplicacion-practica-1","chapter":"Capítulo 5 Lasso","heading":"5.1 Aplicacion practica","text":"Se utiliza el paquete glmnet para realizar la regresión de lasso8 con una sintaxis ligeramente diferente de otras funciones de estimación de modelos. En particular, se debe usar una matriz x y un vector y y utiliza la sintaxis (y ~ x). Se busca predecir Salary en la base de datos Hitters. Antes de comenzar deben eliminarse lo valores missing.La función model.matrix() es particularmente útil para crear x; sólo produce una matriz correspondiente los \\(19\\) predictores, sino también transforma automáticamente cualquier variable cualitativa en variables dummies. La última propiedad es importante porque glmnet() solo puede tomar variables numéricAs y cuantitativas.Ahora dividimos las muestras en una base de entrenamiento y otra de test para estimar el error de test de la regresión lasso. Hay varias formas de dividir aleatoriamente una base de datos.9 Nuestro enfoque elige aleatoriamente un subconjunto de números entre \\(1\\) y \\(n\\); estos se pueden usar como índices para las observaciones de entrenamiento.Primero establecemos una semilla aleatoria para que los resultados obtenidos sean reproducibles.Para estimar un modelo lasso usamos la función glmnet() con el argumento alpha = 1 que indica la metodología que queremos usar.10También definimos una grilla11 de \\(100\\) valores \\(\\lambda\\) que van desde \\(\\lambda=10^{10}\\) \\(\\lambda=10^{-2}\\), cubriendo esencialmente la gama completa de escenarios desde el modelo nulo que contiene solo la constante, hasta el modelo de mínimos cuadrados.Podemos ver en el gráfico de coeficientes que dependiendo de la elección del parámetro de ajuste, algunos de los coeficientes serán exactamente iguales cero.Ahora realizamos un ejercicio de cross validation y calculamos el error de test asociado.Finalmente, el resultado indica que \\(8\\) de los \\(19\\) coeficientes estimados son exactamente cero. Por lo tanto, el modelo de lasso con \\(\\lambda\\) elegido por cross validation contiene solo \\(11\\) variables.","code":"\nlibrary(ISLR2)\nlibrary(glmnet)\nnames(Hitters)##  [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n##  [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n## [13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n## [19] \"Salary\"    \"NewLeague\"\ndim(Hitters)## [1] 322  20\nsum(is.na(Hitters$Salary))## [1] 59\nHitters <- na.omit(Hitters)\ndim(Hitters)## [1] 263  20\nsum(is.na(Hitters))## [1] 0\nx = model.matrix(Salary ~ ., Hitters)[, -1]\ny = Hitters$Salary\nset.seed(1)\ntrain <- sample(1:nrow(x), nrow(x) / 2)\ntest <- (-train)\ny.test <- y[test]\ngrid <- 10^seq(10, -2, length = 100)\nlasso.mod <- glmnet(x[train, ], y[train], alpha = 1,\n    lambda = grid)\nplot(lasso.mod)\nset.seed(1)\ncv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)\nplot(cv.out)\nbestlam <- cv.out$lambda.min\nlasso.pred <- predict(lasso.mod, s = bestlam,\n    newx = x[test, ])\nmean((lasso.pred - y.test)^2)## [1] 143673.6\nout <- glmnet(x, y, alpha = 1, lambda = grid)\nlasso.coef <- predict(out, type = \"coefficients\",\n    s = bestlam)[1:20, ]\nlasso.coef##   (Intercept)         AtBat          Hits         HmRun          Runs \n##    1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n##           RBI         Walks         Years        CAtBat         CHits \n##    0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n##        CHmRun         CRuns          CRBI        CWalks       LeagueN \n##    0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n##     DivisionW       PutOuts       Assists        Errors    NewLeagueN \n## -116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000\nlasso.coef[lasso.coef != 0]##   (Intercept)         AtBat          Hits         Walks         Years \n##    1.27479059   -0.05497143    2.18034583    2.29192406   -0.33806109 \n##        CHmRun         CRuns          CRBI       LeagueN     DivisionW \n##    0.02825013    0.21628385    0.41712537   20.28615023 -116.16755870 \n##       PutOuts        Errors \n##    0.23752385   -0.85629148"},{"path":"logit.html","id":"logit","chapter":"Capítulo 6 Logit","heading":"Capítulo 6 Logit","text":"En la base de datos Default, la variable default pertenece una de dos categorías, Si o . En vez de modelar esta respuesta \\(Y\\) directamente, la regresión logística modela la probabilidad de que \\(Y\\) pertenezca un categoría. Por lo tanto, si se busca estimar la probabilidad de default, se puede transformar la variable original en una variable binaria \\(1\\) (Si); \\(0\\) ().En general, la estrategia consiste en:\\(Y\\) variable binaria \\(0/1\\).\\(X\\) vector de \\(K\\) predictores.Se debe construir un modelo para:\\(p = Pr(Y = 1 \\mid X)\\)Clasificador de Bayes:\\(\\hat{Y} = 1\\) si \\(p > 0,5\\)\\(\\hat{Y} = 1\\) si \\(p > 0,5\\)\\(\\hat{Y} = 0\\) si \\(p \\le 0,5\\)\\(\\hat{Y} = 0\\) si \\(p \\le 0,5\\)Notar que en el caso particular de la probabilidad de default puede resultar relativamente más costoso para un banco clasificar un deudor malo en la categoría default que un deudor bueno como si default (pérdida asimétrica). Por lo tanto, podría elegirse un umbral más bajo (ej. \\(p > 0,3\\)) para la categoría si default.Modelo logit\\[\\begin{equation}\n\\tag{6.1}\n  p = \\frac{e^z}{1+e^z}\n\\end{equation}\\]donde \\(z \\equiv X \\beta\\) con \\(\\beta\\) un vector de \\(K\\) coeficientes.El panel izquierdo de la Figura 6.1 muestra la probabilidad estimada usando regresión lineal donde algunos valores de probabilidad son negativos (también podría haber valores mayores \\(1\\) para valores elevados de la variable balance ambos extremos inconsistentes con la noción de probabilidad). Los puntos naranja indican los valores \\(0/1\\) codificados por defecto (o Si). El panel derecho exhibe las probabilidades estimadas de incumplimiento mediante regresión logística donde todos los valores \\(\\(0,1)\\).\nFigura 6.1: Regresión lineal vs. logística\nInterpretación de un coeficiente en el modelo logitDefinición. Odds ratio: probabilidad de que un evento suceda en relación que el mismo evento suceda.\\(\\frac{p}{1 - p}\\)Por ejemplo, en promedio \\(1\\) de cada \\(5\\) personas con un odds de \\(1/4\\) pagará, ya que \\(p(X) = 0,2\\) implica un odds de \\(\\frac{0,2}{1 − 0,2} = 1/4\\).Si se toma el logaritmo del odds ratio queda:\\[\\begin{equation}\n\\tag{6.2}\n  ln(\\frac{p}{1 - p}) = X \\beta\n\\end{equation}\\]El lado izquierdo se llama log odds o logit. El modelo de regresión logística (6.1) tiene un logit que es lineal en X. Por lo tanto, el \\(k-ésimo\\) coeficiente es:\\[\\begin{equation}\n\\tag{6.3}\n  \\beta_k = \\frac{\\partial ln(\\frac{p}{1 - p})}{\\partial X_k}\n\\end{equation}\\]Aumentar \\(X\\) en una unidad cambia el log odds en \\(\\beta_1\\). Dado que\nla relación entre \\(p(X)\\) y \\(X\\) en (6.1) es lineal, \\(\\beta_1\\) corresponde al cambio en \\(p(X)\\) asociado con el aumento de una unidad en X. La cantidad que \\(p(X)\\) cambia debido un cambio de una unidad en\n\\(X\\) depende del valor actual de \\(X\\). Pero independientemente del valor de \\(X\\), si \\(\\beta_1\\) es positivo, entonces el aumento de \\(X\\) se asocia con el aumento de \\(p(X)\\), y si \\(\\beta_1\\) es negativo, el aumento de \\(X\\) se asocia con la disminución de \\(p(X)\\).12\\(\\hat{{\\beta}}\\) se estima por máxima verosimilitud (es una forma cerrada como MCO).\\[\\begin{equation}\n\\tag{6.4}\n  \\hat{\\theta} = argmax_{\\theta} \\prod_{= 1}^{n} L(\\theta; y_i)\n\\end{equation}\\]Luego se reemplazan los coeficientes estimados en (6.5) para obtener las probabilidades estimadas.\\[\\begin{equation}\n\\tag{6.5}\n  \\hat{p_i} = \\frac{e^{X_i \\hat{\\beta}}}{1+e^{X_i \\hat{\\beta}}}\n\\end{equation}\\]","code":""},{"path":"logit.html","id":"aplicacion-practica-2","chapter":"Capítulo 6 Logit","heading":"6.1 Aplicacion practica","text":"Comenzaremos examinando algunas estadísticas y gráficos de\nlos datos Smarket, que forman parte de la biblioteca ISLR2. Esta base de datos consiste en rendimientos porcentuales para el índice bursátil \\(S\\&P 500\\) para más de \\(1,250\\)~días, desde principios de \\(2001\\) hasta finales de \\(2005\\). Para cada fecha, registra los rendimientos porcentuales para cada de los cinco días hábiles anteriores, de lagone lagfive. También registra el volume (el número de acciones negociadas el día anterior, en miles de millones), Today (el rendimiento porcentual en la fecha en cuestión) y direction (si el mercado estaba ‘Arriba’ o ‘Abajo’ en este día). El objetivo es predecir la “dirección” (una respuesta cualitativa) usando las otras características.La función cor() produce una matriz que contiene todas las correlaciones por pares entre los predictores.Como era de esperar, las correlaciones entre las variables rezagadas y los rendimientos de hoy son cercanas cero. En otras palabras, parece haber poca correlación entre los rendimientos de hoy y los rendimientos de días anteriores. La única correlación sustancial es entre Year y volume. Al graficar los datos, que están ordenados cronológicamente, vemos que el volume aumenta con el tiempo. En otras palabras, el número promedio de acciones negociadas diariamente aumentó de 2001 2005.continuación, ajustaremos un modelo de regresión logística para predecir la direction utilizando lagone hasta lagfive y volume. La función glm() se puede utilizar para muchos tipos de modelos lineales generalizados, incluida la regresión logística. La sintaxis de la función glm() es similar la de lm(), excepto que debemos pasar el argumento family = binomial para decirle R que ejecute una regresión logística.El valor \\(p\\) más pequeño está asociado con lagone. El coeficiente negativo sugiere que si el mercado tuvo un rendimiento positivo ayer, es menos probable que suba hoy. Sin embargo, con un valor de \\(0,15\\), el valor de \\(p\\) sigue siendo relativamente grande, por lo que hay pruebas claras de una asociación real entre lagone y direction.La función predict() se puede utilizar para predecir la probabilidad de que el mercado suba, dados los valores de los predictores. La opción type = \"response\" le dice R que genere probabilidades de la forma \\(P(Y=1|X)\\). Si se proporciona ninguna base de datos la función predict(), se calculan las probabilidades para los datos de entrenamiento que se usaron para ajustar el modelo de regresión logística. Se imprimen las primeras diez probabilidades estimadas. Sabemos que estos valores corresponden la probabilidad de que el mercado suba, en lugar de que baje, porque la función contrasts() indica que R ha creado una variable ficticia con un 1 para .Para hacer una predicción sobre si el mercado subirá o bajará en un día en particular, debemos convertir estas probabilidades pronosticadas en etiquetas de clase, ‘Arriba’ o ‘Abajo.’ Los siguientes dos comandos crean un vector de predicciones de clase basado en si la probabilidad prevista de un aumento del mercado es mayor o menor que \\(0,5\\).El primer comando crea un vector de \\(1.250\\) elementos . La segunda línea transforma en ‘Arriba’ todos los elementos para los que la probabilidad predicha de un aumento del mercado supera los \\(0,5\\). Dadas estas predicciones, la función table() se puede usar para producir una matriz de confusión para determinar cuántas observaciones se clasificaron correcta o incorrectamente. Al ingresar dos vectores cualitativos, R creará una tabla de \\(2 \\times 2\\) con recuentos del número de veces que ocurrió cada combinación. Por ejemplo, pronosticó y el mercado aumentó, predijo y el mercado disminuyó, etc.Los elementos de la diagonal de la matriz de confusión indican predicciones correctas, mientras que los fuera de la diagonal representan predicciones incorrectas. Por lo tanto, el modelo predijo correctamente que el mercado subiría en \\(507\\)~días y que bajaría en \\(145\\)~días, para un total de \\(507+145 = 652\\) predicciones correctas. La función mean() se puede utilizar para calcular la fracción de días en los que la predicción fue correcta. En este caso, la regresión logística predijo correctamente el movimiento del mercado \\(52,2\\%\\) del tiempo.primera vista, parece que el modelo de regresión logística funciona un poco mejor que un modelo aleatorio. Sin embargo, este resultado es engañoso porque entrenamos y testeamos el modelo en el mismo conjunto de observaciones de \\(1.250\\). En otras palabras, \\(100\\%-52.2\\%=47.8\\%\\), es la tasa de error de entrenamiento. Como hemos visto anteriormente, la tasa de error de entrenamiento suele ser demasiado optimista: tiende subestimar la tasa de error de test.Para evaluar mejor la precisión del modelo de regresión logística, podemos ajustar el modelo usando parte de los datos y luego examinar qué tan bien predice los datos retenidos. Esto producirá una tasa de error más realista, en el sentido de que en la práctica estaremos interesados en el rendimiento de nuestro modelo, en los datos que usamos para ajustar el modelo, sino en los días futuros para los que se desconocen los movimientos del mercado.Para implementar esta estrategia, primero crearemos un vector correspondiente las observaciones de \\(2001\\) \\(2004\\). Luego usaremos este vector para crear una base de datos retenidos de observaciones de \\(2005\\).El objeto train es un vector de \\(1.250\\) elementos, correspondientes las observaciones de la base de datos. Los elementos del vector que corresponden observaciones que ocurrieron antes de \\(2005\\) se establecen en VERDADERO, mientras que los que corresponden observaciones en \\(2005\\) se establecen en FALSO. El objeto train es un vector booleano, ya que sus elementos son VERDADERO y FALSO.\nLos vectores booleanos se pueden utilizar para obtener un subconjunto de filas o columnas de una matriz. Por ejemplo, el comando Smarket[train, ] elegiría una submatriz del conjunto de datos del mercado de valores, correspondiente solo las fechas anteriores \\(2005\\), ya que son aquellos para los que los elementos de train son VERDADERO. El símbolo ! (negación) se puede utilizar para invertir todos los elementos de un vector booleano. Es decir, !train es un vector similar train, excepto que los elementos que son VERDADERO en train se cambian FALSO en !train, y los elementos que son FALSO en train se cambia VERDADERO en !train. Por lo tanto, Smarket[!train, ] produce una submatriz de los datos del mercado de valores que contiene solo las observaciones para las cuales train es FALSO, es decir, las observaciones con fechas en \\(2005\\) (\\(252\\) observaciones).Ahora ajustamos un modelo de regresión logística usando solo el subconjunto de las observaciones que corresponden fechas anteriores \\(2005\\), usando el argumento subset. Luego obtenemos las probabilidades pronosticadas de que el mercado de valores suba para cada uno de los días de la base de test, es decir, para los días de \\(2005\\).Tener en cuenta que hemos entrenado y testeado el modelo en dos bases de datos completamente separados: el entrenamiento se realizó solo con las fechas anteriores \\(2005\\) y la prueba se realizó solo con las fechas de 2005. Finalmente, calculamos las predicciones para \\(2005\\) y las comparamos con los movimientos reales del mercado durante ese período de tiempo.La notación != significa que es igual , por lo que el último comando calcula la tasa de error en los datos de test. Los resultados son bastante decepcionantes: la tasa de error de test es de \\(52 \\%\\). Por supuesto, este resultado es tan sorprendente, dado que, en general, se esperaría poder utilizar los rendimientos de días anteriores para predecir el rendimiento futuro del mercado.","code":"\nlibrary(ISLR2)\nnames(Smarket)## [1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n## [7] \"Volume\"    \"Today\"     \"Direction\"\ndim(Smarket)## [1] 1250    9\nsummary(Smarket)##       Year           Lag1                Lag2                Lag3          \n##  Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n##  1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n##  Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n##  Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n##  3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n##  Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n##       Lag4                Lag5              Volume           Today          \n##  Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n##  1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n##  Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n##  Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n##  3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n##  Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n##  Direction \n##  Down:602  \n##  Up  :648  \n##            \n##            \n##            \n## \npairs(Smarket)\ncor(Smarket[, -9])##              Year         Lag1         Lag2         Lag3         Lag4\n## Year   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\n## Lag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\n## Lag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\n## Lag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\n## Lag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\n## Lag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\n## Volume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\n## Today  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n##                Lag5      Volume        Today\n## Year    0.029787995  0.53900647  0.030095229\n## Lag1   -0.005674606  0.04090991 -0.026155045\n## Lag2   -0.003557949 -0.04338321 -0.010250033\n## Lag3   -0.018808338 -0.04182369 -0.002447647\n## Lag4   -0.027083641 -0.04841425 -0.006899527\n## Lag5    1.000000000 -0.02200231 -0.034860083\n## Volume -0.022002315  1.00000000  0.014591823\n## Today  -0.034860083  0.01459182  1.000000000\nattach(Smarket)\nplot(Volume)\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial\n  )\nsummary(glm.fits)## \n## Call:\n## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n##     Volume, family = binomial, data = Smarket)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -1.446  -1.203   1.065   1.145   1.326  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)\n## (Intercept) -0.126000   0.240736  -0.523    0.601\n## Lag1        -0.073074   0.050167  -1.457    0.145\n## Lag2        -0.042301   0.050086  -0.845    0.398\n## Lag3         0.011085   0.049939   0.222    0.824\n## Lag4         0.009359   0.049974   0.187    0.851\n## Lag5         0.010313   0.049511   0.208    0.835\n## Volume       0.135441   0.158360   0.855    0.392\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1731.2  on 1249  degrees of freedom\n## Residual deviance: 1727.6  on 1243  degrees of freedom\n## AIC: 1741.6\n## \n## Number of Fisher Scoring iterations: 3\nglm.probs <- predict(glm.fits, type = \"response\")\nglm.probs[1:10]##         1         2         3         4         5         6         7         8 \n## 0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n##         9        10 \n## 0.5176135 0.4888378\ncontrasts(Direction)##      Up\n## Down  0\n## Up    1\nglm.pred <- rep(\"Down\", 1250)\nglm.pred[glm.probs > .5] = \"Up\"\ntable(glm.pred, Direction)##         Direction\n## glm.pred Down  Up\n##     Down  145 141\n##     Up    457 507\n(507 + 145) / 1250## [1] 0.5216\nmean(glm.pred == Direction)## [1] 0.5216\ntrain <- (Year < 2005)\nSmarket.2005 <- Smarket[!train, ]\ndim(Smarket.2005)## [1] 252   9\nDirection.2005 <- Direction[!train]\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial, subset = train\n  )\nglm.probs <- predict(glm.fits, Smarket.2005,\n    type = \"response\")\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred, Direction.2005)##         Direction.2005\n## glm.pred Down Up\n##     Down   77 97\n##     Up     34 44\nmean(glm.pred == Direction.2005)## [1] 0.4801587\nmean(glm.pred != Direction.2005)## [1] 0.5198413"},{"path":"arboles.html","id":"arboles","chapter":"Capítulo 7 Arboles de decision","heading":"Capítulo 7 Arboles de decision","text":"Los métodos basados en árboles para regresión y clasificación estratifican o segmentan el espacio predictor en varias regiones. Para hacer una predicción para una observación dada normalmente utiliza el valor de respuesta promedio de las observaciones de la base de entrenamiento en la región la que pertenece. En el caso de clasificación se asigna la categoría mayoritaria dentro del nodo terminal.Classification Regression TreeEn el caso de árboles de regresión, si \\(Y\\) es la respuesta y \\(X_1\\) y \\(X_2\\) los inputs se parte el espacio \\((X_1, X_2)\\) en dos regiones, en base una sola variable (partición horizontal o vertical). Dentro de cada región proponemos como predicción la media muestral de \\(Y\\) en cada región.Se busca elegir la variable y el punto de partición de manera óptima (mejor ajuste global). Es computacionalmente inviable considerar cada posible partición del espacio de atributos en \\(J\\) regiones. Por lo tanto, toma un enfoque top-, greedy que se conoce como división binaria recursiva. El enfoque es top-porque comienza en la parte superior del árbol (en cuyo punto todas las observaciones pertenecen una sola región) y luego divide sucesivamente el espacio predictor; cada división se indica través de dos nuevas ramas más abajo en el árbol. Es greedy porque en cada paso del proceso de construcción del árbol, la mejor división se hace en ese paso en particular, en lugar de mirar hacia adelante y elegir una división que conducirá un mejor árbol en algún paso futuro.El panel izquierdo de la Figura 7.1 muestra un árbol de regresión para predecir el logaritmo del salario (en miles de dólares) de un jugador de béisbol, basado en la cantidad de años que ha jugado en las ligas mayores y la cantidad de hits que hizo en el año anterior. En un un nodo interno dado, la etiqueta (de la forma \\(X_j < t_k\\)) indica la rama izquierda que sale de esa división, y la rama de la derecha corresponde \\(X_j \\ge t_k\\). Por ejemplo, la división en la parte superior del árbol da como resultado dos ramas grandes. El la rama izquierda corresponde Years < 4,5, y la rama derecha corresponde Years >= 4,5.13 El árbol tiene dos nodos internos y tres nodos terminales u hojas. El número en cada hoja es la media de la variable de respuesta de las observaciones que caen allí. Por ejemplo, la predicción para el nodo terminal de la izquierda es \\(e^{5,107} \\times 1.000 = \\$165.174\\). El panel derecho la Figura 7.1 muestra las regiones en función de Years y Hits.\nFigura 7.1: Arbol de regresión\nNotar:Cada región tiene su propio modelo.Cada región tiene su propio modelo.Ciertas variables importan en determinadas regiones y en otras (Hits).Ciertas variables importan en determinadas regiones y en otras (Hits).Dado \\(Y\\) y \\(X\\) un vector de \\(p\\) variables de \\(n\\) observaciones.\nEl algoritmo busca determinar cuál variable usar para la partición y que punto de esa variable usar para la partición. Si \\(j\\) es la variable de partición y el punto de partición es \\(s\\), se definen los siguientes semiplanos:\\[\\begin{align*}\n  R_1(j,s) = & {X \\mid X_j < s} \\\\\n  R_2(j,s) = & {X \\mid X_j \\ge s}\n\\end{align*}\\]Se trata de buscar la variable de partición \\(X_j\\) y el punto de partición \\(s\\) que resuelvan (minimizar el \\(MSE\\) en cada región):\\[\\begin{equation}\n\\tag{7.1}\n  \\sum_{: x_i \\R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 +  \\sum_{: x_i \\R_2(j,s)} (y_i - \\hat{y}_{R_2})^2 \n\\end{equation}\\]Donde \\(\\hat{y}_{R_1}\\) y \\(\\hat{y}_{R_2}\\) es el promedio de la respuesta en las regiones \\(1\\) y \\(2\\), respectivamente. Para cada variable de partición y punto de partición, la minimización interna se corresponde con la media dentro de cada región.¿Cuándo parar de realizar divisiones?Un árbol demasiado extenso sobreajusta (overfit) los datos. Pero dado que el proceso es secuencial y cada corte mira lo que puede suceder después, si detengo el proceso demasiado pronto puedo perder un “gran” corte más abajo. Prunning: ajustar un árbol grande y luego podarlo (prune) usando un criterio de cost-complexity.Weakest link pruningUn subárbol \\(T \\T_0\\) es un árbol que se obtiene colapsando los nodos terminales de otro árbol (cortando ramas).Cost-complexity del árbol \\(T\\):\\[\\begin{equation}\n\\tag{7.2}\n   C_{\\alpha}(T) = \\sum_{m=1}^{|T|} n_mQ_m(T) + \\alpha[T]\n\\end{equation}\\]con \\(Q_m(T) = \\frac{1}{n_m} \\sum_{x_i \\R_m} (y_i - \\hat{c}_m)^2\\) (impureza) y \\(n_m\\) cantidad de observaciones en cada partición. Entonces, el primer término mide el (mal) ajuste y el segundo la complejidad. Cuando \\(\\alpha = 0\\), entonces el subárbol \\(T\\) simplemente será igual \\(T_0\\), porque entonces (7.2) solo mide el error de entrenamiento. Sin embargo, medida que \\(\\alpha\\) aumenta, hay que pagar un costo por tener un árbol con muchos nodos terminales, por lo que (7.2) tenderá minimizarse para un subárbol más pequeño.14Objetivo: para un \\(\\alpha\\) dado, encontrar la poda óptima que minimiza \\(C_{\\alpha}(T)\\).Mecanismo de búsqueda de \\(T_{\\alpha}\\) (poda óptima dado \\(\\alpha\\)).\nResultado: para cada \\(\\alpha\\) hay un único subárbol \\(T_{\\alpha}\\) que minimiza \\(C_{\\alpha}(T)\\). Weakest link: eliminar sucesivamente las ramas que producen el mínimo incremento en \\(\\sum_{m=1}^{[T]} n_mQ_m(T)\\) (impureza). Recordar que un árbol grande aumenta la varianza, colapsamos la partición menos necesaria. Un árbol más pequeño con menos divisiones (es decir, menos regiones \\(R1,...,RJ\\)) tiene menor varianza y es más fácil de interpretar costa de un pequeño sesgo.El proceso eventualmente colapsa en el nodo inicial, pero pasa por una sucesión de árboles, desde el más grande, hasta el más chico, por el proceso de weakest link pruning. El árbol óptimo \\(T_{\\alpha}\\) pertenece esta sucesión.Classification treeUn árbol de clasificación es muy similar un árbol de regresión, excepto que se utiliza para predecir una respuesta cualitativa en lugar de una cuantitativa. Recordar que para un árbol de regresión, la respuesta predicha para una observación esta dada por la respuesta media de las observaciones de entrenamiento que pertenecen al mismo nodo terminal. En contraste, para un árbol de clasificación, predecimos que cada observación pertenece la clase que ocurre más comúnmente en las observaciones de entrenamiento en la región la que pertenece. Se basa en el error de clasificación o índice de Gini (pureza), análogo \\(RSS\\) en un árbol de regresión.","code":""},{"path":"arboles.html","id":"bagging","chapter":"Capítulo 7 Arboles de decision","heading":"7.1 Bagging","text":"Ventajas y desventajas de \\(CART\\):Forma inteligente de representar linealidades.Forma inteligente de representar linealidades.Arriba quedan las variables más relevantes entonces es fácil de comunicar. Reproduce proceso decisorio humano.Arriba quedan las variables más relevantes entonces es fácil de comunicar. Reproduce proceso decisorio humano.Si la estructura es lineal, \\(CART\\) anda bien.Si la estructura es lineal, \\(CART\\) anda bien.Poco robusto, variaciones en los datos modifican el resultado.Poco robusto, variaciones en los datos modifican el resultado.Un método de ensemble es un enfoque que combina muchos modelos simples en uno único y potencialmente muy poderoso. Los modelos simples se conocen como modelos de aprendizaje débil, ya que por sí mismos pueden generar predicciones mediocres.Bootstrap aggregationBootstrap training sets: tomar como predicción el promedio de las\npredicciones bootstrap.\\[\\begin{equation}\n\\tag{7.3}\n   \\hat{f}_{bag} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}^{*b}(x)\n\\end{equation}\\]Idea: la varianza del promedio es menor que la de una predicción sola. Bajo independencia si \\(V(x) = \\sigma^2\\) entonces \\(V(\\overline{x}) = \\frac{\\sigma^2}{n}\\).Problema: si hay un predictor fuerte, distintos árboles son muy similares entre sí y, por lo tanto, alta correlación.","code":""},{"path":"arboles.html","id":"random-forest","chapter":"Capítulo 7 Arboles de decision","heading":"7.2 Random Forest","text":"Busca bajar la correlación entre los árboles en el bootstrap. Al igual que en bagging, construye una serie de árboles de decisión en muestras de entrenamiento bootstrap. Pero al construir estos árboles de decisión, cada vez que se considera una división en un árbol, se elige como candidatos de división una muestra aleatoria de \\(m\\) predictores del conjunto completo de \\(p\\) predictores (\\(m < p\\)).","code":""},{"path":"arboles.html","id":"boosting","chapter":"Capítulo 7 Arboles de decision","heading":"7.3 Boosting","text":"Boosting funciona de manera similar bagging, excepto que los árboles van crecido secuencialmente: cada árbol crece usando información de árboles elaborados previamente. Boosting implica un muestreo bootstrap; en cambio cada árbol se ajusta una versión modificada de la base de datos original.Weak classifier: clasificador marginalmente mejor que tirar una moneda. Tasa de error apenas mejor que \\(0,5\\). Por ejemplo, \\(CART\\) con pocas ramas (stump, clasificador con dos ramas).Boosting: promedio ponderado de una sucesión de clasificadores débiles. Notable mejora.Definiciones\\(y \\-1,1\\)Clasificador = \\(\\hat{y} = G(X)\\)Error de predicción = \\(\\frac{1}{N} \\sum_{=1}^{N}(y_i \\neq G(x_i))\\)","code":""},{"path":"arboles.html","id":"ada-boost","chapter":"Capítulo 7 Arboles de decision","heading":"7.3.1 Ada Boost","text":"Comienza con con pesos \\(w_i = \\frac{1}{N}\\)Comienza con con pesos \\(w_i = \\frac{1}{N}\\)Para \\(m = 1,...,M\\):Para \\(m = 1,...,M\\):Calcula una predicciónCalcula una predicciónCalcula el error de predicción agregadoCalcula el error de predicción agregadoCalcula \\(\\alpha_m = ln[\\frac{1 - err_m}{err_m}]\\)Calcula \\(\\alpha_m = ln[\\frac{1 - err_m}{err_m}]\\)Actualiza los ponderadores \\(w_i\\) \\(\\leftarrow\\) \\(w_ic_i\\)Actualiza los ponderadores \\(w_i\\) \\(\\leftarrow\\) \\(w_ic_i\\)con \\(c_i =\\) exp \\([\\alpha_m \\underbrace{(y_i \\neq G(x_i))}_{{0/1}}]\\)Output: \\(G(x) =\\) sgn \\([\\sum_{m=1}^{M} \\alpha_m G_m(x)]\\) (signo del promedio).Si \\(\\) estuvo correctamente predicha, \\(c_i = 1\\), entonces hay ajuste. Caso contrario, \\(c_i =\\) exp\\((\\alpha_m) = \\frac{1 - err_m}{err_m} > 1\\). Notar que si siempre predigo la clase mayoritaria la tasa de error nunca puede ser mayor al \\(50\\%\\) y por eso la expresión anterior es mayor \\(1\\).En cada paso el método da más importancia relativa las observaciones mal predichas. Paso final: promedio ponderado de predicciones en cada paso.\nFigura 7.2: Ada boost\n","code":""},{"path":"arboles.html","id":"aplicacion-practica-3","chapter":"Capítulo 7 Arboles de decision","heading":"7.4 Aplicacion practica","text":"La biblioteca tree se utiliza para construir árboles de clasificación y regresión.Primero usamos árboles de clasificación para analizar la base de datos Carseats. Sales es una variable continua, por lo que comenzamos recodificándola como una variable binaria. Usamos la función ifelse() para crear una variable, llamada High, que toma un valor de Yes si la variable Sales excede \\(8\\) y toma un valor de en caso contrario.Finalmente, usamos la función data.frame() para unir High con el resto de los datos de Carseats.Ahora usamos la función tree() para ajustar un árbol de clasificación con el fin de predecir High usando todas las variables excepto Sales. La sintaxis de la función tree() es bastante similar la de la función lm().Vemos que la tasa de error de entrenamiento es \\(9\\%\\). Para árboles de clasificación, la desviación reportada en la salida de summary() es\ndada por:\\[\n-2 \\sum_m \\sum_k n_{mk} \\log \\hat{p}_{mk},\n\\]donde \\(n_{mk}\\) es el número de observaciones en el nodo terminal \\(m\\) que pertenecen la clase \\(k\\). Una desviación pequeña indica un árbol que proporciona un buen ajuste los datos (de entrenamiento). La desviación media residual informada es simplemente la desviación dividida por \\(n-|{T}_0|\\), que en este caso es \\(400-27=373\\).Una de las propiedades más atractivas de los árboles es que se pueden representar gráficamente. Usamos la función plot() para mostrar la estructura de árbol y la función text() para mostrar las etiquetas de los nodos. El argumento pretty = 0 indica R que incluya los nombres de categoría para cualquier predictor cualitativo, en lugar de simplemente mostrar una letra para cada categoría.La variable más importante para Sales parece ser la ubicación de las estanterías, ya que la primera rama diferencia las ubicaciones Good de las ubicaciones Bad y Medium.Si solo escribimos el nombre del objeto del árbol, R imprime la salida correspondiente cada rama del árbol. R muestra el criterio de división (ej. Price < 92,5), el número de observaciones en esa rama, el desvío, la predicción general para la rama (Yes o ) y la fracción de observaciones en esa rama que toma valores de Yes y . Las ramas que conducen los nodos terminales se indican con asteriscos.Para evaluar correctamente la performance de un árbol de clasificación\ncon estos datos, debemos estimar el error de test en lugar de calcular el error de entrenamiento. Dividimos las observaciones en la base de entrenamiento y una base de test, se construye el árbol usando la base de entrenamiento, y se evalúa su desempeño en los datos en test. La función predict() se usa para este propósito. En el caso de un árbol de clasificación, el argumento type = \"class\" instruye R para devolver una predicción de clase. Este modelo produce predicciones correctas en alrededor del \\(77\\%\\) de los casos en la base de datos de test.continuación, consideramos si podar el árbol podría llevar mejores resultados. La función cv.tree() realiza una cross-validation para determinar el nivel óptimo de complejidad del árbol; se utiliza cost complexity pruning para seleccionar una secuencia de árboles.Usamos el argumento FUN = prune.misclass para indicar que queremos que la tasa de error de clasificación guíe el proceso de cross-validation y poda, en lugar del valor predeterminado para la función cv.tree(), que es el desvío. La función cv.tree() reporta el número de nodos terminales de cada árbol considerado (size), así como la tasa de error correspondiente y el valor del parámetro cost complexity utilizado (k, que corresponde $ $ en (7.2)).pesar de su nombre, dev corresponde al número de errores de cross-validation. El árbol con 9 nodos terminales da como resultado solo 74 errores de cross-validation. Graficamos la tasa de error en función tanto del size y k.Ahora aplicamos la función prune.misclass() para podar el árbol y obtener el árbol de nueve nodos.¿Qué tan bien se desempeña este árbol podado en la base de datos de test? Una vez más, aplicamos la función predict().Ahora el \\(77,5\\%\\) de las observaciones de test se clasifican correctamente, por lo que el proceso de poda solo produjo un árbol más interpretable, sino que también mejoró ligeramente la precisión de la clasificación.Si aumentamos el valor de best, obtenemos un árbol podado más grande con menor precisión de clasificación:","code":"\nlibrary(tree)\nlibrary(ISLR2)\nattach(Carseats)\nHigh <- factor(ifelse(Sales <= 8, 'No', 'Yes'))\nCarseats <- data.frame(Carseats, High)\ntree.carseats <- tree(High ~ . - Sales, Carseats)\nsummary(tree.carseats)## \n## Classification tree:\n## tree(formula = High ~ . - Sales, data = Carseats)\n## Variables actually used in tree construction:\n## [1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n## [6] \"Advertising\" \"Age\"         \"US\"         \n## Number of terminal nodes:  27 \n## Residual mean deviance:  0.4575 = 170.7 / 373 \n## Misclassification error rate: 0.09 = 36 / 400\nplot(tree.carseats)\ntext(tree.carseats, pretty = 0)\ntree.carseats## node), split, n, deviance, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 400 541.500 No ( 0.59000 0.41000 )  \n##     2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 )  \n##       4) Price < 92.5 46  56.530 Yes ( 0.30435 0.69565 )  \n##         8) Income < 57 10  12.220 No ( 0.70000 0.30000 )  \n##          16) CompPrice < 110.5 5   0.000 No ( 1.00000 0.00000 ) *\n##          17) CompPrice > 110.5 5   6.730 Yes ( 0.40000 0.60000 ) *\n##         9) Income > 57 36  35.470 Yes ( 0.19444 0.80556 )  \n##          18) Population < 207.5 16  21.170 Yes ( 0.37500 0.62500 ) *\n##          19) Population > 207.5 20   7.941 Yes ( 0.05000 0.95000 ) *\n##       5) Price > 92.5 269 299.800 No ( 0.75465 0.24535 )  \n##        10) Advertising < 13.5 224 213.200 No ( 0.81696 0.18304 )  \n##          20) CompPrice < 124.5 96  44.890 No ( 0.93750 0.06250 )  \n##            40) Price < 106.5 38  33.150 No ( 0.84211 0.15789 )  \n##              80) Population < 177 12  16.300 No ( 0.58333 0.41667 )  \n##               160) Income < 60.5 6   0.000 No ( 1.00000 0.00000 ) *\n##               161) Income > 60.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n##              81) Population > 177 26   8.477 No ( 0.96154 0.03846 ) *\n##            41) Price > 106.5 58   0.000 No ( 1.00000 0.00000 ) *\n##          21) CompPrice > 124.5 128 150.200 No ( 0.72656 0.27344 )  \n##            42) Price < 122.5 51  70.680 Yes ( 0.49020 0.50980 )  \n##              84) ShelveLoc: Bad 11   6.702 No ( 0.90909 0.09091 ) *\n##              85) ShelveLoc: Medium 40  52.930 Yes ( 0.37500 0.62500 )  \n##               170) Price < 109.5 16   7.481 Yes ( 0.06250 0.93750 ) *\n##               171) Price > 109.5 24  32.600 No ( 0.58333 0.41667 )  \n##                 342) Age < 49.5 13  16.050 Yes ( 0.30769 0.69231 ) *\n##                 343) Age > 49.5 11   6.702 No ( 0.90909 0.09091 ) *\n##            43) Price > 122.5 77  55.540 No ( 0.88312 0.11688 )  \n##              86) CompPrice < 147.5 58  17.400 No ( 0.96552 0.03448 ) *\n##              87) CompPrice > 147.5 19  25.010 No ( 0.63158 0.36842 )  \n##               174) Price < 147 12  16.300 Yes ( 0.41667 0.58333 )  \n##                 348) CompPrice < 152.5 7   5.742 Yes ( 0.14286 0.85714 ) *\n##                 349) CompPrice > 152.5 5   5.004 No ( 0.80000 0.20000 ) *\n##               175) Price > 147 7   0.000 No ( 1.00000 0.00000 ) *\n##        11) Advertising > 13.5 45  61.830 Yes ( 0.44444 0.55556 )  \n##          22) Age < 54.5 25  25.020 Yes ( 0.20000 0.80000 )  \n##            44) CompPrice < 130.5 14  18.250 Yes ( 0.35714 0.64286 )  \n##              88) Income < 100 9  12.370 No ( 0.55556 0.44444 ) *\n##              89) Income > 100 5   0.000 Yes ( 0.00000 1.00000 ) *\n##            45) CompPrice > 130.5 11   0.000 Yes ( 0.00000 1.00000 ) *\n##          23) Age > 54.5 20  22.490 No ( 0.75000 0.25000 )  \n##            46) CompPrice < 122.5 10   0.000 No ( 1.00000 0.00000 ) *\n##            47) CompPrice > 122.5 10  13.860 No ( 0.50000 0.50000 )  \n##              94) Price < 125 5   0.000 Yes ( 0.00000 1.00000 ) *\n##              95) Price > 125 5   0.000 No ( 1.00000 0.00000 ) *\n##     3) ShelveLoc: Good 85  90.330 Yes ( 0.22353 0.77647 )  \n##       6) Price < 135 68  49.260 Yes ( 0.11765 0.88235 )  \n##        12) US: No 17  22.070 Yes ( 0.35294 0.64706 )  \n##          24) Price < 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n##          25) Price > 109 9  11.460 No ( 0.66667 0.33333 ) *\n##        13) US: Yes 51  16.880 Yes ( 0.03922 0.96078 ) *\n##       7) Price > 135 17  22.070 No ( 0.64706 0.35294 )  \n##        14) Income < 46 6   0.000 No ( 1.00000 0.00000 ) *\n##        15) Income > 46 11  15.160 Yes ( 0.45455 0.54545 ) *\nset.seed(2)\ntrain <- sample(1:nrow(Carseats), 200)\nCarseats.test <- Carseats[-train, ]\nHigh.test <- High[-train]\ntree.carseats <- tree(High ~ . - Sales, Carseats,\n    subset = train)\ntree.pred <- predict(tree.carseats, Carseats.test,\n    type = \"class\")\ntable(tree.pred, High.test)##          High.test\n## tree.pred  No Yes\n##       No  104  33\n##       Yes  13  50\n(104 + 50) / 200## [1] 0.77\nset.seed(7)\ncv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)## [1] \"size\"   \"dev\"    \"k\"      \"method\"\ncv.carseats## $size\n## [1] 21 19 14  9  8  5  3  2  1\n## \n## $dev\n## [1] 75 75 75 74 82 83 83 85 82\n## \n## $k\n## [1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n## \n## $method\n## [1] \"misclass\"\n## \n## attr(,\"class\")\n## [1] \"prune\"         \"tree.sequence\"\npar(mfrow = c(1, 2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\nprune.carseats <- prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\ntree.pred <- predict(prune.carseats, Carseats.test,\n    type = \"class\")\ntable(tree.pred, High.test)##          High.test\n## tree.pred No Yes\n##       No  97  25\n##       Yes 20  58\n(97 + 58) / 200## [1] 0.775\nprune.carseats <- prune.misclass(tree.carseats, best = 14)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\ntree.pred <- predict(prune.carseats, Carseats.test,\n    type = \"class\")\ntable(tree.pred, High.test)##          High.test\n## tree.pred  No Yes\n##       No  102  31\n##       Yes  15  52\n(102 + 52) / 200## [1] 0.77"},{"path":"arboles.html","id":"arboles-de-regresion","chapter":"Capítulo 7 Arboles de decision","heading":"7.4.1 Arboles de regresion","text":"Aquí ajustamos un árbol de regresión la base de datos Boston. Primero, creamos una base de entrenamiento y ajustamos el árbol esos datos.Notar que la salida de summary() indica que solo cuatro de las variables han sido usadas en la construcción del árbol. En el contexto de un árbol de regresión, el desvío es simplemente la suma de los errores al cuadrado del árbol. Ahora graficamos el árbol.La variable lstat mide el porcentaje de personas con bajo nivel socioeconómico, mientras que la variable rm corresponde al número promedio de habitaciones. El árbol indica que valores más altos de rm, o valores más bajos de lstat, corresponden casas más caras. Por ejemplo, el árbol predice un precio medio de vivienda de \\(\\$45.400\\) para viviendas en distritos censales en los que rm >= 7,553.Podríamos haber estimado un árbol mucho más grande, usando el argumento control = tree.control(nobs = length(train), mindev = 0) en la función tree().Ahora usamos la función cv.tree() para ver si podar el árbol mejorará el rendimiento.En este caso, el árbol más complejo se selecciona mediante cross-validation. Sin embargo, si deseamos podar el árbol, podemos hacerlo de la siguiente manera, usando la función prune.tree():De acuerdo con los resultados de la cross-validation, usamos el árbol podado para hacer predicciones en la base de test.En otras palabras, el \\(MSE\\) de la la base de test asociado con el árbol de regresión es de \\(35,29\\). Por lo tanto, la raíz cuadrada del \\(MSE\\) es de alrededor de \\(5,941\\), lo que indica que este modelo lleva predicciones de test que están (en promedio) dentro de aproximadamente \\(\\$5.941\\) del valor medio real de la vivienda para el distrito censal.","code":"\nset.seed(1)\ntrain <- sample(1:nrow(Boston), nrow(Boston) / 2)\ntree.boston <- tree(medv ~ ., Boston, subset = train)\nsummary(tree.boston)## \n## Regression tree:\n## tree(formula = medv ~ ., data = Boston, subset = train)\n## Variables actually used in tree construction:\n## [1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \n## Number of terminal nodes:  7 \n## Residual mean deviance:  10.38 = 2555 / 246 \n## Distribution of residuals:\n##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n## -10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\ncv.boston <- cv.tree(tree.boston)\nplot(cv.boston$size, cv.boston$dev, type = \"b\")\nprune.boston <- prune.tree(tree.boston, best = 5)\nplot(prune.boston)\ntext(prune.boston, pretty = 0)\nyhat <- predict(tree.boston, newdata = Boston[-train, ])\nboston.test <- Boston[-train, \"medv\"]\nplot(yhat, boston.test)\nabline(0, 1)\nmean((yhat - boston.test)^2)## [1] 35.28688"},{"path":"arboles.html","id":"bagging-y-random-forests","chapter":"Capítulo 7 Arboles de decision","heading":"7.4.2 Bagging y Random Forests","text":"Aquí aplicamos bagging y random forests los datos de Boston, usando el paquete randomForest en R. Los resultados exactos obtenidos en esta sección pueden depender de la versión de R y la versión del paquete randomForest instalado. Recuerde que el bagging es simplemente un caso especial de un random forests con\n\\(m = p\\). Por lo tanto, la función randomForest() se puede utilizar para realizar random forests y bagging. Realizamos el bagging de la siguiente manera:El argumento mtry = 12 indica que se deben considerar todos los predictores (hay \\(12\\) en total) para cada división del árbol; en otras palabras, se debe realizar el bagging.\n¿Qué tan bien se desempeña este modelo en la base de test?El \\(MSE\\) de la base de test asociado con el árbol de regresión bagging es de \\(23,42\\), aproximadamente dos tercios del obtenido utilizando un árbol único podado de manera óptima.\nPodríamos cambiar el número de árboles elaborados por randomForest() usando el argumento ntree:El estimación de un random forests procede exactamente de la misma manera, excepto que usamos un valor más pequeño del argumento mtry. Por defecto, randomForest() usa \\(p/3\\) variables cuando construye un random forests de árboles de regresión, y \\(\\sqrt{p}\\) variables cuando construye un random forests de árboles de clasificación. Aquí usamos mtry = 6.El \\(MSE\\) de la base de datos de test es de \\(20,07\\); esto indica que random forests produjo una mejora con respecto bagging en este caso.Usando la función importance(), podemos ver la importancia de cada variable.Se reportan dos medidas de importancia variable. El primero se basa en la disminución media de la precisión en las predicciones sobre las muestras bag cuando se permuta una variable determinada. La segunda es una medida de la disminución total en la impureza de los nodos que resulta de las divisiones sobre esa variable, promediada sobre todos los árboles. En el caso de los árboles de regresión, la impureza del nodo se mide por el \\(RSS\\) de entrenamiento, y para los árboles de clasificación por el desvío. Los gráficos de estas medidas de importancia se pueden producir utilizando la función varImpPlot().Los resultados indican que en todos los árboles considerados en el random forests, la riqueza de la comunidad (lstat) y el tamaño de la casa (rm) son las dos variables más importantes.","code":"\nlibrary(randomForest)\nset.seed(1)\nbag.boston <- randomForest(medv ~ ., data = Boston,\n    subset = train, mtry = 12, importance = TRUE)\nbag.boston## \n## Call:\n##  randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) \n##                Type of random forest: regression\n##                      Number of trees: 500\n## No. of variables tried at each split: 12\n## \n##           Mean of squared residuals: 11.40162\n##                     % Var explained: 85.17\nyhat.bag <- predict(bag.boston, newdata = Boston[-train, ])\nplot(yhat.bag, boston.test)\nabline(0, 1)\nmean((yhat.bag - boston.test)^2)## [1] 23.41916\nbag.boston <- randomForest(medv ~ ., data = Boston,\n    subset = train, mtry = 12, ntree = 25)\nyhat.bag <- predict(bag.boston, newdata = Boston[-train, ])\nmean((yhat.bag - boston.test)^2)## [1] 25.75055\nset.seed(1)\nrf.boston <- randomForest(medv ~ ., data = Boston,\n    subset = train, mtry = 6, importance = TRUE)\nyhat.rf <- predict(rf.boston, newdata = Boston[-train, ])\nmean((yhat.rf - boston.test)^2)## [1] 20.06644\nimportance(rf.boston)##           %IncMSE IncNodePurity\n## crim    19.435587    1070.42307\n## zn       3.091630      82.19257\n## indus    6.140529     590.09536\n## chas     1.370310      36.70356\n## nox     13.263466     859.97091\n## rm      35.094741    8270.33906\n## age     15.144821     634.31220\n## dis      9.163776     684.87953\n## rad      4.793720      83.18719\n## tax      4.410714     292.20949\n## ptratio  8.612780     902.20190\n## lstat   28.725343    5813.04833\nvarImpPlot(rf.boston)"},{"path":"arboles.html","id":"boosting-1","chapter":"Capítulo 7 Arboles de decision","heading":"7.4.3 Boosting","text":"Aquí usamos el paquete gbm y la función gbm(), para ajustar árboles de regresión boosting la base de datos Boston. Ejecutamos gbm() con la opción distribution = \"gaussian\" ya que este es un problema de regresión; si fuera un problema de clasificación binaria, usaríamos distribution = \"bernoulli\".\nEl argumento n.trees = 5000 indica que queremos árboles de \\(5.000\\), y la opción interaction. depth = 4 limita la profundidad de cada árbol.La función summary() produce un gráfico y estadísticas de influencia relativa.Nuevamente, vemos que lstat y rm son las variables más importantes. También podemos producir gráficos de dependencia parcial para estas dos variables. Estos gráficos ilustran el efecto marginal de las variables seleccionadas en la respuesta después de integrar las otras variables. En este caso, como cabría esperar, los precios medios de la vivienda aumentan con rm y disminuyen con lstat.Ahora usamos el modelo boosting para predecir medv en la base de test:El \\(MSE\\) de test obtenido es de \\(18,39\\): esto es superior al \\(MSE\\) de test de random forests y bagging. Podemos realizar un boosting con un valor diferente del parámetro de contracción \\(\\lambda\\). El valor predeterminado es \\(0,001\\), pero esto se modifica fácilmente. Aquí tomamos \\(\\lambda=0,2\\).En este caso, usar \\(\\lambda=0,2\\) produce un \\(MSE\\) de prueba más bajo que \\(\\lambda=0,001\\).","code":"\nlibrary(gbm)\nset.seed(1)\nboost.boston <- gbm(medv ~ ., data = Boston[train, ],\n    distribution = \"gaussian\", n.trees = 5000,\n    interaction.depth = 4)\nsummary(boost.boston)##             var     rel.inf\n## rm           rm 44.48249588\n## lstat     lstat 32.70281223\n## crim       crim  4.85109954\n## dis         dis  4.48693083\n## nox         nox  3.75222394\n## age         age  3.19769210\n## ptratio ptratio  2.81354826\n## tax         tax  1.54417603\n## indus     indus  1.03384666\n## rad         rad  0.87625748\n## zn           zn  0.16220479\n## chas       chas  0.09671228\nplot(boost.boston, i = \"rm\")\nplot(boost.boston, i = \"lstat\")\nyhat.boost <- predict(boost.boston,\n    newdata = Boston[-train, ], n.trees = 5000)\nmean((yhat.boost - boston.test)^2)## [1] 18.39057\nboost.boston <- gbm(medv ~ ., data = Boston[train, ],\n    distribution = \"gaussian\", n.trees = 5000,\n    interaction.depth = 4, shrinkage = 0.2, verbose = F)\nyhat.boost <- predict(boost.boston,\n    newdata = Boston[-train, ], n.trees = 5000)\nmean((yhat.boost - boston.test)^2)## [1] 16.54778"},{"path":"cluster.html","id":"cluster","chapter":"Capítulo 8 Analisis de clusters","heading":"Capítulo 8 Analisis de clusters","text":"Clustering se refiere un conjunto amplio de técnicas para encontrar subgrupos o clusteres, en una base de datos. Cuando agrupamos las observaciones de una base de datos, se busca dividirlos en grupos distintos para que las observaciones dentro cada grupo sean bastante similares entre sí, mientras que las observaciones en grupos diferentes sean muy distintas entre sí. Por supuesto, se debe definir qué significa que dos o más observaciones sean similares o diferentes.Si \\(X\\) es una matriz de \\(n\\) filas (observaciones) y \\(p\\) columnas (variables), cada fila es un “punto” de \\(p\\) dimensiones y cada columna se corresponde con una variable. Por ejemplo, una base con 40 alumnos y cuatro preguntas en un examen (cada alumno es un “punto”).Clustering busca armar grupos de puntos con esos datos. Este es un problema supervisado porque estamos tratando de descubrir estructura, grupos distintos, en base los datos. Útil para segmentación de mercado.DissimilaritySuponer dos puntos \\(x, z \\\\Re^p\\). ¿Cuán disímiles son \\(y\\) y \\(z\\), con \\(p\\) coordenadas cada uno?Variables cuantitativas, distancia euclídea (agrega disimilitudes para cada atributo):\\[\\begin{equation}\n\\tag{8.1}\n   d(x,z) = [\\sum_{j=1}^{p} (x_j - z_j)^{2}]^{\\frac{1}{2}}\n\\end{equation}\\]Distancia de Minkowski:\\[\\begin{equation}\n\\tag{8.2}\n   d(x,z) = [ \\sum_{j=1}^{p} |x_j - z_j|^{m}]^{\\frac{1}{2}}\n\\end{equation}\\]Dissimilarity Matrix\\(D\\) es una matriz \\(N \\times N\\) donde \\(D_{ij} = D(x_i, x_j)\\) es el input del análisis de clusters. Idealmente las \\(D_{ij}\\) son verdaderas distancias, de modo que \\(D\\) es simétrica y con diagonal principal nula, es siempre el caso. El análisis es muy sensible la elección de \\(D\\).Cluster analysisCada punto, indizado por \\(\\{1,...,N}\\). Supongamos que sabemos de antemano que hay \\(K\\) clusters. Cada cluster, indizado por \\(k \\{1,...,K}\\).\nUn mecanismo de clusters asigna cada punto un solo cluster\\(k = C()\\)\\(C()\\) = “enconder”\\(C()\\): \\({1,...,N} \\{1,...,K}\\).Análisis de Cluster: encontrar \\(C^{*}()\\) óptimo, en base la matriz de dissimilarities.Consideremos la siguiente función de pérdida:\\[\\begin{equation}\n\\tag{8.3}\n   W(C) = \\frac{1}{2} \\sum_{k=1}^{K} [\\sum_{,j/C() = C(j) = k} d(x_i,x_j)]\n\\end{equation}\\]Intuitivamente: \\(W(C)\\) agrega las disimilitudes dentro de cada cluster.Notar que:\\[\\begin{align*}\n T = & \\frac{1}{2} \\sum_{,j} d_{,j} \\\\\n   = & \\frac{1}{2} \\sum_{k=1}^{K} [\\sum_{,j/C() = C(j) = k} d(x_i,x_j) + \\sum_{k=1}^{K} [\\sum_{,j/C() = C(j) \\neq k} d(x_i,x_j)] \\\\\n   = & W(C) + B(C)\n\\end{align*}\\]\\(T\\) es la disimilitud total, entre todas las observaciones (depende\nde la clusterización). \\(B(C)\\) es la agregación de las distancias entre clusters. Entonces, si se propone como objetivo minimizar la disimilitud dentro de los clusters, como \\(T\\) esta fijo, minimizar \\(W(C)\\) es equivalente maximizar \\(B(C)\\).","code":""},{"path":"cluster.html","id":"k-means-clustering","chapter":"Capítulo 8 Analisis de clusters","heading":"8.1 K-Means Clustering","text":"En K-Means Clustering se busca dividir las observaciones en un número preespecificado de grupos. Luego el algoritmo asignará cada observación exactamente uno de los \\(K\\) clusters.La Figura 8.1 muestra los resultados obtenidos al realizar K-means clustering en un ejemplo simulado con 150 observaciones en dos dimensiones utilizando tres valores diferentes de \\(K\\).\nFigura 8.1: Clusters\n\\[\\begin{equation}\n\\tag{8.4}\n   W(C) = \\sum_{k=1}^{K} N_k \\sum_{C() = k} || x_i - \\overline{x}_k ||^2\n\\end{equation}\\]donde \\(\\overline{x}_k = (\\overline{x}_{1k},...,\\overline{x}_{pk})\\) es el vector de medias asociado con el cluster \\(k\\) y \\(N_k = \\sum_{=1}^{N} (C()=k)\\).Algoritmo de k-means:Asignar aleatoriamente un número, del \\(1\\) al \\(K\\), cada una de las observaciones. Estos sirven como asignaciones de grupos iniciales para las observaciones.Asignar aleatoriamente un número, del \\(1\\) al \\(K\\), cada una de las observaciones. Estos sirven como asignaciones de grupos iniciales para las observaciones.Iterar hasta que las asignaciones de clusteres dejen de cambiar.Iterar hasta que las asignaciones de clusteres dejen de cambiar.Para cada uno de los \\(K\\) cluster, calcular el centroide del cluster. El k-ésimo centroide del cluster es el vector de \\(p\\) medias de atributos para las observaciones en el k-ésimo cluster.Para cada uno de los \\(K\\) cluster, calcular el centroide del cluster. El k-ésimo centroide del cluster es el vector de \\(p\\) medias de atributos para las observaciones en el k-ésimo cluster.Asignar cada observación al cluster cuyo centroide esté más cerca\n(usando la distancia euclidiana)Asignar cada observación al cluster cuyo centroide esté más cerca\n(usando la distancia euclidiana)\nFigura 8.2: Algoritmo\nIdea: el mecanismo optimiza primero dentro del cluster (elige las medias) y luego optimiza reasignando las observaciones, dejando quietas las medias.Dado que el algoritmo de K-means encuentra un óptimo local en lugar de un óptimo global, los resultados obtenidos dependerán de la asignación inicial (aleatoria) de clusters de cada observación en el paso \\(1\\) del algoritmo. Por esta razón, es importante ejecutar el algoritmo varias veces desde diferentes puntos aleatorios iniciales. Luego se selecciona la mejor solución, es decir, aquella para la cual el objetivo (8.4) es el más pequeño.La Figura 8.3 muestra los óptimos locales obtenidos\nejecutando K-means clustering seis veces usando seis asignaciones iniciales diferentes.\nFigura 8.3: Algoritmo\nCuestiones prácticasInicialización: puede ser en base clusters o medias.Número de clusters: hay un mecanismo comúnmente aceptado. En algunos casos es exógeno.Cuestión: La within dissimilarity \\(W(C)\\) cae con el número de clusters.\\(K\\) óptimo se corresponde con un quiebre en el dibujo de \\(W(C)\\) incrementando la cantidad de clusters (trade-sesgo-varianza determinado por cantidad de clusters).","code":""},{"path":"cluster.html","id":"aplicacion-practica-4","chapter":"Capítulo 8 Analisis de clusters","heading":"8.2 Aplicacion practica","text":"La función kmeans() realiza K-means clustering en R. Comenzamos con un ejemplo simulado simple en el que hay realmente son dos grupos en los datos: las primeras 25 observaciones tienen un cambio medio relativo las siguientes 25 observaciones.Ahora estimamos K-means clustering con \\(K = 2\\)Las asignaciones de clusters de las 50 observaciones están contenidas en km.$cluster.El K-means clustering separó perfectamente las observaciones en dos grupos pesar de que proporcionamos ninguna información de grupo kmeans(). Podemos graficar los datos, con cada observación\ncoloreada de acuerdo con su asignación de grupo.En este ejemplo, sabíamos que realmente había dos clusteres porque generamos los datos. Sin embargo, para datos reales, en general \nse conoce el verdadero número de clusters. Podríamos haber realizado K-means clustering en este ejemplo con \\(K = 3\\).Cuando \\(K=3\\), K-means clustering divide los dos grupos.Para ejecutar la función kmeans() en R con múltiples asignaciones de clusteres iniciales, usamos el argumento nstart. Si se usa un valor de nstart mayor que uno, entonces K-means clustering se realizará usando múltiples asignaciones aleatorias en el Paso 1 del algoritmo, y la función kmeans() reportará solo los mejores resultados. Aquí comparamos el uso de nstart = 1 con nstart = 20.Tener en cuenta que km.$tot.withinss es la suma total de cuadrados dentro del grupo, que buscamos minimizar realizando K-means clustering. Las sumas de cuadrados individuales dentro del grupo están contenidas en el vector km.$withinss.Se recomienda ejecutar siempre K-means clustering con un valor grande de nstart, como 20 o 50, ya que de lo contrario puede obtenerse un óptimo local indeseable.Al realizar un K-means clustering, además de utilizar varias asignaciones de agrupamiento iniciales, también es importante establecer una semilla aleatoria usando la función set.seed(). De esta manera, las asignaciones de grupos iniciales en el Paso 1 pueden ser replicadas y la salida de K-means será totalmente reproducible.","code":"\nset.seed(2)\nx <- matrix(rnorm(50 * 2), ncol = 2)\nx[1:25, 1] <- x[1:25, 1] + 3\nx[1:25, 2] <- x[1:25, 2] - 4\nkm.out <- kmeans(x, 2, nstart = 20)\nkm.out$cluster##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [39] 2 2 2 2 2 2 2 2 2 2 2 2\nplot(x, col = (km.out$cluster + 1),\n    main = \"K-Means Clustering Results with K = 2\",\n    xlab = \"\", ylab = \"\", pch = 20, cex = 2)\nset.seed(4)\nkm.out <- kmeans(x, 3, nstart = 20)\nkm.out## K-means clustering with 3 clusters of sizes 17, 23, 10\n## \n## Cluster means:\n##         [,1]        [,2]\n## 1  3.7789567 -4.56200798\n## 2 -0.3820397 -0.08740753\n## 3  2.3001545 -2.69622023\n## \n## Clustering vector:\n##  [1] 1 3 1 3 1 1 1 3 1 3 1 3 1 3 1 3 1 1 1 1 1 3 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [39] 2 2 2 2 2 3 2 3 2 2 2 2\n## \n## Within cluster sum of squares by cluster:\n## [1] 25.74089 52.67700 19.56137\n##  (between_SS / total_SS =  79.3 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nplot(x, col = (km.out$cluster + 1),\n    main = \"K-Means Clustering Results with K = 3\",\n    xlab = \"\", ylab = \"\", pch = 20, cex = 2)\nset.seed(4)\nkm.out <- kmeans(x, 3, nstart = 1)\nkm.out$tot.withinss## [1] 104.3319\nkm.out <- kmeans(x, 3, nstart = 20)\nkm.out$tot.withinss## [1] 97.97927"},{"path":"bibliografia.html","id":"bibliografia","chapter":"Bibliografia","heading":"Bibliografia","text":"","code":""}]
