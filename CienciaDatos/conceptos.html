<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 3 Conceptos generales | Ciencia de Datos</title>
  <meta name="description" content="BCRA" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 3 Conceptos generales | Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="BCRA" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 3 Conceptos generales | Ciencia de Datos" />
  <meta name="twitter:site" content="@msangia" />
  <meta name="twitter:description" content="BCRA" />
  

<meta name="author" content="Máximo Sangiácomo" />


<meta name="date" content="2022-02-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bd.html"/>
<link rel="next" href="mco.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="r4ds.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Descripcion del curso</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduccion a R</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#primeros-pasos"><i class="fa fa-check"></i><b>1.1</b> Primeros pasos</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#busacar-ayuda"><i class="fa fa-check"></i><b>1.2</b> Busacar ayuda</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#tipos-de-datos"><i class="fa fa-check"></i><b>1.3</b> Tipos de datos</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#limpieza-de-memoria"><i class="fa fa-check"></i><b>1.4</b> Limpieza de memoria</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#asignación-de-valores"><i class="fa fa-check"></i><b>1.5</b> Asignación de valores</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#operadores-aritméticos"><i class="fa fa-check"></i><b>1.6</b> Operadores aritméticos</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#operadores-relacionales"><i class="fa fa-check"></i><b>1.7</b> Operadores relacionales</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#operadores-lógicos"><i class="fa fa-check"></i><b>1.8</b> Operadores lógicos</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#vectores"><i class="fa fa-check"></i><b>1.9</b> Vectores</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#secuencias"><i class="fa fa-check"></i><b>1.10</b> Secuencias</a></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#factores"><i class="fa fa-check"></i><b>1.11</b> Factores</a></li>
<li class="chapter" data-level="1.12" data-path="intro.html"><a href="intro.html#matrices"><i class="fa fa-check"></i><b>1.12</b> Matrices</a></li>
<li class="chapter" data-level="1.13" data-path="intro.html"><a href="intro.html#listas"><i class="fa fa-check"></i><b>1.13</b> Listas</a></li>
<li class="chapter" data-level="1.14" data-path="intro.html"><a href="intro.html#data-frames"><i class="fa fa-check"></i><b>1.14</b> Data frames</a></li>
<li class="chapter" data-level="1.15" data-path="intro.html"><a href="intro.html#r-base"><i class="fa fa-check"></i><b>1.15</b> R base</a></li>
<li class="chapter" data-level="1.16" data-path="intro.html"><a href="intro.html#apply-y-tapply"><i class="fa fa-check"></i><b>1.16</b> Apply y tapply</a></li>
<li class="chapter" data-level="1.17" data-path="intro.html"><a href="intro.html#map"><i class="fa fa-check"></i><b>1.17</b> Map</a></li>
<li class="chapter" data-level="1.18" data-path="intro.html"><a href="intro.html#loops"><i class="fa fa-check"></i><b>1.18</b> Loops</a></li>
<li class="chapter" data-level="1.19" data-path="intro.html"><a href="intro.html#condicionales"><i class="fa fa-check"></i><b>1.19</b> Condicionales</a></li>
<li class="chapter" data-level="1.20" data-path="intro.html"><a href="intro.html#funciones"><i class="fa fa-check"></i><b>1.20</b> Funciones</a><ul>
<li class="chapter" data-level="1.20.1" data-path="intro.html"><a href="intro.html#output-más-de-un-resultado"><i class="fa fa-check"></i><b>1.20.1</b> Output más de un resultado</a></li>
<li class="chapter" data-level="1.20.2" data-path="intro.html"><a href="intro.html#argumentos-con-valores-default"><i class="fa fa-check"></i><b>1.20.2</b> Argumentos con valores default</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bd.html"><a href="bd.html"><i class="fa fa-check"></i><b>2</b> Base de datos</a><ul>
<li class="chapter" data-level="2.1" data-path="bd.html"><a href="bd.html#directorio-de-trabajo"><i class="fa fa-check"></i><b>2.1</b> Directorio de trabajo</a></li>
<li class="chapter" data-level="2.2" data-path="bd.html"><a href="bd.html#cargar-datos"><i class="fa fa-check"></i><b>2.2</b> Cargar datos</a><ul>
<li class="chapter" data-level="2.2.1" data-path="bd.html"><a href="bd.html#ingrasar-datos-con-tidyverse"><i class="fa fa-check"></i><b>2.2.1</b> Ingrasar datos con <code>tidyverse</code></a></li>
<li class="chapter" data-level="2.2.2" data-path="bd.html"><a href="bd.html#bases-de-stata"><i class="fa fa-check"></i><b>2.2.2</b> Bases de Stata</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bd.html"><a href="bd.html#problemas-de-imputación"><i class="fa fa-check"></i><b>2.3</b> Problemas de imputación</a></li>
<li class="chapter" data-level="2.4" data-path="bd.html"><a href="bd.html#exportar-datos"><i class="fa fa-check"></i><b>2.4</b> Exportar datos</a></li>
<li class="chapter" data-level="2.5" data-path="bd.html"><a href="bd.html#variables"><i class="fa fa-check"></i><b>2.5</b> Variables</a></li>
<li class="chapter" data-level="2.6" data-path="bd.html"><a href="bd.html#merge"><i class="fa fa-check"></i><b>2.6</b> Merge</a></li>
<li class="chapter" data-level="2.7" data-path="bd.html"><a href="bd.html#group_by-mutate"><i class="fa fa-check"></i><b>2.7</b> group_by, mutate</a></li>
<li class="chapter" data-level="2.8" data-path="bd.html"><a href="bd.html#guardar-datos"><i class="fa fa-check"></i><b>2.8</b> Guardar datos</a></li>
<li class="chapter" data-level="2.9" data-path="bd.html"><a href="bd.html#valores-missing"><i class="fa fa-check"></i><b>2.9</b> Valores missing</a><ul>
<li class="chapter" data-level="2.9.1" data-path="bd.html"><a href="bd.html#eliminar-valores-missing"><i class="fa fa-check"></i><b>2.9.1</b> Eliminar valores missing</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="bd.html"><a href="bd.html#loop"><i class="fa fa-check"></i><b>2.10</b> Loop</a></li>
<li class="chapter" data-level="2.11" data-path="bd.html"><a href="bd.html#pivot"><i class="fa fa-check"></i><b>2.11</b> Pivot</a></li>
<li class="chapter" data-level="2.12" data-path="bd.html"><a href="bd.html#append"><i class="fa fa-check"></i><b>2.12</b> Append</a></li>
<li class="chapter" data-level="2.13" data-path="bd.html"><a href="bd.html#strings"><i class="fa fa-check"></i><b>2.13</b> Strings</a></li>
<li class="chapter" data-level="2.14" data-path="bd.html"><a href="bd.html#fechas"><i class="fa fa-check"></i><b>2.14</b> Fechas</a><ul>
<li class="chapter" data-level="2.14.1" data-path="bd.html"><a href="bd.html#year"><i class="fa fa-check"></i><b>2.14.1</b> Year</a></li>
<li class="chapter" data-level="2.14.2" data-path="bd.html"><a href="bd.html#month"><i class="fa fa-check"></i><b>2.14.2</b> Month</a></li>
<li class="chapter" data-level="2.14.3" data-path="bd.html"><a href="bd.html#day"><i class="fa fa-check"></i><b>2.14.3</b> Day</a></li>
<li class="chapter" data-level="2.14.4" data-path="bd.html"><a href="bd.html#manipulación-de-fechas"><i class="fa fa-check"></i><b>2.14.4</b> Manipulación de fechas</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="bd.html"><a href="bd.html#análisis-de-datos"><i class="fa fa-check"></i><b>2.15</b> Análisis de datos</a><ul>
<li class="chapter" data-level="2.15.1" data-path="bd.html"><a href="bd.html#tablas"><i class="fa fa-check"></i><b>2.15.1</b> Tablas</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="bd.html"><a href="bd.html#group_by-summarise"><i class="fa fa-check"></i><b>2.16</b> group_by, summarise</a></li>
<li class="chapter" data-level="2.17" data-path="bd.html"><a href="bd.html#vector-de-resultados"><i class="fa fa-check"></i><b>2.17</b> Vector de resultados</a></li>
<li class="chapter" data-level="2.18" data-path="bd.html"><a href="bd.html#gráficos"><i class="fa fa-check"></i><b>2.18</b> Gráficos</a></li>
<li class="chapter" data-level="2.19" data-path="bd.html"><a href="bd.html#ggplot"><i class="fa fa-check"></i><b>2.19</b> GGPlot</a></li>
<li class="chapter" data-level="2.20" data-path="bd.html"><a href="bd.html#guardar-un-gráfico"><i class="fa fa-check"></i><b>2.20</b> Guardar un gráfico</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conceptos.html"><a href="conceptos.html"><i class="fa fa-check"></i><b>3</b> Conceptos generales</a><ul>
<li class="chapter" data-level="3.1" data-path="conceptos.html"><a href="conceptos.html#estimacion"><i class="fa fa-check"></i><b>3.1</b> Estimacion</a></li>
<li class="chapter" data-level="3.2" data-path="conceptos.html"><a href="conceptos.html#prediccion"><i class="fa fa-check"></i><b>3.2</b> Prediccion</a></li>
<li class="chapter" data-level="3.3" data-path="conceptos.html"><a href="conceptos.html#inferencia"><i class="fa fa-check"></i><b>3.3</b> Inferencia</a></li>
<li class="chapter" data-level="3.4" data-path="conceptos.html"><a href="conceptos.html#metodos-parametricos"><i class="fa fa-check"></i><b>3.4</b> Metodos parametricos</a></li>
<li class="chapter" data-level="3.5" data-path="conceptos.html"><a href="conceptos.html#metodos-no-parametricos"><i class="fa fa-check"></i><b>3.5</b> Metodos no parametricos</a></li>
<li class="chapter" data-level="3.6" data-path="conceptos.html"><a href="conceptos.html#evaluacion-de-la-precision-del-modelo"><i class="fa fa-check"></i><b>3.6</b> Evaluacion de la precision del modelo</a><ul>
<li class="chapter" data-level="3.6.1" data-path="conceptos.html"><a href="conceptos.html#calidad-del-ajuste"><i class="fa fa-check"></i><b>3.6.1</b> Calidad del ajuste</a></li>
<li class="chapter" data-level="3.6.2" data-path="conceptos.html"><a href="conceptos.html#trade-off-sesgo-varianza"><i class="fa fa-check"></i><b>3.6.2</b> Trade-off Sesgo-Varianza</a></li>
<li class="chapter" data-level="3.6.3" data-path="conceptos.html"><a href="conceptos.html#clasificacion"><i class="fa fa-check"></i><b>3.6.3</b> Clasificacion</a></li>
<li class="chapter" data-level="3.6.4" data-path="conceptos.html"><a href="conceptos.html#matriz-de-confusion"><i class="fa fa-check"></i><b>3.6.4</b> Matriz de confusion</a></li>
<li class="chapter" data-level="3.6.5" data-path="conceptos.html"><a href="conceptos.html#curva-roc"><i class="fa fa-check"></i><b>3.6.5</b> Curva ROC</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="conceptos.html"><a href="conceptos.html#cross-validation"><i class="fa fa-check"></i><b>3.7</b> Cross Validation</a></li>
<li class="chapter" data-level="3.8" data-path="conceptos.html"><a href="conceptos.html#bootstrap"><i class="fa fa-check"></i><b>3.8</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mco.html"><a href="mco.html"><i class="fa fa-check"></i><b>4</b> Regresion lineal</a><ul>
<li class="chapter" data-level="4.1" data-path="mco.html"><a href="mco.html#aplicacion-practica"><i class="fa fa-check"></i><b>4.1</b> Aplicacion practica</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>5</b> Lasso</a><ul>
<li class="chapter" data-level="5.1" data-path="lasso.html"><a href="lasso.html#aplicacion-practica-1"><i class="fa fa-check"></i><b>5.1</b> Aplicacion practica</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="logit.html"><a href="logit.html"><i class="fa fa-check"></i><b>6</b> Logit</a><ul>
<li class="chapter" data-level="6.1" data-path="logit.html"><a href="logit.html#aplicacion-practica-2"><i class="fa fa-check"></i><b>6.1</b> Aplicacion practica</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="arboles.html"><a href="arboles.html"><i class="fa fa-check"></i><b>7</b> Arboles de decision</a><ul>
<li class="chapter" data-level="7.1" data-path="arboles.html"><a href="arboles.html#bagging"><i class="fa fa-check"></i><b>7.1</b> Bagging</a></li>
<li class="chapter" data-level="7.2" data-path="arboles.html"><a href="arboles.html#random-forest"><i class="fa fa-check"></i><b>7.2</b> Random Forest</a></li>
<li class="chapter" data-level="7.3" data-path="arboles.html"><a href="arboles.html#boosting"><i class="fa fa-check"></i><b>7.3</b> Boosting</a><ul>
<li class="chapter" data-level="7.3.1" data-path="arboles.html"><a href="arboles.html#ada-boost"><i class="fa fa-check"></i><b>7.3.1</b> Ada Boost</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="arboles.html"><a href="arboles.html#aplicacion-practica-3"><i class="fa fa-check"></i><b>7.4</b> Aplicacion practica</a><ul>
<li class="chapter" data-level="7.4.1" data-path="arboles.html"><a href="arboles.html#arboles-de-regresion"><i class="fa fa-check"></i><b>7.4.1</b> Arboles de regresion</a></li>
<li class="chapter" data-level="7.4.2" data-path="arboles.html"><a href="arboles.html#bagging-y-random-forests"><i class="fa fa-check"></i><b>7.4.2</b> Bagging y Random Forests</a></li>
<li class="chapter" data-level="7.4.3" data-path="arboles.html"><a href="arboles.html#boosting-1"><i class="fa fa-check"></i><b>7.4.3</b> Boosting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="cluster.html"><a href="cluster.html"><i class="fa fa-check"></i><b>8</b> Analisis de clusters</a><ul>
<li class="chapter" data-level="8.1" data-path="cluster.html"><a href="cluster.html#k-means-clustering"><i class="fa fa-check"></i><b>8.1</b> K-Means Clustering</a></li>
<li class="chapter" data-level="8.2" data-path="cluster.html"><a href="cluster.html#aplicacion-practica-4"><i class="fa fa-check"></i><b>8.2</b> Aplicacion practica</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="conceptos" class="section level1">
<h1><span class="header-section-number">Capítulo 3</span> Conceptos generales</h1>
<p>El <strong>objetivo</strong> de la Ciencia de Datos es <strong>aprender</strong> algo de los <strong>datos</strong>. Si se dispone de una variable <em>output</em><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> y otras <em>input</em><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> el aprendizaje se denomina <strong>supervisado</strong>, si sólo hay <em>inputs</em> el aprendizaje es <strong>no supervisado</strong>.</p>
<p>Dentro aprendizaje supervisado podemos distinguir la <strong>predicción</strong>, cuando la variable <em>output</em> es cuantitativa, de la <strong>clasificación</strong> donde la la variable <em>output</em> es discreta/categórica (ej. <span class="math inline">\(0/1\)</span>).<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> Por su parte, el análisis no supervisado busca relaciones y estructura dentro de los datos (ej. distinguir <em>clusters</em>/grupos de clientes para promociones publicitarias).</p>
<div id="estimacion" class="section level2">
<h2><span class="header-section-number">3.1</span> Estimacion</h2>
<p>Supongamos que se quiere estudiar la relación entre el gasto en publicidad a través de diversos canales como televisión, radio, diarios (<em>inputs</em>) y las ventas en distintos mercados (<em>output</em>).</p>
<p><span class="math display" id="eq:estima">\[\begin{equation}
\tag{3.1}
  Y = f(X) + \epsilon
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(f\)</span> es una función desconocida de <span class="math inline">\((X_1, X_2, X_3)\)</span> y <span class="math inline">\(\epsilon\)</span> es un término de error aleatorio independiente de <span class="math inline">\(X\)</span> con media <span class="math inline">\(0\)</span>.</p>
<p>En esencia, el aprendizaje estadístico se refiere a un conjunto de enfoques para estimar <span class="math inline">\(f\)</span>.</p>
</div>
<div id="prediccion" class="section level2">
<h2><span class="header-section-number">3.2</span> Prediccion</h2>
<p>Supongamos que se dispone de datos de variables independientes por no de la dependiente, en ese caso, dado que el error en promedio es <span class="math inline">\(0\)</span> podríamos predecir <span class="math inline">\(Y\)</span> utilizando:</p>
<p><span class="math display" id="eq:pred">\[\begin{equation}
\tag{3.2}
  \hat{Y} = \hat{f}(X)
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(\hat{f}\)</span> representa nuestra estimación de <span class="math inline">\(f\)</span> y <span class="math inline">\(\hat{Y}\)</span> representa la predicción de <span class="math inline">\(Y\)</span>. En este contexto, <span class="math inline">\(\hat{f}\)</span> a menudo se trata como una <strong>caja negra</strong>, en el sentido que no importa la forma exacta de <span class="math inline">\(\hat{f}\)</span>, siempre que produzca predicciones precisas de <span class="math inline">\(Y\)</span>.</p>
<p>La precisión con la que <span class="math inline">\(\hat{Y}\)</span> se acerca a <span class="math inline">\(Y\)</span> depende de dos cantidades, el error <em>reducible</em> y el <em>irreducible</em>. En general, <span class="math inline">\(\hat{f}\)</span> no será una estimación perfecta de <span class="math inline">\(f\)</span>, y esta inexactitud introducirá
un error que es reducible porque potencialmente podemos mejorar la
precisión de <span class="math inline">\(\hat{f}\)</span> usando la técnica de aprendizaje estadístico más apropiada para estimar <span class="math inline">\(f\)</span>. Sin embargo, si fuera posible estimar <span class="math inline">\(f\)</span> de manera perfecta de manera que la respuesta estimada <span class="math inline">\(\hat{Y} = f(X)\)</span>, nuestra predicción todavía tendría algún error dado que <span class="math inline">\(Y\)</span> también es función de <span class="math inline">\(\epsilon\)</span>, que por definición, no se puede predecir usando <span class="math inline">\(X\)</span>. Por lo tanto, la variabilidad asociada con <span class="math inline">\(\epsilon\)</span> también afecta la precisión de nuestras predicciones. Esto se conoce como el error irreducible, porque no importa qué tan bien estimemos <span class="math inline">\(f\)</span>, no puede reducir el error introducido por <span class="math inline">\(\epsilon\)</span>.</p>
<p>El término de error <span class="math inline">\(\epsilon\)</span> puede contener variables no observables que son útiles para predecir <span class="math inline">\(Y\)</span> y, por lo tanto, <span class="math inline">\(f\)</span> no puede usarlos para su predicción.</p>
<p><span class="math display">\[\begin{align*}
  E(Y - \hat{Y})^2 &amp; = E[f(X) + \epsilon - \hat{f}(X)]^2 \\
                   &amp; = \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{Var(\epsilon)}_{\text{Irreducible}}
\end{align*}\]</span></p>
<p>donde de <span class="math inline">\(E(Y − \hat{Y})^2\)</span> representa el promedio, o valor esperado, de la diferencia entre el valor predicho y real de <span class="math inline">\(Y\)</span> al cuadrado, y <span class="math inline">\(Var(\epsilon)\)</span> representa la varianza asociada al término de error <span class="math inline">\(\epsilon\)</span>.</p>
<p>El foco de este curso está en las técnicas para estimar <span class="math inline">\(f\)</span> con el objetivo de minimizar el error reducible. Es importante tener en cuenta que la error irreducible siempre proporcionará un límite superior en la precisión de nuestra predicción para <span class="math inline">\(Y\)</span> que en la práctica casi siempre es desconocido.</p>
</div>
<div id="inferencia" class="section level2">
<h2><span class="header-section-number">3.3</span> Inferencia</h2>
<p>En este caso el interés está en comprender la asociación entre <span class="math inline">\(Y\)</span> y <span class="math inline">\(X_1,..., X_p\)</span>. Se busca estimar <span class="math inline">\(f\)</span>, pero el objetivo no es necesariamente hacer predicciones sobre <span class="math inline">\(Y\)</span>. Ahora <span class="math inline">\(\hat{f}\)</span> <strong>no</strong> puede ser tratada como una <strong>caja negra</strong>, porque se necesita conocer su forma exacta. Así se busca determinar:</p>
<ul>
<li>Qué variables se deben incluir en el modelo</li>
<li>Cómo es la relación entre el la variable explicada y cada predictor</li>
<li>Si la relación se puede aproximar con un modelo lineal o uno más complejo</li>
</ul>
</div>
<div id="metodos-parametricos" class="section level2">
<h2><span class="header-section-number">3.4</span> Metodos parametricos</h2>
<p>Se realiza en dos etapas:</p>
<ul>
<li>Asumir una forma funcional (<strong>modelo</strong>, por ejemplo lineal)</li>
</ul>
<p><span class="math display" id="eq:ml">\[\begin{equation}
\tag{3.3}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p
\end{equation}\]</span></p>
<ul>
<li>Estimar los parámetros (<strong>método</strong>, por ejemplo MCO)</li>
</ul>
<p>Si bien el problema se reduce a estimar <span class="math inline">\(p + 1\)</span> parámetros, la desventaja es que la forma funcional elegida puede diferir de la verdadera <span class="math inline">\(f\)</span>.</p>
</div>
<div id="metodos-no-parametricos" class="section level2">
<h2><span class="header-section-number">3.5</span> Metodos no parametricos</h2>
<p>No realizan supuestos sobre la forma funcional de <span class="math inline">\(f\)</span> sino que tratan de buscar una estimación que se acerque lo más posible a los datos sin ser ni demasiado tosco ni demasiado ondulado. El costo es que se necesitan más datos para estimar.</p>
</div>
<div id="evaluacion-de-la-precision-del-modelo" class="section level2">
<h2><span class="header-section-number">3.6</span> Evaluacion de la precision del modelo</h2>
<p>Ningún método domina al resto sobre todas las bases de datos posibles. En un conjunto de datos en particular, un método específico puede funcionar mejor, pero algún otro método lo puede superar con otra base de datos. Por lo tanto, en cada caso se debe decidir qué método produce los mejores resultados.</p>
<div id="calidad-del-ajuste" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Calidad del ajuste</h3>
<p>Para evaluar el desempeño de un método de aprendizaje estadístico en
un conjunto de datos dado, se necesita alguna forma de medir qué tan bien sus predicciones coinciden con los datos observados. Es decir, se necesita cuantificar el grado en el cual el valor pronosticado para una observación dada está cerca de el verdadero valor de respuesta para esa observación. En el escenario de regresión, la medida más utilizada es el error cuadrático medio (<span class="math inline">\(MSE\)</span>):</p>
<p><span class="math display" id="eq:mse">\[\begin{equation}
\tag{3.4}
  MSE = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(\hat{f}(x_i)\)</span> es la predicción que <span class="math inline">\(\hat{f}\)</span> hace sobre la observación <span class="math inline">\(i\)</span>. El <span class="math inline">\(MSE\)</span> será pequeño si las respuestas predichas están muy cerca de las respuestas verdaderas y será grande si para algunas observaciones difieren demasiado.</p>
<p>El <span class="math inline">\(MSE\)</span> en <a href="conceptos.html#eq:mse">(3.4)</a> se calcula usando los datos de entrenamiento (<em>training</em>) que se usaron para estimar el modelo, por lo que debería denominarse con mayor precisión <span class="math inline">\(MSE\)</span> de entrenamiento. Pero en general, no nos interesa realmente qué tan bien funciona el método
sobre los datos de entrenamiento. Más bien, estamos interesados en la precisión de las predicciones que obtenemos cuando aplicamos nuestro método <strong>datos de <em>test</em></strong> que no fueron visto antes (datos no utilizados para entrenar el modelo). Es decir, se busca elegir el método que produzca el menor <span class="math inline">\(MSE\)</span> de <em>test</em>.</p>
<p><span class="math display" id="eq:msetest">\[\begin{equation}
\tag{3.5}
  Prom(y_0 - \hat{f}(x_0))^2
\end{equation}\]</span></p>
<p>el error de predicción cuadrático promedio para estas observaciones de <em>test</em> <span class="math inline">\((y_0, x_0)\)</span>.</p>
<p>¿Qué sucede si se elige en base al MSE de <em>training</em> <a href="conceptos.html#eq:mse">(3.4)</a> ? no hay garantía de que el método con el MSE de entrenamiento más bajo también tenga el MSE de test más bajo.</p>
<p>El panel de la izquierda de la Figura <a href="conceptos.html#fig:f29">3.1</a> muestra la verdadera <span class="math inline">\(f\)</span> dada por la curva negra. Las curvas naranja, azul y verde ilustran
tres posibles estimaciones de <span class="math inline">\(f\)</span> obtenidas utilizando métodos con
niveles de flexibilidad. La línea naranja es el ajuste de regresión lineal, que es relativamente inflexible. Las curvas azul y verde se produjeron usando <em>splines</em> con diferentes niveles de suavidad. Es claro que a medida que aumenta el nivel de flexibilidad, las curvas se ajustan mejor a los datos observados. La curva verde es la más flexible y coincide con la datos muy bien; sin embargo, se observa que se ajusta mal a la verdadera <span class="math inline">\(f\)</span> (en negro) porque es demasiado ondulada. Cambiando el nivel de flexibilidad del <em>spline</em> se pueden producir ajustes diferentes para estos datos.</p>
<p>En el panel de la derecha de la Figura <a href="conceptos.html#fig:f29">3.1</a> la curva gris
muestra el <span class="math inline">\(MSE\)</span> de entrenamiento promedio en función de la flexibilidad, o más formalmente los grados de libertad (resume la flexibilidad de una curva), para una serie de <em>splines</em>. Los cuadrados naranja, azul y verde
indican los <span class="math inline">\(MSE\)</span> asociados con las curvas correspondientes en el panel izquierdo. El <span class="math inline">\(MSE\)</span> de entrenamiento MSE disminuye monótonamente a medida que aumenta la flexibilidad. En este ejemplo el verdadera <span class="math inline">\(f\)</span> es no lineal, por lo que el ajuste lineal naranja no es lo suficientemente flexible para estimar bien <span class="math inline">\(f\)</span>. La curva verde tiene el <span class="math inline">\(MSE\)</span> de entrenamiento más bajo de los tres métodos, ya que corresponde a la más flexible de las tres curvas.</p>
<p>En este ejemplo, se conoce la verdadera función <span class="math inline">\(f\)</span>, por lo que también se puede calcular el <span class="math inline">\(MSE\)</span> de <em>test</em> (en general <span class="math inline">\(f\)</span> es desconocida, por lo que esto no es posible). El <span class="math inline">\(MSE\)</span> de <em>test</em> se muestra usando la curva roja en el panel derecho de la Figura <a href="conceptos.html#fig:f29">3.1</a>. Como con el MSE de entrenamiento, el MSE de <em>test</em> disminuye inicialmente a medida que el nivel de flexibilidad aumenta. Sin embargo, en algún momento el MSE de <em>test</em> se nivela y luego empieza a aumentar. En consecuencia, las curvas naranja y verde tienen un <span class="math inline">\(MSE\)</span> de <em>test</em> alto. La curva azul minimiza el <span class="math inline">\(MSE\)</span> de <em>test</em>, dado que visualmente parece estimar <span class="math inline">\(f\)</span> lo mejor en el panel izquierdo. La línea discontinua horizontal indica <span class="math inline">\(Var(\epsilon)\)</span>, el error irreducible en la ecuación de <span class="math inline">\(E(Y - \hat{Y})^2\)</span>, que corresponde al menor alcanzable por el MSE de <em>test</em> entre todos los métodos posibles. Por lo tanto, la <em>spline</em> de suavizado representada por la curva azul está cerca del óptimo.</p>
<p>En el panel de la derecha de la Figura <a href="conceptos.html#fig:f29">3.1</a>, como la flexibilidad del método de aprendizaje aumente, se observa una disminución monótona en el <span class="math inline">\(MSE\)</span> de entrenamiento y una forma de U en el <span class="math inline">\(MSE\)</span> de <em>test</em>. Esta es una propiedad fundamental de aprendizaje estadístico que se mantiene independientemente del conjunto de datos particular en cuestión e independientemente del método estadístico que se utilice.</p>
<p>Cuando un método dado produce un <span class="math inline">\(MSE\)</span> de entrenamiento pequeño pero un <span class="math inline">\(MSE\)</span> de <em>test</em> grande, se dice que está haciendo <em>overfitting</em>/sobreajustando los datos. Esto sucede porque nuestro aprendizaje estadístico está trabajando demasiado para encontrar patrones en los datos de entrenamiento, y puede estar detectando algunos patrones que son causados por casualidad en lugar de por las verdaderas propiedades de la función desconocida <span class="math inline">\(f\)</span>. <em>Overfitting</em> se refiere específicamente al caso en el que un modelo menos flexible podría haber producido un menor error de predicción en <em>test</em>.</p>
<div class="figure"><span style="display:block;" id="fig:f29"></span>
<img src="2.9MSE_TestTrain.png" alt="Datos en curva y MSE" width="80%" />
<p class="caption">
Figura 3.1: Datos en curva y MSE
</p>
</div>
<p>La Figura <a href="conceptos.html#fig:f210">3.2</a> proporciona otro ejemplo en el que la verdadera <span class="math inline">\(f\)</span> es aproximadamente lineal por lo que este tipo de modelos obtienen el menor <span class="math inline">\(MSE\)</span> en <em>test</em>.</p>
<div class="figure"><span style="display:block;" id="fig:f210"></span>
<img src="2.10MSE_TestTrain.png" alt="Datos lineales y MSE" width="80%" />
<p class="caption">
Figura 3.2: Datos lineales y MSE
</p>
</div>
</div>
<div id="trade-off-sesgo-varianza" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Trade-off Sesgo-Varianza</h3>
<p>La Figura <a href="conceptos.html#fig:fsv">3.3</a> muestra el <em>trade-off</em> <strong>Sesgo - Varianza</strong> intuitivamente.</p>
<div class="figure"><span style="display:block;" id="fig:fsv"></span>
<img src="SesgoVar.png" alt="Estimacion y MSE" width="70%" />
<p class="caption">
Figura 3.3: Estimacion y MSE
</p>
</div>
<p>La forma de U observada en las curvas <span class="math inline">\(MSE\)</span> de <em>test</em> es el resultado de dos propiedades que compiten en los métodos de aprendizaje estadístico. El <span class="math inline">\(MSE\)</span> de <em>test</em> esperado, para un valor dado <span class="math inline">\(x_0\)</span>, puede descomponerse en la suma de tres cantidades fundamentales: la
varianza de <span class="math inline">\(\hat{f}(x_0)\)</span>, el sesgo al cuadrado de <span class="math inline">\(\hat{f}(x_0)\)</span> y la varianza del error <span class="math inline">\(\epsilon\)</span>.</p>
<p><span class="math display" id="eq:sv">\[\begin{equation}
\tag{3.6}
  E(y_0 - \hat{f}(x_0))^2 = Var(\hat{f}(x_0)) + [Sesgo(\hat{f}(x_0))]^2 + Var(\epsilon)
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(E(y_0 - \hat{f}(x_0))^2\)</span> el valor esperado de <span class="math inline">\(MSE\)</span> de <em>test</em> en <span class="math inline">\(x_0\)</span>. Para minimizar el error de <em>test</em> esperado, se necesita seleccionar un método de aprendizaje estadístico que logre simultáneamente <strong>baja varianza</strong> y <strong>bajo sesgo</strong>.</p>
<p>La <strong>varianza</strong> se refiere al valor en que <span class="math inline">\(f\)</span> cambiaría si
se estimara utilizando una base de datos de entrenamiento diferente. <strong>Sesgo</strong> se refiere al error que se introduce al aproximar un problema de la vida real, que puede ser extremadamente complicado, por mucho modelo más simple. Como regla general, a medida que se utilizan métodos más flexibles, la varianza aumenta y el sesgo disminuye. La tasa relativa de cambio de estos dos cantidades determina si el <span class="math inline">\(MSE\)</span> de <em>test</em> aumenta o disminuye.</p>
<p>Los dos paneles de la Figura <a href="conceptos.html#fig:f212">3.4</a> ilustran la Ecuación <a href="conceptos.html#eq:sv">(3.6)</a> para los ejemplos en Figuras <a href="conceptos.html#fig:f29">3.1</a> y <a href="conceptos.html#fig:f210">3.2</a>. En cada caso, la curva sólida azul representa el cuadrado del sesgo, para diferentes niveles de flexibilidad, mientras que la curva naranja corresponde a la varianza. La línea discontinua horizontal representa <span class="math inline">\(Var(\epsilon)\)</span>, el error irreducible. Finalmente, la curva roja, corresponde al MSE de <em>test</em>, es la suma de estas tres cantidades.</p>
<div class="figure"><span style="display:block;" id="fig:f212"></span>
<img src="2.12MSE_SesgoVar.png" alt="Estimacion y MSE" width="80%" />
<p class="caption">
Figura 3.4: Estimacion y MSE
</p>
</div>
</div>
<div id="clasificacion" class="section level3">
<h3><span class="header-section-number">3.6.3</span> Clasificacion</h3>
<p>Muchos de los conceptos del contexto de regresion, como
el <em>trade-off</em> sesgo-varianza, se transfieren al entorno de clasificación donde ahora <span class="math inline">\(y_i\)</span> es cualitativa. El el enfoque más común para cuantificar la precisión de la estimación <span class="math inline">\(\hat{f}\)</span> es la <strong>tasa de error</strong> de entrenamiento, la proporción de errores que se cometen si aplicamos nuestra estimación <span class="math inline">\(\hat{f}\)</span> a las observaciones de entrenamiento</p>
<p><span class="math display" id="eq:terror">\[\begin{equation}
\tag{3.7}
  \frac{1}{n} \sum_{i=1}^{n}I(y_i \neq \hat{y_i})
\end{equation}\]</span></p>
<p>Aquí <span class="math inline">\(\hat{y_i}\)</span> es la etiqueta de clase predicha para la i-ésima observación usando <span class="math inline">\(\hat{f}\)</span>. Y <span class="math inline">\(I(y_i \neq \hat{y_i})\)</span> es una variable indicadora que es igual a 1 si <span class="math inline">\(y_i \neq \hat{y_i}\)</span> y cero si <span class="math inline">\(y_i = \hat{y_i}\)</span>. indicador Si <span class="math inline">\(I(y_i \neq \hat{y_i}) = 0\)</span> entonces la i-ésima observación fue clasificada correctamente por el método de clasificación; de lo contrario, fue mal clasificado.</p>
<p>La tasa de error de <em>test</em> asociada con un conjunto de observaciones de <em>test</em> de la forma <span class="math inline">\((x_0, y_0)\)</span> está dada por:</p>
<p><span class="math display" id="eq:terror2">\[\begin{equation}
\tag{3.8}
  Prom(I(y_0 \neq \hat{y_0}))
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(\hat{y_0}\)</span> es la etiqueta de clase predicha que resulta de aplicar el clasificador a la observación de prueba con predictor <span class="math inline">\(x_0\)</span>. Un buen clasificador es aquel para el cual el error de prueba <a href="conceptos.html#eq:terror2">(3.8)</a> es el más pequeño.</p>
<div id="clasificador-de-bayes" class="section level4">
<h4><span class="header-section-number">3.6.3.1</span> Clasificador de Bayes</h4>
<p>Es posible mostrar bajo penalidad simétrica<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> la tasa de error de <em>test</em> postulada en <a href="conceptos.html#eq:terror2">(3.8)</a> se minimiza, en promedio, por un clasificador muy simple que asigna cada observación a la clase más probable, dados sus valores predictores. En otras palabras, se debería asignar una observación de test con vector predictor <span class="math inline">\(x_0\)</span> a la clase <span class="math inline">\(j\)</span> para la cual <a href="conceptos.html#eq:bayes">(3.9)</a> es mayor.</p>
<p><span class="math display" id="eq:bayes">\[\begin{equation}
\tag{3.9}
  Pr(Y = j \mid X = x_0)
\end{equation}\]</span></p>
<p>Es decir, en un problema donde sólo hay dos categorías el clasificador de Bayes predice la clase <span class="math inline">\(1\)</span> si <span class="math inline">\(Pr(Y = 1 \mid X = x_0)&gt;0.5\)</span> y la clase <span class="math inline">\(2\)</span> en caso contrario.</p>
</div>
</div>
<div id="matriz-de-confusion" class="section level3">
<h3><span class="header-section-number">3.6.4</span> Matriz de confusion</h3>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"></th>
<th align="center">Observado</th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="center"><strong>Predicción</strong></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(VN\)</span></td>
<td align="center"><span class="math inline">\(FN\)</span></td>
</tr>
<tr class="odd">
<td align="center">(decision)</td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(FP\)</span></td>
<td align="center"><span class="math inline">\(VP\)</span></td>
</tr>
</tbody>
</table>
<p><br> </br></p>
<p><span class="math inline">\(VN\)</span>: Verdadero Negativo; <span class="math inline">\(FN\)</span>: Falso Negativo; <span class="math inline">\(FP\)</span>: Falso Positivo; <span class="math inline">\(VP\)</span>: Verdadero Positivo</p>
<p>Métricas para comparar modelos. La <strong>precisión</strong> es la cantidad de predicciones correctas, la <strong>sensibilidad</strong> es la cantidad de <span class="math inline">\(VP\)</span> identificados sobre el total de positivos y la <strong>especificidad</strong> es la cantidad de <span class="math inline">\(VN\)</span> identificados sobre el total de negativos.</p>
<p><span class="math display">\[\begin{align*}
Precision &amp; = \frac{VP + VN}{VP + VN + FP + FN} \\ 
Sensiblilidad &amp; = \frac{VP}{VP + FN} \\ 
Especificidad &amp; = \frac{VN}{VN + FP} 
\end{align*}\]</span></p>
</div>
<div id="curva-roc" class="section level3">
<h3><span class="header-section-number">3.6.5</span> Curva ROC</h3>
<p>El nombre viene de <em>receiver operating characteristics</em> (comunicación). Si se modifica el umbral <span class="math inline">\(p_i &gt; c\)</span>, cambian los resultados de la matriz de confusión. Por ejemplo, al estimar la probabilidad de <em>default</em>, para un banco podría resultar relativamente más costoso clasificar a un mal deudor como no <em>default</em> que a uno bueno como <em>default</em>. Entonces podría bajar el umbral <span class="math inline">\(p_i &gt; 0,3\)</span> (asimétrico) para clasificar casos positivos afectando las tasas de errores.</p>
<p>Si se define:
<span class="math display">\[\begin{align*}
TPR = &amp; VP / P \\ 
FPR = &amp; FP / N
\end{align*}\]</span></p>
<p>La curva <span class="math inline">\(ROC\)</span> representa la relación entre <span class="math inline">\(TPR\)</span> y <span class="math inline">\(FPR\)</span> (notar que la <span class="math inline">\(FPR = 1 - Especificidad\)</span>) para todos los valores posibles de <span class="math inline">\(c \in [0, 1]\)</span>. La curva <span class="math inline">\(ROC\)</span> permite medir capacidad predictiva y comparar modelos.</p>
<div class="figure"><span style="display:block;" id="fig:roc"></span>
<img src="4.8ROC.png" alt="Curva ROC" width="75%" />
<p class="caption">
Figura 3.5: Curva ROC
</p>
</div>
<p>Casos extremos</p>
<ul>
<li><span class="math inline">\(c = 1\)</span> todos clasificados como negativos ; <span class="math inline">\(tpr = 0\)</span>, <span class="math inline">\(fpr = 0\)</span></li>
<li><span class="math inline">\(c = 0\)</span> todos clasificados como positivos ; <span class="math inline">\(tpr = 1\)</span>, <span class="math inline">\(fpr = 1\)</span></li>
</ul>
<div class="figure"><span style="display:block;" id="fig:roc2"></span>
<img src="rocideal.png" alt="Curva ROC puntos importantes" width="65%" />
<p class="caption">
Figura 3.6: Curva ROC puntos importantes
</p>
</div>
<p><span class="math inline">\(AUC\)</span> (o <span class="math inline">\(AUROC\)</span>): área bajo la curva <span class="math inline">\(ROC\)</span>. Cuán parecida es la curva <span class="math inline">\(ROC\)</span> a la ideal, es decir cuanto <span class="math inline">\(AUC\)</span> está más cerca de <span class="math inline">\(1\)</span> mejor es el clasificador. Por su parte, un clasificador aleatorio debe tener un <span class="math inline">\(AUC = 0,5\)</span> (línea de <span class="math inline">\(45^\circ\)</span>).</p>
</div>
</div>
<div id="cross-validation" class="section level2">
<h2><span class="header-section-number">3.7</span> Cross Validation</h2>
<p>Recordar la diferencia entre la tasa de error de <em>test</em> y la
tasa de error de entrenamiento. El error de <em>test</em> es el error promedio que resulta de usar un método de aprendizaje estadístico para predecir la respuesta en una nueva observación, es decir, que no fue utilizada en el entrenamiento del método.</p>
<p>Modelos complejos predicen bien dentro de la muestra pero mal fuera de la misma (<em>overfit</em>) y nuestro interés está puesto en esta última.</p>
<p><strong>Objetivo:</strong> buscar el nivel de complejidad óptimo para predecir fuera de la muestra.</p>
<p>Pérdida</p>
<ul>
<li>Regresión = <span class="math inline">\((Y - \hat{Y})^2\)</span></li>
<li>Clasificación = <span class="math inline">\(1(Y \neq \hat{Y})\)</span></li>
</ul>
<p><strong><em>k-Fold cross-validation</em></strong></p>
<ol style="list-style-type: decimal">
<li>Dividir la muestra en <span class="math inline">\(K\)</span> partes al azar.</li>
<li>Tomar <span class="math inline">\(K - 1\)</span> partes y estimar en modelo.</li>
<li>Calcular el error de predicción para los datos no utilizados.</li>
<li>Repetir para <span class="math inline">\(k = 1,...,K\)</span></li>
</ol>
<div class="figure"><span style="display:block;" id="fig:cv"></span>
<img src="5.5CrossVal.png" alt="K-Fold cross-validation" width="65%" />
<p class="caption">
Figura 3.7: K-Fold cross-validation
</p>
</div>
<p>La estimación por <em>cross-validation</em> del error de predicción es:</p>
<p><span class="math display" id="eq:cv">\[\begin{equation}
\tag{3.10}
  CV(\hat{f}) = \frac{1}{N}L(Y_i - \hat{Y}_{-k}(x_i))
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(\hat{Y}_{-k}(x_i)\)</span> es la predicción hecha cuando la observación no fue usada para estimar. Cada observación se utiliza en dos roles: Entrenamiento y <em>test</em>. De esta forma se estima el modelo <span class="math inline">\(K\)</span> veces para construir el error de pronóstico.</p>
<p><em>Cross-validation</em> para eleccion de modelos
Si <span class="math inline">\(\alpha\)</span> representa la complejidad de un modelo (por ejemplo el grado de un polinomio)</p>
<p><span class="math display" id="eq:cv2">\[\begin{equation}
\tag{3.11}
  CV(\hat{f}, \alpha) = \frac{1}{N}L(Y_i - \hat{Y}_{-k}(x_i, \alpha))
\end{equation}\]</span></p>
<p>Computar <span class="math inline">\(CV(\hat{f}, \alpha)\)</span> para distintos valores de <span class="math inline">\(\alpha\)</span> y elegir el modelo que minimiza el error.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
</div>
<div id="bootstrap" class="section level2">
<h2><span class="header-section-number">3.8</span> Bootstrap</h2>
<p><em>Bootstrap</em> es una herramienta estadística que se puede utilizar para cuantificar la incertidumbre asociada con un estimador o un método de aprendizaje estadístico.</p>
<p>Dado <span class="math inline">\(Y_1, Y_2,...,Y_n\)</span> iid <span class="math inline">\(Y \sim (\mu, \sigma^2)\)</span></p>
<p>Se quiere estimar la varianza de la media muestral <span class="math inline">\(V(\overline{Y}) = \frac{\sigma^2}{n}\)</span></p>
<p>Formula: <span class="math inline">\(\frac{\hat{\sigma}^2}{n}\)</span></p>
<p><span class="math display" id="eq:sigma2">\[\begin{equation}
\tag{3.12}
  \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n}(Y_i-\overline{Y})^2 
\end{equation}\]</span></p>
<p><strong>Método sin fórmula</strong></p>
<p>De los <span class="math inline">\(N\)</span> datos originales <span class="math inline">\(y_1, y_2,...,y_N\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Tomar una muestra <strong>con remplazo</strong> de tamaño <span class="math inline">\(n\)</span> (una observación puede entrar más de una vez y otra puede no entrar nunca).</li>
<li>Computar la media muestral de esta muestra.</li>
<li>Repetir <span class="math inline">\(B\)</span> veces. Al terminar tendremos <span class="math inline">\(B\)</span> estimaciones de la media.</li>
<li>Calcular la varianza de las <span class="math inline">\(B\)</span> medias.</li>
</ol>
<p><strong>En términos generales</strong></p>
<p>Dado <span class="math inline">\(Y_i\)</span> con <span class="math inline">\(i = 1,...,n\)</span> y <span class="math inline">\(\theta\)</span> es una magnitud de interés</p>
<ol style="list-style-type: decimal">
<li>Tomar una muestra <strong>con remplazo</strong> de tamaño <span class="math inline">\(n\)</span> (muestra <em>bootstrap</em>).</li>
<li>Computar <span class="math inline">\(\hat{\theta}_j\)</span>, con <span class="math inline">\(j = 1,...,B\)</span>.</li>
<li>Repetir <span class="math inline">\(B\)</span> veces.</li>
<li>Calcular:</li>
</ol>
<p><span class="math display" id="eq:theta">\[\begin{equation}
\tag{3.13}
  \hat{V}(\hat{\theta})_B = \frac{1}{B} \sum_{j=1}^{B}(\hat{\theta}_j - \overline{\hat{\theta}})^2 
\end{equation}\]</span></p>
<p><strong>Ejemplo:</strong></p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb340-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb340-2" title="2">poblacion =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb340-3" title="3"></a>
<a class="sourceLine" id="cb340-4" title="4"><span class="co"># Bootstrap con 10000 repeticiones:</span></a>
<a class="sourceLine" id="cb340-5" title="5">muestra_boot =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb340-6" title="6"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {</a>
<a class="sourceLine" id="cb340-7" title="7">  muestra =<span class="st"> </span><span class="kw">sample</span>(poblacion, <span class="dv">300</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb340-8" title="8">  muestra_boot =<span class="st"> </span><span class="kw">c</span>(muestra_boot, <span class="kw">mean</span>(muestra))</a>
<a class="sourceLine" id="cb340-9" title="9">}  </a>
<a class="sourceLine" id="cb340-10" title="10"></a>
<a class="sourceLine" id="cb340-11" title="11"><span class="co"># Media calculada con MUESTRA BOOTSTRAP</span></a>
<a class="sourceLine" id="cb340-12" title="12">simulated_mean =<span class="st"> </span><span class="kw">mean</span>(muestra_boot)</a>
<a class="sourceLine" id="cb340-13" title="13"></a>
<a class="sourceLine" id="cb340-14" title="14"><span class="co"># Varianza de la MUESTRA BOOTSTRAP</span></a>
<a class="sourceLine" id="cb340-15" title="15">simulated_var =<span class="st"> </span><span class="kw">sd</span>(muestra_boot)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb340-16" title="16"></a>
<a class="sourceLine" id="cb340-17" title="17"><span class="co"># Comparemos medias:</span></a>
<a class="sourceLine" id="cb340-18" title="18"><span class="kw">mean</span>(poblacion); simulated_mean</a></code></pre></div>
<pre><code>## [1] -0.0265972</code></pre>
<pre><code>## [1] -0.02612666</code></pre>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" title="1"><span class="co"># Comparemos varianza:</span></a>
<a class="sourceLine" id="cb343-2" title="2"><span class="kw">sd</span>(poblacion)<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">300</span>; simulated_var</a></code></pre></div>
<pre><code>## [1] 0.003315608</code></pre>
<pre><code>## [1] 0.003312673</code></pre>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>También variable dependiente o variable explicada<a href="conceptos.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>También variables independientes o variables explicativas<a href="conceptos.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>En general el interés no esta puesto en realizar inferencia/análisis condicional.<a href="conceptos.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>¿Útil para probabilidad de <em>default</em>?<a href="conceptos.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Luego de seleccionar el modelo se estima con la muestra completa.<a href="conceptos.html#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bd.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mco.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-conceptos.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Libro.pdf", "Libro.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
